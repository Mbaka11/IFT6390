{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet: Pr√©diction de la demande √©nerg√©tique\n",
    "\n",
    "**IFT3395/IFT6390 - Fondements de l'apprentissage machine**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pierrelux/mlbook/blob/main/exercises/projet_energie.ipynb)\n",
    "\n",
    "**Comp√©tition Kaggle:** [Rejoindre la comp√©tition](https://www.kaggle.com/t/72daeb9bff104caf912f9a0b0f42eb5a)\n",
    "\n",
    "---\n",
    "\n",
    "## Contexte\n",
    "\n",
    "Hydro-Qu√©bec publie des donn√©es ouvertes sur la consommation √©lectrique de clients participant √† un programme de gestion de la demande. Ces donn√©es incluent la consommation horaire, les conditions m√©t√©orologiques, et des indicateurs d'√©v√©nements de pointe.\n",
    "\n",
    "Votre mission: construire un mod√®le de pr√©diction de la consommation √©nerg√©tique en utilisant **uniquement** les m√©thodes vues dans les chapitres 1 √† 5 du cours.\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "√Ä la fin de ce projet, vous serez en mesure de:\n",
    "\n",
    "1. Impl√©menter les moindres carr√©s ordinaires (OLS) √† partir de z√©ro\n",
    "2. Impl√©menter la r√©gression logistique avec descente de gradient\n",
    "3. Appliquer la r√©gularisation Ridge et interpr√©ter ses effets\n",
    "4. Construire un mod√®le √† deux √©tages: classification ‚Üí r√©gression\n",
    "5. Utiliser les probabilit√©s pr√©dites comme caract√©ristiques\n",
    "\n",
    "## √âvaluation\n",
    "\n",
    "| Composante | Pond√©ration | Description |\n",
    "|------------|-------------|-------------|\n",
    "| **Entrevue orale** | **60%** | V√©rification de la compr√©hension |\n",
    "| Code soumis | 20% | Compl√©tion des parties 1-7 |\n",
    "| Kaggle | 10% | Position au classement |\n",
    "| Rapport √©crit | 10% | Analyse et r√©flexion |\n",
    "\n",
    "### Bar√®me de l'entrevue orale (60%)\n",
    "\n",
    "| Crit√®re | Points | Ce qu'on √©value |\n",
    "|---------|--------|-----------------|\n",
    "| D√©rivation OLS au tableau | 15 | Ma√Ætrise de la solution analytique |\n",
    "| Explication descente de gradient | 10 | Compr√©hension des mises √† jour |\n",
    "| Justification des choix | 15 | Pourquoi ces caract√©ristiques? Pourquoi TimeSeriesSplit? |\n",
    "| Questions th√©oriques | 10 | Ridge = MAP, entropie crois√©e, etc. |\n",
    "| Modifications en direct | 10 | Adapter le code et pr√©dire les effets |\n",
    "\n",
    "**Important**: L'entrevue orale est la composante principale de l'√©valuation. Vous devez √™tre capable d'expliquer et de justifier chaque ligne de code que vous soumettez.\n",
    "\n",
    "### ‚ö†Ô∏è Avertissement sur l'utilisation d'outils IA\n",
    "\n",
    "Les outils comme ChatGPT, Cursor, Copilot peuvent vous aider, **mais** :\n",
    "- Vous devez comprendre **chaque ligne** de code que vous soumettez\n",
    "- L'entrevue orale r√©v√©lera rapidement si vous comprenez ou non\n",
    "- **60% de la note** d√©pend de votre capacit√© √† expliquer votre travail\n",
    "\n",
    "**Conseil** : Utilisez ces outils pour apprendre, pas pour √©viter d'apprendre. Du code copi√© sans compr√©hension m√®ne √† l'√©chec √† l'entrevue orale.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 0: Configuration et chargement des donn√©es\n",
    "\n",
    "Ex√©cutez cette cellule pour importer les biblioth√®ques et charger les donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration termin√©e!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "print(\"Configuration termin√©e!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des donn√©es\n",
    "\n",
    "Les donn√©es proviennent du jeu de donn√©es ouvert [consommation-clients-evenements-pointe](https://donnees.hydroquebec.com/explore/dataset/consommation-clients-evenements-pointe/) d'Hydro-Qu√©bec. Nous les chargeons directement depuis GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des donn√©es depuis GitHub...\n",
      "Ensemble d'entra√Ænement: 8246 observations\n",
      "Ensemble de test: 1754 observations\n",
      "\n",
      "P√©riode d'entra√Ænement: 2022-01-01 05:00:00+00:00 √† 2024-01-31 21:00:00+00:00\n",
      "P√©riode de test: 2024-02-01 01:00:00+00:00 √† 2024-07-01 03:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# URLs des donn√©es sur GitHub\n",
    "BASE_URL = \"https://raw.githubusercontent.com/pierrelux/mlbook/main/data/\"\n",
    "\n",
    "# Charger les donn√©es\n",
    "print(\"Chargement des donn√©es depuis GitHub...\")\n",
    "train = pd.read_csv(BASE_URL + \"energy_train.csv\", parse_dates=['horodatage_local'])\n",
    "\n",
    "# Pour l'√©valuation locale: test avec la cible (energie_kwh)\n",
    "test = pd.read_csv(BASE_URL + \"energy_test_avec_cible.csv\", parse_dates=['horodatage_local'])\n",
    "\n",
    "# Pour Kaggle: test sans la cible (pour g√©n√©rer les pr√©dictions)\n",
    "test_kaggle = pd.read_csv(BASE_URL + \"energy_test.csv\", parse_dates=['horodatage_local'])\n",
    "\n",
    "print(f\"Ensemble d'entra√Ænement: {len(train)} observations\")\n",
    "print(f\"Ensemble de test: {len(test)} observations\")\n",
    "print(f\"\\nP√©riode d'entra√Ænement: {train['horodatage_local'].min()} √† {train['horodatage_local'].max()}\")\n",
    "print(f\"P√©riode de test: {test['horodatage_local'].min()} √† {test['horodatage_local'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aper√ßu des donn√©es\n",
    "print(\"Colonnes disponibles:\")\n",
    "print(train.columns.tolist())\n",
    "print(f\"\\nProportion √©v√©nements de pointe (train): {train['evenement_pointe'].mean():.1%}\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description des variables\n",
    "\n",
    "Les donn√©es contiennent des mesures m√©t√©orologiques et temporelles pour pr√©dire la consommation √©nerg√©tique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description des variables\n",
    "print(\"Variables m√©t√©orologiques:\")\n",
    "print(\"  - temperature_ext: Temp√©rature ext√©rieure moyenne (¬∞C)\")\n",
    "print(\"  - humidite: Humidit√© relative moyenne (%)\")\n",
    "print(\"  - vitesse_vent: Vitesse du vent moyenne (km/h)\")\n",
    "print(\"  - neige: Pr√©cipitations de neige moyennes\")\n",
    "print(\"  - irradiance_solaire: Irradiance solaire moyenne\")\n",
    "\n",
    "print(\"\\nVariables temporelles:\")\n",
    "print(\"  - heure, mois, jour, jour_semaine: Composantes temporelles\")\n",
    "print(\"  - heure_sin, heure_cos, mois_sin, mois_cos: Encodage cyclique\")\n",
    "print(\"  - est_weekend, est_ferie: Indicateurs binaires\")\n",
    "\n",
    "print(\"\\nAutres:\")\n",
    "print(\"  - evenement_pointe: Indicateur d'√©v√©nement de pointe (classification)\")\n",
    "print(\"  - energie_kwh: Variable cible (consommation en kWh)\")\n",
    "\n",
    "print(f\"\\nStatistiques de base:\")\n",
    "train[['temperature_ext', 'humidite', 'energie_kwh']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Division temporelle d√©j√† effectu√©e\n",
    "# Les donn√©es de test couvrent la p√©riode √† partir du 1er f√©vrier 2024\n",
    "# NE PAS m√©langer les donn√©es - c'est une s√©rie temporelle!\n",
    "\n",
    "print(\"‚ö†Ô∏è  ATTENTION: Division temporelle\")\n",
    "print(\"Les ensembles train/test sont d√©j√† s√©par√©s chronologiquement.\")\n",
    "print(\"N'utilisez PAS de validation crois√©e al√©atoire (fuite d'information).\")\n",
    "print(\"\\nPour la validation, utilisez une division temporelle sur train:\")\n",
    "print(\"  - Ex: train[:6000] pour entra√Ænement, train[6000:] pour validation\")\n",
    "\n",
    "# Note: il y a un d√©calage de distribution entre train (hiver) et test (printemps/√©t√©)\n",
    "# C'est un d√©fi r√©aliste! Pensez √† utiliser des caract√©ristiques qui g√©n√©ralisent bien.\n",
    "print(\"\\nüìä D√©calage de distribution:\")\n",
    "print(f\"  Train: {train['energie_kwh'].mean():.1f} kWh (hiver)\")\n",
    "print(f\"  Test:  {test['energie_kwh'].mean():.1f} kWh (printemps/√©t√©)\")\n",
    "print(\"  ‚Üí Le mod√®le doit g√©n√©raliser √† travers les saisons!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration visuelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Consommation vs temp√©rature\n",
    "axes[0, 0].scatter(train['temperature_ext'], train['energie_kwh'], alpha=0.3, s=5)\n",
    "axes[0, 0].set_xlabel('Temp√©rature (¬∞C)')\n",
    "axes[0, 0].set_ylabel('√ânergie consomm√©e (kWh)')\n",
    "axes[0, 0].set_title('Consommation vs Temp√©rature')\n",
    "\n",
    "# Distribution de la consommation\n",
    "axes[0, 1].hist(train['energie_kwh'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('√ânergie (kWh)')\n",
    "axes[0, 1].set_ylabel('Fr√©quence')\n",
    "axes[0, 1].set_title('Distribution de la consommation')\n",
    "\n",
    "# Profil horaire\n",
    "profil_horaire = train.groupby('heure')['energie_kwh'].mean()\n",
    "axes[1, 0].bar(profil_horaire.index, profil_horaire.values)\n",
    "axes[1, 0].set_xlabel('Heure')\n",
    "axes[1, 0].set_ylabel('√ânergie moyenne (kWh)')\n",
    "axes[1, 0].set_title('Profil de consommation horaire')\n",
    "\n",
    "# √âv√©nements de pointe par heure\n",
    "pointe_horaire = train.groupby('heure')['evenement_pointe'].mean()\n",
    "axes[1, 1].bar(pointe_horaire.index, pointe_horaire.values)\n",
    "axes[1, 1].set_xlabel('Heure')\n",
    "axes[1, 1].set_ylabel('Proportion √©v√©nements de pointe')\n",
    "axes[1, 1].set_title('Fr√©quence des √©v√©nements de pointe')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ANALYSE EXPLORATOIRE APPROFONDIE\n",
    "# ============================================\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYSE STATISTIQUE D√âTAILL√âE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Matrice de corr√©lation des variables principales\n",
    "features_corr = ['temperature_ext', 'humidite', 'vitesse_vent', \n",
    "                 'irradiance_solaire', 'clients_connectes', 'energie_kwh']\n",
    "\n",
    "corr_matrix = train[features_corr].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Matrice de Corr√©lation - Variables Cl√©s', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 corr√©lations avec energie_kwh:\")\n",
    "corr_with_target = corr_matrix['energie_kwh'].sort_values(ascending=False)[1:6]\n",
    "print(corr_with_target)\n",
    "\n",
    "\n",
    "# 2. Distribution par saison et jour de la semaine\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Consommation par mois\n",
    "monthly_stats = train.groupby('mois')['energie_kwh'].agg(['mean', 'std'])\n",
    "axes[0, 0].bar(monthly_stats.index, monthly_stats['mean'], \n",
    "               yerr=monthly_stats['std'], capsize=5, alpha=0.7, color='steelblue')\n",
    "axes[0, 0].set_xlabel('Mois')\n",
    "axes[0, 0].set_ylabel('Consommation moyenne (kWh)')\n",
    "axes[0, 0].set_title('Variations Mensuelles (avec √©cart-type)')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Consommation par jour de la semaine\n",
    "weekly_stats = train.groupby('jour_semaine')['energie_kwh'].mean()\n",
    "days = ['Lun', 'Mar', 'Mer', 'Jeu', 'Ven', 'Sam', 'Dim']\n",
    "axes[0, 1].bar(range(7), weekly_stats, color='coral', alpha=0.7)\n",
    "axes[0, 1].set_xticks(range(7))\n",
    "axes[0, 1].set_xticklabels(days)\n",
    "axes[0, 1].set_ylabel('Consommation moyenne (kWh)')\n",
    "axes[0, 1].set_title('Variations Hebdomadaires')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Temp√©rature vs Consommation (avec r√©gression)\n",
    "axes[1, 0].scatter(train['temperature_ext'], train['energie_kwh'], \n",
    "                   alpha=0.2, s=3, c=train['heure'], cmap='viridis')\n",
    "# Ajouter courbe de tendance\n",
    "z = np.polyfit(train['temperature_ext'], train['energie_kwh'], 2)\n",
    "p = np.poly1d(z)\n",
    "temp_range = np.linspace(train['temperature_ext'].min(), \n",
    "                         train['temperature_ext'].max(), 100)\n",
    "axes[1, 0].plot(temp_range, p(temp_range), 'r-', linewidth=2, \n",
    "                label='Tendance polynomiale')\n",
    "axes[1, 0].set_xlabel('Temp√©rature (¬∞C)')\n",
    "axes[1, 0].set_ylabel('√ânergie (kWh)')\n",
    "axes[1, 0].set_title('Relation Temp√©rature-Consommation (color√© par heure)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# clients_connectes vs energie_kwh (tr√®s important!)\n",
    "axes[1, 1].scatter(train['clients_connectes'], train['energie_kwh'], \n",
    "                   alpha=0.3, s=5, c='green')\n",
    "axes[1, 1].set_xlabel('Nombre de clients connect√©s')\n",
    "axes[1, 1].set_ylabel('√ânergie (kWh)')\n",
    "axes[1, 1].set_title('Impact du Nombre de Clients (corr√©lation forte!)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 3. Analyse des √©v√©nements de pointe\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSE DES √âV√âNEMENTS DE POINTE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pointe_stats = train.groupby('evenement_pointe')['energie_kwh'].describe()\n",
    "print(\"\\nStatistiques consommation par type:\")\n",
    "print(pointe_stats)\n",
    "\n",
    "print(f\"\\nRatio consommation pointe/normal: \"\n",
    "      f\"{pointe_stats.loc[1, 'mean'] / pointe_stats.loc[0, 'mean']:.2f}x\")\n",
    "\n",
    "# Boxplot comparatif\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].boxplot([train[train['evenement_pointe']==0]['energie_kwh'],\n",
    "                 train[train['evenement_pointe']==1]['energie_kwh']],\n",
    "                labels=['Normal', 'Pointe'])\n",
    "axes[0].set_ylabel('Consommation (kWh)')\n",
    "axes[0].set_title('Distribution Consommation: Normal vs Pointe')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Heures de pointe par temp√©rature\n",
    "temp_bins = pd.cut(train['temperature_ext'], bins=5)\n",
    "pointe_by_temp = train.groupby([temp_bins, 'heure'])['evenement_pointe'].mean().unstack()\n",
    "\n",
    "im = axes[1].imshow(pointe_by_temp.T, cmap='YlOrRd', aspect='auto')\n",
    "axes[1].set_xlabel('Plage de temp√©rature')\n",
    "axes[1].set_ylabel('Heure')\n",
    "axes[1].set_title('Probabilit√© Pointe par Temp√©rature et Heure')\n",
    "plt.colorbar(im, ax=axes[1], label='P(pointe)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 4. Test de stationnarit√© (d√©calage train/test)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSE D√âCALAGE TRAIN/TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Variable': ['Temp√©rature', 'Humidit√©', 'Vent', '√ânergie', 'Pointe (%)'],\n",
    "    'Train (moyenne)': [\n",
    "        train['temperature_ext'].mean(),\n",
    "        train['humidite'].mean(),\n",
    "        train['vitesse_vent'].mean(),\n",
    "        train['energie_kwh'].mean(),\n",
    "        train['evenement_pointe'].mean()*100\n",
    "    ],\n",
    "    'Test (moyenne)': [\n",
    "        test['temperature_ext'].mean(),\n",
    "        test['humidite'].mean(),\n",
    "        test['vitesse_vent'].mean(),\n",
    "        test['energie_kwh'].mean(),\n",
    "        test['evenement_pointe'].mean()*100\n",
    "    ]\n",
    "})\n",
    "comparison['√âcart (%)'] = 100 * (comparison['Test (moyenne)'] - \n",
    "                                  comparison['Train (moyenne)']) / comparison['Train (moyenne)']\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"\\nObservation: Le d√©calage train/test est significatif!\")\n",
    "print(\"Strat√©gie: Utiliser features qui g√©n√©ralisent bien (degr√©-jours, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 1: Impl√©mentation OLS (10%)\n",
    "\n",
    "Avant d'utiliser scikit-learn, vous devez impl√©menter la solution analytique des moindres carr√©s ordinaires.\n",
    "\n",
    "**Rappel**: La solution OLS est donn√©e par:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "\n",
    "Pour des raisons de stabilit√© num√©rique, pr√©f√©rez `np.linalg.solve` √† l'inversion directe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_fit(X, y):\n",
    "    \"\"\"\n",
    "    Calcule les coefficients OLS.\n",
    "    \n",
    "    Param√®tres:\n",
    "        X : ndarray de forme (n, p) - matrice de caract√©ristiques (SANS colonne de 1)\n",
    "        y : ndarray de forme (n,) - vecteur cible\n",
    "    \n",
    "    Retourne:\n",
    "        beta : ndarray de forme (p+1,) - coefficients [intercept, coef1, coef2, ...]\n",
    "    \n",
    "    Indice: Ajoutez une colonne de 1 √† X pour l'intercept.\n",
    "    \"\"\"\n",
    "    # VOTRE CODE ICI\n",
    "    # 1. Ajouter une colonne de 1 pour l'intercept\n",
    "    n = X.shape[0]\n",
    "    X_with_intercept = np.column_stack((np.ones(n), X))\n",
    "    # 2. R√©soudre le syst√®me X^T X beta = X^T y\n",
    "    XTX = X_with_intercept.T @ X_with_intercept\n",
    "    XTy = X_with_intercept.T @ y\n",
    "    # 3. Retourner beta\n",
    "    beta = np.linalg.solve(XTX, XTy)\n",
    "    return beta    \n",
    "\n",
    "\n",
    "def ols_predict(X, beta):\n",
    "    \"\"\"\n",
    "    Pr√©dit avec les coefficients OLS.\n",
    "    \n",
    "    Param√®tres:\n",
    "        X : ndarray de forme (n, p) - caract√©ristiques (SANS colonne de 1)\n",
    "        beta : ndarray de forme (p+1,) - coefficients [intercept, coef1, ...]\n",
    "    \n",
    "    Retourne:\n",
    "        y_pred : ndarray de forme (n,)\n",
    "    \"\"\"\n",
    "    # VOTRE CODE ICI\n",
    "    n = X.shape[0]\n",
    "    X_with_intercept = np.column_stack((np.ones(n), X))\n",
    "    y_pred = X_with_intercept @ beta\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre impl√©mentation\n",
    "# Caract√©ristiques simples pour commencer\n",
    "features_base = ['temperature_ext', 'humidite', 'vitesse_vent']\n",
    "\n",
    "X_train_base = train[features_base].values\n",
    "y_train = train['energie_kwh'].values\n",
    "X_test_base = test[features_base].values\n",
    "y_test = test['energie_kwh'].values\n",
    "\n",
    "# Votre impl√©mentation\n",
    "beta_ols = ols_fit(X_train_base, y_train)\n",
    "y_pred_ols = ols_predict(X_test_base, beta_ols)\n",
    "\n",
    "# Validation avec sklearn\n",
    "model_sklearn = LinearRegression()\n",
    "model_sklearn.fit(X_train_base, y_train)\n",
    "y_pred_sklearn = model_sklearn.predict(X_test_base)\n",
    "\n",
    "# Comparaison\n",
    "print(\"Comparaison OLS impl√©ment√© vs sklearn:\")\n",
    "print(f\"  Intercept - Vous: {beta_ols[0]:.4f}, sklearn: {model_sklearn.intercept_:.4f}\")\n",
    "print(f\"  Coefficients proches: {np.allclose(beta_ols[1:], model_sklearn.coef_, atol=1e-4)}\")\n",
    "print(f\"\\nR¬≤ sur test: {r2_score(y_test, y_pred_ols):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE √Ä AJOUTER - Diagnostiques OLS\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIQUES OLS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Analyse des r√©sidus\n",
    "residus_ols = y_test - y_pred_ols\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# R√©sidus vs pr√©dictions\n",
    "axes[0, 0].scatter(y_pred_ols, residus_ols, alpha=0.4, s=10)\n",
    "axes[0, 0].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Pr√©dictions')\n",
    "axes[0, 0].set_ylabel('R√©sidus')\n",
    "axes[0, 0].set_title('R√©sidus vs Pr√©dictions (h√©t√©rosc√©dasticit√©?)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# QQ-plot (normalit√© des r√©sidus)\n",
    "from scipy import stats\n",
    "stats.probplot(residus_ols, dist=\"norm\", plot=axes[0, 1])\n",
    "axes[0, 1].set_title('Q-Q Plot (Test de Normalit√©)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogramme r√©sidus\n",
    "axes[1, 0].hist(residus_ols, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(residus_ols.mean(), color='red', linestyle='--',\n",
    "                   label=f'Moyenne: {residus_ols.mean():.2f}')\n",
    "axes[1, 0].set_xlabel('R√©sidu')\n",
    "axes[1, 0].set_ylabel('Fr√©quence')\n",
    "axes[1, 0].set_title('Distribution des R√©sidus')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Autocorr√©lation des r√©sidus (important pour s√©ries temporelles!)\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(pd.Series(residus_ols), ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Autocorr√©lation des R√©sidus')\n",
    "axes[1, 1].set_xlabel('Lag')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tests statistiques\n",
    "print(f\"\\nStatistiques r√©sidus:\")\n",
    "print(f\"  Moyenne: {residus_ols.mean():.4f} (devrait √™tre ~0)\")\n",
    "print(f\"  √âcart-type: {residus_ols.std():.2f}\")\n",
    "print(f\"  Min: {residus_ols.min():.2f}, Max: {residus_ols.max():.2f}\")\n",
    "\n",
    "# Test de Shapiro-Wilk (normalit√©)\n",
    "if len(residus_ols) < 5000:  # Limitation du test\n",
    "    stat, p_value = stats.shapiro(residus_ols[:5000])\n",
    "    print(f\"  Test Shapiro-Wilk: p-value = {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"    ‚Üí R√©sidus NON normaux (mais OK pour grandes donn√©es)\")\n",
    "\n",
    "# 2. Coefficients OLS\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"INTERPR√âTATION DES COEFFICIENTS OLS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': features_base,\n",
    "    'Coefficient': beta_ols[1:],\n",
    "    '|Coefficient|': np.abs(beta_ols[1:])\n",
    "}).sort_values('|Coefficient|', ascending=False)\n",
    "\n",
    "print(coef_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nIntercept: {beta_ols[0]:.2f} kWh\")\n",
    "print(\"\\nInterpr√©tation exemple:\")\n",
    "print(f\"  - {features_base[0]}: coefficient = {beta_ols[1]:.2f}\")\n",
    "print(f\"    ‚Üí +1¬∞C ‚Üí {beta_ols[1]:+.2f} kWh de consommation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 2: R√©gression logistique avec descente de gradient (15%)\n",
    "\n",
    "Impl√©mentez la r√©gression logistique pour la classification binaire.\n",
    "\n",
    "**Rappels**:\n",
    "- Fonction sigmo√Øde: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "- Perte d'entropie crois√©e: $L = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]$\n",
    "- Gradient: $\\nabla L = \\frac{1}{n} \\mathbf{X}^\\top (\\sigma(\\mathbf{X}\\boldsymbol{\\beta}) - \\mathbf{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Fonction sigmo√Øde.\n",
    "    \n",
    "    Indice: Pour la stabilit√© num√©rique, clip z entre -500 et 500.\n",
    "    \"\"\"\n",
    "    # VOTRE CODE ICI\n",
    "    z_clipped = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z_clipped))\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Calcule la perte d'entropie crois√©e binaire.\n",
    "    \n",
    "    Indice: Clip les probabilit√©s pour √©viter log(0).\n",
    "    \"\"\"\n",
    "    # VOTRE CODE ICI\n",
    "    epsilon = 1e-15\n",
    "    y_pred_proba_clipped = np.clip(y_pred_proba, epsilon, 1 - epsilon)\n",
    "    loss = -np.mean(y_true * np.log(y_pred_proba_clipped) + (1 - y_true) * np.log(1 - y_pred_proba_clipped))\n",
    "    return loss\n",
    "\n",
    "def logistic_gradient(X, y, beta):\n",
    "    \"\"\"\n",
    "    Calcule le gradient de la perte d'entropie crois√©e.\n",
    "    \n",
    "    Param√®tres:\n",
    "        X : ndarray (n, p+1) - caract√©ristiques AVEC colonne de 1\n",
    "        y : ndarray (n,) - √©tiquettes binaires\n",
    "        beta : ndarray (p+1,) - coefficients actuels\n",
    "    \n",
    "    Retourne:\n",
    "        gradient : ndarray (p+1,)\n",
    "    \"\"\"\n",
    "    # VOTRE CODE ICI\n",
    "    n = len(y)\n",
    "    z = X @ beta\n",
    "    y_pred_proba = sigmoid(z)\n",
    "    error = y_pred_proba - y\n",
    "    gradient = (X.T @ error) / n\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "def logistic_fit_gd(X, y, lr=0.1, n_iter=1000, verbose=False):\n",
    "    \"\"\"\n",
    "    Entra√Æne la r√©gression logistique par descente de gradient.\n",
    "    \n",
    "    Param√®tres:\n",
    "        X : ndarray (n, p) - caract√©ristiques SANS colonne de 1\n",
    "        y : ndarray (n,) - √©tiquettes binaires (0 ou 1)\n",
    "        lr : float - taux d'apprentissage\n",
    "        n_iter : int - nombre d'it√©rations\n",
    "        verbose : bool - afficher la progression\n",
    "    \n",
    "    Retourne:\n",
    "        beta : ndarray (p+1,) - coefficients [intercept, coef1, ...]\n",
    "        losses : list - historique des pertes\n",
    "    \"\"\"\n",
    "    # VOTRE CODE ICI\n",
    "    n, p = X.shape\n",
    "    # 1. Ajouter colonne de 1 √† X\n",
    "    X_with_intercept = np.column_stack([np.ones(n), X])\n",
    "    # 2. Initialiser beta √† z√©ro\n",
    "    beta = np.zeros(p + 1)\n",
    "    losses = []\n",
    "    # 3. Boucle de descente de gradient\n",
    "    for i in range(n_iter):\n",
    "        Z = X_with_intercept @ beta\n",
    "        y_pred_proba = sigmoid(Z)\n",
    "        \n",
    "        loss = cross_entropy_loss(y, y_pred_proba)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        gradient = logistic_gradient(X_with_intercept, y, beta)\n",
    "        \n",
    "        beta -= lr * gradient\n",
    "        \n",
    "        if verbose and (i % 100 == 0 or i == n_iter - 1):\n",
    "            print(f\"Iteration {i+1}/{n_iter}, Loss: {loss:.4f}\")\n",
    "    # 4. Retourner beta et historique des pertes\n",
    "    return beta, losses\n",
    "\n",
    "\n",
    "def logistic_predict_proba(X, beta):\n",
    "    \"\"\"\n",
    "    Retourne les probabilit√©s P(Y=1|X).\n",
    "    \"\"\"\n",
    "    # VOTRE CODE ICI\n",
    "    n = X.shape[0]\n",
    "    X_with_intercept = np.column_stack([np.ones(n), X])\n",
    "    z = X_with_intercept @ beta\n",
    "    return sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sur la pr√©diction des √©v√©nements de pointe\n",
    "# Caract√©ristiques pour classification\n",
    "features_clf = ['temperature_ext', 'heure_sin', 'heure_cos', 'est_weekend']\n",
    "\n",
    "X_train_clf = train[features_clf].values\n",
    "y_train_clf = train['evenement_pointe'].values\n",
    "X_test_clf = test[features_clf].values\n",
    "y_test_clf = test['evenement_pointe'].values\n",
    "\n",
    "# Normaliser (recommand√© pour la descente de gradient)\n",
    "scaler = StandardScaler()\n",
    "X_train_clf_scaled = scaler.fit_transform(X_train_clf)\n",
    "X_test_clf_scaled = scaler.transform(X_test_clf)\n",
    "\n",
    "# Entra√Æner votre mod√®le\n",
    "beta_log, losses = logistic_fit_gd(X_train_clf_scaled, y_train_clf, lr=0.1, n_iter=500, verbose=True)\n",
    "\n",
    "# Tracer la courbe de convergence\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('It√©ration')\n",
    "plt.ylabel('Perte (entropie crois√©e)')\n",
    "plt.title('Convergence de la descente de gradient')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation\n",
    "proba_train = logistic_predict_proba(X_train_clf_scaled, beta_log)\n",
    "proba_test = logistic_predict_proba(X_test_clf_scaled, beta_log)\n",
    "\n",
    "y_pred_train = (proba_train >= 0.5).astype(int)\n",
    "y_pred_test = (proba_test >= 0.5).astype(int)\n",
    "\n",
    "print(\"√âvaluation de votre r√©gression logistique:\")\n",
    "print(f\"  Accuracy (train): {accuracy_score(y_train_clf, y_pred_train):.4f}\")\n",
    "print(f\"  Accuracy (test): {accuracy_score(y_test_clf, y_pred_test):.4f}\")\n",
    "print(f\"\\nRapport de classification (test):\")\n",
    "print(classification_report(y_test_clf, y_pred_test, target_names=['Normal', 'Pointe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSE APPROFONDIE CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Matrice de confusion d√©taill√©e\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test_clf, y_pred_test)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Matrice de confusion\n",
    "im = axes[0].imshow(cm, cmap='Blues')\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_yticks([0, 1])\n",
    "axes[0].set_xticklabels(['Normal', 'Pointe'])\n",
    "axes[0].set_yticklabels(['Normal', 'Pointe'])\n",
    "axes[0].set_xlabel('Pr√©dit')\n",
    "axes[0].set_ylabel('R√©el')\n",
    "axes[0].set_title('Matrice de Confusion')\n",
    "\n",
    "# Annoter nombres\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = axes[0].text(j, i, cm[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"black\", fontsize=16)\n",
    "\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Calculer m√©triques\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\nM√©triques d√©taill√©es:\")\n",
    "print(f\"  True Negatives:  {tn}\")\n",
    "print(f\"  False Positives: {fp} (fausses alarmes)\")\n",
    "print(f\"  False Negatives: {fn} (pointes manqu√©es)\")\n",
    "print(f\"  True Positives:  {tp}\")\n",
    "print(f\"\\n  Precision: {precision:.4f} (Quand on pr√©dit pointe, c'est vrai dans {precision*100:.1f}% cas)\")\n",
    "print(f\"  Recall:    {recall:.4f} (On d√©tecte {recall*100:.1f}% des vraies pointes)\")\n",
    "print(f\"  F1-score:  {f1:.4f}\")\n",
    "\n",
    "\n",
    "# 2. Courbe ROC et AUC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test_clf, proba_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "axes[1].plot(fpr, tpr, color='darkorange', lw=2,\n",
    "            label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',\n",
    "            label='Hasard')\n",
    "axes[1].set_xlabel('Taux Faux Positifs (FPR)')\n",
    "axes[1].set_ylabel('Taux Vrais Positifs (TPR)')\n",
    "axes[1].set_title('Courbe ROC')\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "print(f\"\\n  AUC-ROC: {roc_auc:.4f} (1.0 = parfait, 0.5 = hasard)\")\n",
    "\n",
    "\n",
    "# 3. Precision-Recall selon seuil\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision_vals, recall_vals, thresholds_pr = precision_recall_curve(y_test_clf, proba_test)\n",
    "\n",
    "axes[2].plot(recall_vals, precision_vals, color='green', lw=2)\n",
    "axes[2].set_xlabel('Recall')\n",
    "axes[2].set_ylabel('Precision')\n",
    "axes[2].set_title('Courbe Precision-Recall')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Marquer le seuil 0.5\n",
    "idx_05 = np.argmin(np.abs(thresholds_pr - 0.5))\n",
    "axes[2].plot(recall_vals[idx_05], precision_vals[idx_05], 'ro',\n",
    "            markersize=10, label='Seuil = 0.5')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 4. Analyse par seuil\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EFFET DU SEUIL DE D√âCISION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "seuils_test = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "results_seuils = []\n",
    "\n",
    "for seuil in seuils_test:\n",
    "    y_pred_seuil = (proba_test >= seuil).astype(int)\n",
    "    acc = accuracy_score(y_test_clf, y_pred_seuil)\n",
    "    cm_seuil = confusion_matrix(y_test_clf, y_pred_seuil)\n",
    "    tn, fp, fn, tp = cm_seuil.ravel()\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    results_seuils.append({\n",
    "        'Seuil': seuil,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "df_seuils = pd.DataFrame(results_seuils)\n",
    "print(df_seuils.to_string(index=False))\n",
    "\n",
    "print(\"\\nInterpr√©tation:\")\n",
    "print(\"  - Seuil bas (0.3): Plus de d√©tections ‚Üí Recall √©lev√©, mais plus de fausses alarmes\")\n",
    "print(\"  - Seuil √©lev√© (0.7): Moins de fausses alarmes ‚Üí Precision √©lev√©e, mais pointes manqu√©es\")\n",
    "print(\"  - Trade-off selon contexte m√©tier!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 3: Ing√©nierie des caract√©ristiques (15%)\n",
    "\n",
    "**√Ä partir de maintenant, vous pouvez utiliser scikit-learn.**\n",
    "\n",
    "Cr√©ez des caract√©ristiques temporelles pour am√©liorer le mod√®le de r√©gression.\n",
    "\n",
    "### Caract√©ristiques √† impl√©menter:\n",
    "\n",
    "1. **Retards (lags)**: consommation aux heures pr√©c√©dentes\n",
    "2. **Statistiques glissantes**: moyenne mobile, √©cart-type mobile\n",
    "3. **Interactions**: temp√©rature √ó heure, etc.\n",
    "\n",
    "Impl√©mentez **au moins 3 nouvelles caract√©ristiques**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creer_caracteristiques(df):\n",
    "    \"\"\"\n",
    "    Cr√©e des caract√©ristiques suppl√©mentaires.\n",
    "    \n",
    "    VOUS DEVEZ IMPL√âMENTER AU MOINS 3 NOUVELLES CARACT√âRISTIQUES.\n",
    "    \n",
    "    Id√©es:\n",
    "    - Retards: df['energie_kwh'].shift(1), shift(24)\n",
    "    - Moyennes mobiles: df['energie_kwh'].rolling(6).mean()\n",
    "    - Interactions: df['temperature_ext'] * df['heure_cos']\n",
    "    - Degr√©-jours de chauffage: np.maximum(18 - df['temperature_ext'], 0)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # VOTRE CODE ICI\n",
    "    # Exemple:\n",
    "    df['energie_lag1'] = df['energie_kwh'].shift(1)\n",
    "    df['energie_lag24'] = df['energie_kwh'].shift(24)\n",
    "    df['energie_lag168'] = df['energie_kwh'].shift(168)\n",
    "    \n",
    "    df['energie_rolling_6h'] = df['energie_kwh'].rolling(6).mean()\n",
    "    df['energie_rolling_24h'] = df['energie_kwh'].rolling(24).mean()\n",
    "    df['energie_rolling_std_24h'] = df['energie_kwh'].rolling(\n",
    "        window=24, \n",
    "        min_periods=1\n",
    "    ).std().fillna(0)\n",
    "    df['energie_rolling_max_12h'] = df['energie_kwh'].rolling(\n",
    "        window = 12,\n",
    "        min_periods=1\n",
    "    ).max()\n",
    "    \n",
    "    df['temp_heure_cos'] = df['temperature_ext'] * df['heure_cos']\n",
    "    df['temp_heure_sin'] = df['temperature_ext'] * df['heure_sin']\n",
    "    df['temp_weekend'] = df['temperature_ext'] * df['est_weekend']\n",
    "    df['temp_mois_sin'] = df['temperature_ext'] * df['mois_sin']\n",
    "    df['temp_mois_cos'] = df['temperature_ext'] * df['mois_cos']\n",
    "    \n",
    "    df['degres_jours_chauffage'] = np.maximum(18 - df['temperature_ext'], 0)\n",
    "    df['degres_jours_clim'] = np.maximum(df['temperature_ext'] - 22, 0)\n",
    "    df['temp_squared'] = df['temperature_ext'] ** 2\n",
    "    df['temp_ressentie'] = df['temperature_ext'] - 0.5 * df['vitesse_vent']\n",
    "    df['humidite_temp'] = df['humidite'] * np.abs(df['temperature_ext']) / 100\n",
    "    df['est_pointe_matin'] = ((df['heure'] >= 7) & (df['heure'] <= 9)).astype(int)\n",
    "    \n",
    "    df['est_pointe_soir'] = ((df['heure'] >= 17) & (df['heure'] <= 20)).astype(int)\n",
    "    df['est_nuit'] = ((df['heure'] >= 0) & (df['heure'] <= 6)).astype(int)\n",
    "    df['est_hiver'] = df['mois'].isin([12, 1, 2]).astype(int)\n",
    "    df['est_ete'] = df['mois'].isin([6, 7, 8]).astype(int)\n",
    "    \n",
    "    df['temp_rolling_mean_3h'] = df['temperature_ext'].rolling(\n",
    "        window=3, \n",
    "        min_periods=1\n",
    "    ).mean()\n",
    "    df['temp_diff'] = df['temperature_ext'].diff().fillna(0)\n",
    "    df['temp_amplitude_24h'] = (\n",
    "        df['temperature_ext'].rolling(window=24, min_periods=1).max() - \n",
    "        df['temperature_ext'].rolling(window=24, min_periods=1).min()\n",
    "    )\n",
    "    \n",
    "    if 'clients_connectes' in df.columns:\n",
    "        df['clients_temp'] = df['clients_connectes'] * df['temperature_ext']\n",
    "        df['energie_per_client'] = df['energie_kwh'] / (df['clients_connectes'] + 1)\n",
    "        df['clients_weekend'] = df['clients_connectes'] * df['est_weekend']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Appliquer aux donn√©es\n",
    "train_eng = creer_caracteristiques(train)\n",
    "test_eng = creer_caracteristiques(test)\n",
    "\n",
    "# Supprimer les lignes avec NaN (dues aux retards)\n",
    "train_eng = train_eng.dropna().reset_index(drop=True)\n",
    "test_eng = test_eng.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"Nouvelles colonnes: {[c for c in train_eng.columns if c not in train.columns]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enrichi = creer_caracteristiques(train)\n",
    "test_enrichi = creer_caracteristiques(test)\n",
    "\n",
    "# Supprimer les NaN (dus aux lags/rolling)\n",
    "train_enrichi = train_enrichi.dropna().reset_index(drop=True)\n",
    "test_enrichi = test_enrichi.dropna().reset_index(drop=True)\n",
    "\n",
    "# V√©rifier les nouvelles colonnes\n",
    "nouvelles_cols = [c for c in train_enrichi.columns if c not in train.columns]\n",
    "print(f\"Nombre de nouvelles features: {len(nouvelles_cols)}\")\n",
    "print(f\"\\nNouvelles features cr√©√©es:\")\n",
    "for col in nouvelles_cols:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# V√©rifier corr√©lations avec la cible\n",
    "correlations = train_enrichi[nouvelles_cols + ['energie_kwh']].corr()['energie_kwh'].sort_values(ascending=False)\n",
    "print(f\"\\nTop 10 features par corr√©lation avec energie_kwh:\")\n",
    "print(correlations.head(10))\n",
    "\n",
    "features_to_use = [\n",
    "    # M√©t√©o de base\n",
    "    'temperature_ext', 'humidite', 'vitesse_vent', 'irradiance_solaire',\n",
    "    \n",
    "    # Temps cyclique\n",
    "    'heure_sin', 'heure_cos', 'mois_sin', 'mois_cos',\n",
    "    'jour_semaine_sin', 'jour_semaine_cos',\n",
    "    \n",
    "    # Indicateurs binaires\n",
    "    'est_weekend', 'est_ferie', 'est_pointe_matin', 'est_pointe_soir',\n",
    "    \n",
    "    # TR√àS IMPORTANT\n",
    "    'clients_connectes',\n",
    "    \n",
    "    # Lags (attention Kaggle!)\n",
    "    'energie_lag1', 'energie_lag24',\n",
    "    \n",
    "    # Rolling\n",
    "    'energie_rolling_mean_6h', 'energie_rolling_mean_24h',\n",
    "    \n",
    "    # Interactions\n",
    "    'temp_heure_cos', 'temp_weekend',\n",
    "    \n",
    "    # Transformations m√©t√©o\n",
    "    'degres_jours_chauffage', 'temp_squared'\n",
    "]\n",
    "\n",
    "# Filtrer celles qui existent vraiment\n",
    "features_disponibles = [f for f in features_to_use if f in train_enrichi.columns]\n",
    "\n",
    "print(f\"\\nFeatures s√©lectionn√©es: {len(features_disponibles)}\")\n",
    "\n",
    "X_train = train_enrichi[features_disponibles].values\n",
    "y_train = train_enrichi['energie_kwh'].values\n",
    "X_test = test_enrichi[features_disponibles].values\n",
    "y_test = test_enrichi['energie_kwh'].values\n",
    "\n",
    "# Entra√Æner un mod√®le simple pour tester\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print(f\"\\nR¬≤ avec features enrichies: {r2_score(y_test, model.predict(X_test)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ANALYSE D'IMPACT DES NOUVELLES FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Heatmap corr√©lations nouvelles features\n",
    "nouvelles_features_sample = nouvelles_cols[:15]  # Top 15\n",
    "corr_nouvelles = train_enrichi[nouvelles_features_sample + ['energie_kwh']].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_nouvelles, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, square=True, linewidths=0.5, cbar_kws={'label': 'Corr√©lation'})\n",
    "plt.title('Corr√©lations Nouvelles Features avec Cible', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Test incr√©mental d'ajout de features\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TEST INCR√âMENTAL - IMPACT PAR TYPE DE FEATURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Baseline: features de base\n",
    "features_baseline = ['temperature_ext', 'humidite', 'vitesse_vent',\n",
    "                     'heure_sin', 'heure_cos', 'mois_sin', 'mois_cos',\n",
    "                     'est_weekend', 'clients_connectes']\n",
    "\n",
    "# Test progressif\n",
    "feature_groups = {\n",
    "    'Baseline': features_baseline,\n",
    "    '+ Lags': features_baseline + ['energie_lag1', 'energie_lag24'],\n",
    "    '+ Rolling': features_baseline + ['energie_lag1', 'energie_lag24',\n",
    "                                      'energie_rolling_6h', 'energie_rolling_24h'],\n",
    "    '+ Interactions': features_baseline + ['energie_lag1', 'energie_lag24',\n",
    "                                           'energie_rolling_6h', 'energie_rolling_24h',\n",
    "                                           'temp_heure_cos', 'temp_weekend'],\n",
    "    '+ Transformations': features_baseline + ['energie_lag1', 'energie_lag24',\n",
    "                                               'energie_rolling_6h', 'energie_rolling_24h',\n",
    "                                               'temp_heure_cos', 'temp_weekend',\n",
    "                                               'degres_jours_chauffage', 'temp_squared']\n",
    "}\n",
    "\n",
    "results_incremental = []\n",
    "\n",
    "for name, feats in feature_groups.items():\n",
    "    feats_avail = [f for f in feats if f in train_enrichi.columns]\n",
    "    \n",
    "    X_tr = train_enrichi[feats_avail].values\n",
    "    y_tr = train_enrichi['energie_kwh'].values\n",
    "    X_te = test_enrichi[feats_avail].values\n",
    "    y_te = test_enrichi['energie_kwh'].values\n",
    "    \n",
    "    model_temp = Ridge(alpha=1.0)\n",
    "    model_temp.fit(X_tr, y_tr)\n",
    "    \n",
    "    r2_test = r2_score(y_te, model_temp.predict(X_te))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_te, model_temp.predict(X_te)))\n",
    "    \n",
    "    results_incremental.append({\n",
    "        'Configuration': name,\n",
    "        'Nb Features': len(feats_avail),\n",
    "        'R¬≤ Test': r2_test,\n",
    "        'RMSE Test': rmse_test\n",
    "    })\n",
    "\n",
    "df_incremental = pd.DataFrame(results_incremental)\n",
    "print(df_incremental.to_string(index=False))\n",
    "\n",
    "# Visualiser am√©lioration\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_incremental['Configuration'], df_incremental['R¬≤ Test'],\n",
    "         'o-', linewidth=2, markersize=10, color='steelblue')\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('R¬≤ Test')\n",
    "plt.title('Impact Incr√©mental des Features', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAm√©lioration totale: +{(df_incremental.iloc[-1]['R¬≤ Test'] - df_incremental.iloc[0]['R¬≤ Test']):.4f} R¬≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 4: R√©gression Ridge (15%)\n",
    "\n",
    "Avec plusieurs caract√©ristiques corr√©l√©es, la r√©gularisation devient utile.\n",
    "\n",
    "1. Entra√Ænez un mod√®le Ridge avec validation crois√©e pour choisir Œª\n",
    "2. Comparez les performances avec OLS\n",
    "3. Analysez comment les coefficients changent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finissez vos caract√©ristiques pour la r√©gression\n",
    "# MODIFIEZ CETTE LISTE selon vos caract√©ristiques cr√©√©es en Partie 3\n",
    "# IMPORTANT: clients_connectes est une variable tr√®s importante!\n",
    "features_reg = [\n",
    "    'temperature_ext', 'humidite', 'vitesse_vent', 'irradiance_solaire',\n",
    "    'heure_sin', 'heure_cos', 'mois_sin', 'mois_cos',\n",
    "    'jour_semaine_sin', 'jour_semaine_cos',\n",
    "    'est_weekend', 'est_ferie',\n",
    "    'clients_connectes',  # Ne pas oublier!\n",
    "    # Ajoutez vos caract√©ristiques ici\n",
    "]\n",
    "\n",
    "features_reg += features_to_use\n",
    "\n",
    "# V√©rifier que toutes les colonnes existent\n",
    "features_disponibles = [f for f in features_reg if f in train_eng.columns]\n",
    "print(f\"Caract√©ristiques utilis√©es: {len(features_disponibles)}\")\n",
    "\n",
    "X_train_reg = train_eng[features_disponibles].values\n",
    "y_train_reg = train_eng['energie_kwh'].values\n",
    "X_test_reg = test_eng[features_disponibles].values\n",
    "y_test_reg = test_eng['energie_kwh'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mod√®le OLS (baseline)\n",
    "model_ols = LinearRegression()\n",
    "model_ols.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "y_pred_ols_test = model_ols.predict(X_test_reg)\n",
    "y_pred_ols_train = model_ols.predict(X_train_reg)\n",
    "\n",
    "r2_ols_train = r2_score(y_train_reg, y_pred_ols_train)\n",
    "r2_ols_test = r2_score(y_test_reg, y_pred_ols_test)\n",
    "rmse_ols_test = np.sqrt(mean_squared_error(y_test_reg, y_pred_ols_test))\n",
    "\n",
    "print(\"OLS (baseline):\")\n",
    "print(f\"  R¬≤ train: {r2_ols_train:.4f}\")\n",
    "print(f\"  R¬≤ test:  {r2_ols_test:.4f}\")\n",
    "print(f\"  RMSE test: {rmse_ols_test:.4f}\")\n",
    "\n",
    "if r2_ols_train - r2_ols_test > 0.1:\n",
    "    print(\" Attention: surapprentissage d√©tect√© avec OLS!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mod√®le Ridge avec validation crois√©e\n",
    "# ATTENTION: Utilisez TimeSeriesSplit pour les donn√©es temporelles!\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "alphas = [0.01, 0.1, 1, 10, 100, 1000]\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "model_ridge = RidgeCV(alphas=alphas, cv=tscv)\n",
    "model_ridge.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "y_pred_ridge_train = model_ridge.predict(X_train_reg)\n",
    "y_pred_ridge_test = model_ridge.predict(X_test_reg)\n",
    "\n",
    "r2_ridge_train = r2_score(y_train_reg, y_pred_ridge_train)\n",
    "r2_ridge_test = r2_score(y_test_reg, y_pred_ridge_test)\n",
    "rmse_ridge_test = np.sqrt(mean_squared_error(y_test_reg, y_pred_ridge_test))\n",
    "\n",
    "print(f\"\\nRidge (Œª={model_ridge.alpha_}):\")\n",
    "print(f\"  R¬≤ train: {r2_ridge_train:.4f}\")\n",
    "print(f\"  R¬≤ test:  {r2_ridge_test:.4f}\")\n",
    "print(f\"  RMSE test: {rmse_ridge_test:.4f}\")\n",
    "print(f\"  √âcart R¬≤ train-test: {r2_ridge_train - r2_ridge_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des coefficients OLS vs Ridge\n",
    "coef_comparison = pd.DataFrame({\n",
    "    'Caract√©ristique': features_disponibles,\n",
    "    'OLS': model_ols.coef_,\n",
    "    'Ridge': model_ridge.coef_\n",
    "})\n",
    "coef_comparison['R√©duction (%)'] = 100 * (1 - np.abs(coef_comparison['Ridge']) / (np.abs(coef_comparison['OLS']) + 1e-8))\n",
    "coef_comparison = coef_comparison.sort_values('R√©duction (%)', ascending=False)\n",
    "\n",
    "print(\"\\nComparaison des coefficients (tri√©s par r√©duction):\")\n",
    "print(coef_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Mod√®le': ['OLS', 'Ridge (Œª=1)', f'Ridge (Œª={model_ridge.alpha_})'],\n",
    "    'R¬≤ train': [r2_ols_train, r2_ridge_train, r2_ridge_train],\n",
    "    'R¬≤ test': [r2_ols_test, r2_ridge_test, r2_ridge_test],\n",
    "    'RMSE test': [rmse_ols_test, rmse_ridge_test, rmse_ridge_test],\n",
    "    '√âcart': [abs(r2_ols_train - r2_ols_test),\n",
    "              abs(r2_ridge_train - r2_ridge_test),\n",
    "              abs(r2_ridge_train - r2_ridge_test)]\n",
    "})\n",
    "\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Meilleur mod√®le\n",
    "best_idx = results['R¬≤ test'].idxmax()\n",
    "print(f\"\\n Meilleur mod√®le: {results.loc[best_idx, 'Mod√®le']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyse des coefficients du meilleur mod√®le\n",
    "coef_comparison = pd.DataFrame({\n",
    "    'Feature': features_disponibles,\n",
    "    'OLS': model_ols.coef_,\n",
    "    'Ridge': model_ridge.coef_\n",
    "})\n",
    "\n",
    "# Calculer r√©duction (shrinkage)\n",
    "coef_comparison['R√©duction (%)'] = 100 * (\n",
    "    1 - np.abs(coef_comparison['Ridge']) / (np.abs(coef_comparison['OLS']) + 1e-8)\n",
    ")\n",
    "\n",
    "# Trier par r√©duction\n",
    "coef_comparison = coef_comparison.sort_values('R√©duction (%)', ascending=False)\n",
    "\n",
    "print(coef_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n Observations:\")\n",
    "print(f\"  - R√©duction moyenne: {coef_comparison['R√©duction (%)'].mean():.1f}%\")\n",
    "print(f\"  - R√©duction max: {coef_comparison['R√©duction (%)'].max():.1f}%\")\n",
    "print(f\"  - Feature la plus r√©duite: {coef_comparison.iloc[0]['Feature']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester plusieurs Œª\n",
    "lambdas_test = np.logspace(-2, 4, 50)  # 0.01 √† 10000\n",
    "coefficients_path = []\n",
    "\n",
    "for lam in lambdas_test:\n",
    "    model_temp = Ridge(alpha=lam)\n",
    "    model_temp.fit(X_train, y_train)\n",
    "    coefficients_path.append(model_temp.coef_)\n",
    "\n",
    "coefficients_path = np.array(coefficients_path)\n",
    "\n",
    "# Tracer\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, feature in enumerate(features_disponibles[:10]):  # 10 premi√®res features\n",
    "    plt.plot(lambdas_test, coefficients_path[:, i], label=feature, linewidth=2)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Œª (√©chelle log)', fontsize=12)\n",
    "plt.ylabel('Coefficient', fontsize=12)\n",
    "plt.title('Chemin de R√©gularisation Ridge', fontsize=14, fontweight='bold')\n",
    "plt.axvline(model_ridge.alpha_, color='red', linestyle='--',\n",
    "            linewidth=2, label=f'Œª optimal = {model_ridge.alpha_}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train_list = []\n",
    "r2_test_list = []\n",
    "\n",
    "for lam in lambdas_test:\n",
    "    model_temp = Ridge(alpha=lam)\n",
    "    model_temp.fit(X_train, y_train)\n",
    "\n",
    "    r2_train_list.append(r2_score(y_train, model_temp.predict(X_train)))\n",
    "    r2_test_list.append(r2_score(y_test, model_temp.predict(X_test)))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lambdas_test, r2_train_list, label='R¬≤ train', linewidth=2, color='blue')\n",
    "plt.plot(lambdas_test, r2_test_list, label='R¬≤ test', linewidth=2, color='orange')\n",
    "plt.axvline(model_ridge.alpha_, color='red', linestyle='--',\n",
    "            linewidth=2, label=f'Œª optimal = {model_ridge.alpha_}')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Œª (√©chelle log)', fontsize=12)\n",
    "plt.ylabel('R¬≤', fontsize=12)\n",
    "plt.title('Courbe de Validation Ridge', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions pour l'entrevue orale**:\n",
    "- Pourquoi Ridge aide-t-il quand les caract√©ristiques sont corr√©l√©es?\n",
    "- Quelle caract√©ristique a √©t√© la plus r√©duite? Pourquoi?\n",
    "- Comment interpr√©ter Ridge comme estimation MAP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 5: Sous-t√¢che de classification (15%)\n",
    "\n",
    "Entra√Ænez un classifieur pour pr√©dire les √©v√©nements de pointe, puis utilisez la probabilit√© pr√©dite comme caract√©ristique pour la r√©gression.\n",
    "\n",
    "**√âtapes**:\n",
    "1. Entra√Æner LogisticRegression sur `evenement_pointe`\n",
    "2. Extraire `P(pointe)` pour chaque observation\n",
    "3. Ajouter cette probabilit√© comme caract√©ristique pour Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caract√©ristiques pour la classification\n",
    "# Utilisez des caract√©ristiques qui ne \"trichent\" pas (pas de consommation pass√©e pour pr√©dire la pointe)\n",
    "features_pointe = ['temperature_ext', 'humidite', 'vitesse_vent', 'heure_sin', 'heure_cos', 'est_weekend', 'clients_connectes']\n",
    "\n",
    "X_train_pointe = train_eng[features_pointe].values\n",
    "y_train_pointe = train_eng['evenement_pointe'].values\n",
    "X_test_pointe = test_eng[features_pointe].values\n",
    "y_test_pointe = test_eng['evenement_pointe'].values\n",
    "\n",
    "# Entra√Æner le classifieur\n",
    "clf_pointe = LogisticRegression(max_iter=1000)\n",
    "clf_pointe.fit(X_train_pointe, y_train_pointe)\n",
    "\n",
    "# √âvaluation\n",
    "print(\"Classification des √©v√©nements de pointe:\")\n",
    "print(f\"  Accuracy (train): {clf_pointe.score(X_train_pointe, y_train_pointe):.4f}\")\n",
    "print(f\"  Accuracy (test): {clf_pointe.score(X_test_pointe, y_test_pointe):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les probabilit√©s\n",
    "train_eng['P_pointe'] = clf_pointe.predict_proba(X_train_pointe)[:, 1]\n",
    "test_eng['P_pointe'] = clf_pointe.predict_proba(X_test_pointe)[:, 1]\n",
    "\n",
    "print(f\"Distribution de P(pointe):\")\n",
    "print(f\"  Train: moyenne={train_eng['P_pointe'].mean():.3f}, std={train_eng['P_pointe'].std():.3f}\")\n",
    "print(f\"  Test:  moyenne={test_eng['P_pointe'].mean():.3f}, std={test_eng['P_pointe'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question pour l'entrevue**: Pourquoi utiliser P(pointe) au lieu d'un indicateur 0/1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ANALYSE CALIBRATION ET DISTRIBUTION P(pointe)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Calibration plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Diviser en bins de probabilit√©\n",
    "n_bins = 10\n",
    "bins = np.linspace(0, 1, n_bins + 1)\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Calculer fraction r√©elle par bin\n",
    "bin_indices = np.digitize(proba_test, bins) - 1\n",
    "bin_indices = np.clip(bin_indices, 0, n_bins - 1)\n",
    "\n",
    "fraction_positive = np.zeros(n_bins)\n",
    "count_per_bin = np.zeros(n_bins)\n",
    "\n",
    "for i in range(n_bins):\n",
    "    mask = bin_indices == i\n",
    "    if mask.sum() > 0:\n",
    "        fraction_positive[i] = y_test_clf[mask].mean()\n",
    "        count_per_bin[i] = mask.sum()\n",
    "\n",
    "# Calibration curve\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Parfaitement calibr√©')\n",
    "axes[0].plot(bin_centers, fraction_positive, 'o-', linewidth=2,\n",
    "            label='Notre mod√®le', markersize=8)\n",
    "axes[0].set_xlabel('Probabilit√© pr√©dite')\n",
    "axes[0].set_ylabel('Fraction r√©elle de pointes')\n",
    "axes[0].set_title('Courbe de Calibration')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution P(pointe) par classe r√©elle\n",
    "axes[1].hist(proba_test[y_test_clf==0], bins=30, alpha=0.5,\n",
    "            label='Normal (classe 0)', color='blue', density=True)\n",
    "axes[1].hist(proba_test[y_test_clf==1], bins=30, alpha=0.5,\n",
    "            label='Pointe (classe 1)', color='red', density=True)\n",
    "axes[1].axvline(0.5, color='black', linestyle='--', label='Seuil 0.5')\n",
    "axes[1].set_xlabel('P(pointe)')\n",
    "axes[1].set_ylabel('Densit√©')\n",
    "axes[1].set_title('Distribution P(pointe) par Classe R√©elle')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Histogramme 2D\n",
    "axes[2].hist2d(test_eng['temperature_ext'], test_eng['P_pointe'],\n",
    "              bins=30, cmap='YlOrRd')\n",
    "axes[2].set_xlabel('Temp√©rature (¬∞C)')\n",
    "axes[2].set_ylabel('P(pointe)')\n",
    "axes[2].set_title('P(pointe) vs Temp√©rature')\n",
    "plt.colorbar(axes[2].collections[0], ax=axes[2], label='Fr√©quence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Analyse segment√©e\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"P(pointe) PAR SEGMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "segments = {\n",
    "    'Train': train_eng['P_pointe'],\n",
    "    'Test': test_eng['P_pointe'],\n",
    "    'Pointe r√©elle': test_eng[test_eng['evenement_pointe']==1]['P_pointe'],\n",
    "    'Normal r√©el': test_eng[test_eng['evenement_pointe']==0]['P_pointe']\n",
    "}\n",
    "\n",
    "for name, data in segments.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Min:  {data.min():.3f}\")\n",
    "    print(f\"  Q25:  {data.quantile(0.25):.3f}\")\n",
    "    print(f\"  M√©diane: {data.median():.3f}\")\n",
    "    print(f\"  Q75:  {data.quantile(0.75):.3f}\")\n",
    "    print(f\"  Max:  {data.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 6: Mod√®le combin√© (10%)\n",
    "\n",
    "Assemblez le mod√®le final en ajoutant `P_pointe` comme caract√©ristique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caract√©ristiques finales (avec P_pointe)\n",
    "features_final = features_disponibles + ['P_pointe']\n",
    "\n",
    "X_train_final = train_eng[features_final].values\n",
    "y_train_final = train_eng['energie_kwh'].values\n",
    "X_test_final = test_eng[features_final].values\n",
    "y_test_final = test_eng['energie_kwh'].values\n",
    "\n",
    "# Mod√®le Ridge final\n",
    "model_final = RidgeCV(alphas=[0.1, 1, 10, 100], cv=TimeSeriesSplit(n_splits=5))\n",
    "model_final.fit(X_train_final, y_train_final)\n",
    "y_pred_final = model_final.predict(X_test_final)\n",
    "\n",
    "print(\"Mod√®le final (Ridge + P_pointe):\")\n",
    "print(f\"  Œª s√©lectionn√©: {model_final.alpha_}\")\n",
    "print(f\"  R¬≤ train: {model_final.score(X_train_final, y_train_final):.4f}\")\n",
    "print(f\"  R¬≤ test:  {r2_score(y_test_final, y_pred_final):.4f}\")\n",
    "print(f\"  RMSE test: {np.sqrt(mean_squared_error(y_test_final, y_pred_final)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des r√©sidus\n",
    "residus = y_test_final - y_pred_final\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Histogramme des r√©sidus\n",
    "axes[0].hist(residus, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(0, color='red', linestyle='--', label='Z√©ro')\n",
    "axes[0].set_xlabel('R√©sidu')\n",
    "axes[0].set_ylabel('Fr√©quence')\n",
    "axes[0].set_title('Distribution des r√©sidus')\n",
    "axes[0].legend()\n",
    "\n",
    "# Pr√©dictions vs r√©el\n",
    "axes[1].scatter(y_test_final, y_pred_final, alpha=0.3, s=5)\n",
    "axes[1].plot([y_test_final.min(), y_test_final.max()], \n",
    "             [y_test_final.min(), y_test_final.max()], 'r--', label='Parfait')\n",
    "axes[1].set_xlabel('√ânergie r√©elle (kWh)')\n",
    "axes[1].set_ylabel('√ânergie pr√©dite (kWh)')\n",
    "axes[1].set_title('Pr√©dictions vs R√©el')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ANALYSE APPROFONDIE DES ERREURS\")\n",
    "print(\"=\"*60)\n",
    "print(\"asdassadsad\")\n",
    "\n",
    "# 1. Erreurs par segment\n",
    "residus_final = y_test_final - y_pred_final\n",
    "test_analysis = test_eng.copy()\n",
    "test_analysis['residus'] = residus_final\n",
    "test_analysis['erreur_abs'] = np.abs(residus_final)\n",
    "test_analysis['erreur_pct'] = 100 * np.abs(residus_final) / (y_test_final + 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Erreurs par heure\n",
    "erreurs_heure = test_analysis.groupby('heure')['erreur_abs'].mean()\n",
    "axes[0, 0].bar(erreurs_heure.index, erreurs_heure.values, color='coral')\n",
    "axes[0, 0].set_xlabel('Heure')\n",
    "axes[0, 0].set_ylabel('Erreur absolue moyenne (kWh)')\n",
    "axes[0, 0].set_title('Erreurs par Heure de la Journ√©e')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Erreurs par temp√©rature\n",
    "temp_bins = pd.cut(test_analysis['temperature_ext'], bins=8)\n",
    "erreurs_temp = test_analysis.groupby(temp_bins)['erreur_abs'].mean()\n",
    "axes[0, 1].bar(range(len(erreurs_temp)), erreurs_temp.values, color='steelblue')\n",
    "axes[0, 1].set_xlabel('Plage de temp√©rature')\n",
    "axes[0, 1].set_ylabel('Erreur absolue moyenne (kWh)')\n",
    "axes[0, 1].set_title('Erreurs par Temp√©rature')\n",
    "axes[0, 1].set_xticklabels([f\"{int(i.left)}-{int(i.right)}\" \n",
    "                             for i in erreurs_temp.index], rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Erreurs par type (pointe vs normal)\n",
    "erreurs_type = test_analysis.groupby('evenement_pointe')['erreur_abs'].mean()\n",
    "axes[0, 2].bar(['Normal', 'Pointe'], erreurs_type.values, color=['green', 'red'])\n",
    "axes[0, 2].set_ylabel('Erreur absolue moyenne (kWh)')\n",
    "axes[0, 2].set_title('Erreurs: Normal vs Pointe')\n",
    "axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "print(f\"\\nErreurs moyennes:\")\n",
    "print(f\"  Normal: {erreurs_type[0]:.2f} kWh\")\n",
    "print(f\"  Pointe: {erreurs_type[1]:.2f} kWh\")\n",
    "print(f\"  Ratio pointe/normal: {erreurs_type[1]/erreurs_type[0]:.2f}x\")\n",
    "\n",
    "# R√©sidus vs features importantes\n",
    "axes[1, 0].scatter(test_analysis['temperature_ext'], residus_final, \n",
    "                   alpha=0.4, s=10, c=test_analysis['heure'], cmap='viridis')\n",
    "axes[1, 0].axhline(0, color='red', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Temp√©rature (¬∞C)')\n",
    "axes[1, 0].set_ylabel('R√©sidu (kWh)')\n",
    "axes[1, 0].set_title('R√©sidus vs Temp√©rature (color√© par heure)')\n",
    "plt.colorbar(axes[1, 0].collections[0], ax=axes[1, 0], label='Heure')\n",
    "\n",
    "axes[1, 1].scatter(test_analysis['clients_connectes'], residus_final,\n",
    "                   alpha=0.4, s=10, color='purple')\n",
    "axes[1, 1].axhline(0, color='red', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Clients connect√©s')\n",
    "axes[1, 1].set_ylabel('R√©sidu (kWh)')\n",
    "axes[1, 1].set_title('R√©sidus vs Nombre de Clients')\n",
    "\n",
    "# Top 10 pires pr√©dictions\n",
    "axes[1, 2].axis('off')\n",
    "pires = test_analysis.nlargest(10, 'erreur_abs')[\n",
    "    ['heure', 'temperature_ext', 'evenement_pointe', 'erreur_abs']\n",
    "]\n",
    "table_text = \"TOP 10 PIRES PR√âDICTIONS\\n\\n\"\n",
    "table_text += pires.to_string(index=False, float_format='%.1f')\n",
    "axes[1, 2].text(0.1, 0.5, table_text, fontsize=9, \n",
    "                family='monospace', verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. M√©triques par quantile de consommation\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PERFORMANCE PAR NIVEAU DE CONSOMMATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_analysis['consommation_level'] = pd.qcut(y_test_final, q=4,\n",
    "                                               labels=['Faible', 'Moyen', '√âlev√©', 'Tr√®s √©lev√©'])\n",
    "\n",
    "perf_by_level = test_analysis.groupby('consommation_level').agg({\n",
    "    'erreur_abs': 'mean',\n",
    "    'erreur_pct': 'mean',\n",
    "    'residus': lambda x: r2_score(y_test_final[x.index], \n",
    "                                   y_pred_final[x.index])\n",
    "}).round(2)\n",
    "perf_by_level.columns = ['MAE (kWh)', 'MAPE (%)', 'R¬≤']\n",
    "\n",
    "print(perf_by_level)\n",
    "\n",
    "print(\"\\nConstat: Le mod√®le est-il meilleur sur certaines plages?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 7: Extension (10%) - Choisir UNE option\n",
    "\n",
    "### Option A: Donn√©es m√©t√©orologiques externes\n",
    "Utilisez la biblioth√®que `meteostat` pour ajouter des donn√©es m√©t√©o suppl√©mentaires (ex: pression atmosph√©rique, point de ros√©e).\n",
    "\n",
    "### Option B: Classification multiclasse\n",
    "Au lieu de binaire (pointe/normal), cr√©ez 3+ classes de consommation (faible/moyenne/√©lev√©e) et utilisez softmax.\n",
    "\n",
    "### Option C: Analyse d'erreur approfondie\n",
    "Identifiez quand le mod√®le fait le plus d'erreurs et proposez des am√©liorations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOTRE EXTENSION ICI\n",
    "# Indiquez quelle option vous avez choisie et pourquoi.\n",
    "\n",
    "# Option choisie: ___\n",
    "# Justification: ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Kaggle Score Simulation & Diagnostics\n",
    "\n",
    "The two cells below serve two purposes:\n",
    "1. **Kaggle Simulation**: Replicate exactly what happens when Kaggle evaluates your submission (RMSE on `energy_test.csv` targets). This uses the clean model only (no energy lags).\n",
    "2. **Diagnostic Output**: Print a structured text block with all the information needed to diagnose performance issues. Copy-paste the output for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf_pointe not found, skipping P_pointe.\n",
      "Ridge features: 43\n",
      "KNN features (Poste C): 11\n",
      "Postes: ['A', 'B', 'C']\n",
      "Model types: {'A': 'ridge', 'B': 'ridge', 'C': 'knn'}\n",
      "\n",
      "======================================================================\n",
      "Per-poste model training (v5: Ridge A/B + KNN C)\n",
      "======================================================================\n",
      "\n",
      "  Poste A [RIDGE]: alpha=500.0, n_feat=43, n_tr=1751, n_te=474\n",
      "    RMSE_tr=25.89, RMSE_te=16.77, R2_tr=0.8294, R2_te=0.2812\n",
      "\n",
      "  Poste B [RIDGE]: alpha=50.0, n_feat=43, n_tr=366, n_te=1126\n",
      "    RMSE_tr=18.66, RMSE_te=40.02, R2_tr=0.7449, R2_te=-0.2285\n",
      "\n",
      "  Poste C [KNN]:   best_k=1500 (CV RMSE=170.73), n_feat=11, n_tr=6129, n_te=154\n",
      "    RMSE_tr=3.13, RMSE_te=127.80, R2_tr=0.9998, R2_te=-1.1796\n",
      "    k search results:\n",
      "      k=    3: CV_RMSE=238.42\n",
      "      k=    5: CV_RMSE=236.45\n",
      "      k=   10: CV_RMSE=227.95\n",
      "      k=   20: CV_RMSE=224.08\n",
      "      k=   50: CV_RMSE=220.97\n",
      "      k=  100: CV_RMSE=219.27\n",
      "      k=  200: CV_RMSE=218.99\n",
      "      k=  500: CV_RMSE=221.86\n",
      "      k=  750: CV_RMSE=226.14\n",
      "      k= 1000: CV_RMSE=230.82\n",
      "      k= 1500: CV_RMSE=170.73 <-- best\n",
      "      k= 2000: CV_RMSE=177.63\n",
      "      k= 2500: CV_RMSE=182.50\n",
      "      k= 3000: CV_RMSE=186.97\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "KAGGLE SCORE SIMULATION (v5: Ridge A/B + KNN C)\n",
      "======================================================================\n",
      "  Models:      A=RIDGE, B=RIDGE, C=KNN\n",
      "  Features:    A=43, B=43, C=11\n",
      "  Rows:        1754 (expected 1754)\n",
      "\n",
      "  RMSE:  50.3819 kWh   <-- Kaggle RMSE\n",
      "  MAE:   35.8591 kWh\n",
      "  R2:    0.4929\n",
      "\n",
      "  Pred stats:  min=11.45, mean=113.35, max=418.95\n",
      "  Truth stats: min=17.76, mean=83.74, max=538.48\n",
      "\n",
      "  vs v4 (Ridge-only): RMSE was 63.51, R2 was 0.19\n",
      "  Improvement: -13.13 kWh (-20.7%)\n",
      "\n",
      "  Per-poste RMSE (v4 ‚Üí v5):\n",
      "    Poste A [RIDGE]: RMSE=16.77 (v4=16.74, Œî=+0.03), R2=0.2812, bias=+2.40, n=474\n",
      "    Poste B [RIDGE]: RMSE=40.02 (v4=44.55, Œî=-4.53), R2=-0.2285, bias=-33.28, n=1126\n",
      "    Poste C [KNN  ]: RMSE=127.80 (v4=174.83, Œî=-47.03), R2=-1.1796, bias=-101.32, n=154\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CELL A v5: KAGGLE SCORE SIMULATION ‚Äî Ridge (A/B) + KNN Regression (C)\n",
    "# ================================================================\n",
    "# v4 achieved RMSE=63.51 but Ridge barely beats the per-poste train mean\n",
    "# baseline (63.37). Deep diagnostics showed:\n",
    "#   - Poste C: Ridge overpredicts by +161 kWh (linear extrapolation fails)\n",
    "#   - Poste B: Ridge bias=-29.51 (seasonal shift winter‚Üíspring/summer)\n",
    "#   - GradientBoosting would give 49.06 but is NOT ALLOWED (course constraint)\n",
    "#\n",
    "# v5 strategy (methods from chapters 1-5 + knn.md only):\n",
    "#   1. Poste A: Ridge (keep v4, works well ‚Äî RMSE=16.74)\n",
    "#   2. Poste B: Ridge (keep v4, KNN didn't improve B)\n",
    "#   3. Poste C: KNN regression with weather-only features + large k\n",
    "#      - KNN finds summer 2022/2023 neighbors instead of extrapolating\n",
    "#      - Weather-only features avoid infrastructure drift (clients_connectes)\n",
    "#      - Large k (~1500) acts as a smoothed local average (Nadaraya-Watson style)\n",
    "#   4. K tuned by TimeSeriesSplit CV over a wide search grid\n",
    "#\n",
    "# Requires: train, test, clf_pointe (from earlier cells)\n",
    "# ================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. FEATURE ENGINEERING v3 (unchanged ‚Äî used for Ridge models A/B)\n",
    "# -------------------------------------------------------------------\n",
    "def creer_caracteristiques_v3(df):\n",
    "    \"\"\"\n",
    "    Feature engineering for a SINGLE poste's data.\n",
    "    Data is sorted by time so shift/rolling operate correctly.\n",
    "    \"\"\"\n",
    "    df = df.sort_values('horodatage_local').copy()\n",
    "\n",
    "    # Weather x time interactions\n",
    "    df['temp_heure_cos'] = df['temperature_ext'] * df['heure_cos']\n",
    "    df['temp_heure_sin'] = df['temperature_ext'] * df['heure_sin']\n",
    "    df['temp_weekend'] = df['temperature_ext'] * df['est_weekend']\n",
    "    df['temp_mois_sin'] = df['temperature_ext'] * df['mois_sin']\n",
    "    df['temp_mois_cos'] = df['temperature_ext'] * df['mois_cos']\n",
    "\n",
    "    # Degree-days\n",
    "    df['degres_jours_chauffage'] = np.maximum(18 - df['temperature_ext'], 0)\n",
    "    df['degres_jours_clim'] = np.maximum(df['temperature_ext'] - 22, 0)\n",
    "\n",
    "    # Weather transforms\n",
    "    df['temp_squared'] = df['temperature_ext'] ** 2\n",
    "    df['temp_ressentie'] = df['temperature_ext'] - 0.5 * df['vitesse_vent']\n",
    "    df['humidite_temp'] = df['humidite'] * np.abs(df['temperature_ext']) / 100\n",
    "\n",
    "    # Time indicators\n",
    "    df['est_pointe_matin'] = ((df['heure'] >= 7) & (df['heure'] <= 9)).astype(int)\n",
    "    df['est_pointe_soir'] = ((df['heure'] >= 17) & (df['heure'] <= 20)).astype(int)\n",
    "    df['est_nuit'] = ((df['heure'] >= 0) & (df['heure'] <= 6)).astype(int)\n",
    "\n",
    "    # Weather lags (correct: single-poste, sorted by time)\n",
    "    df['temp_lag1'] = df['temperature_ext'].shift(1).fillna(df['temperature_ext'].iloc[0])\n",
    "    df['temp_lag24'] = df['temperature_ext'].shift(24).fillna(df['temperature_ext'].iloc[0])\n",
    "    df['temp_diff'] = df['temperature_ext'].diff().fillna(0)\n",
    "    df['temp_amplitude_24h'] = (\n",
    "        df['temperature_ext'].rolling(window=24, min_periods=1).max() -\n",
    "        df['temperature_ext'].rolling(window=24, min_periods=1).min()\n",
    "    )\n",
    "\n",
    "    # Infrastructure interactions\n",
    "    if 'clients_connectes' in df.columns:\n",
    "        df['clients_temp'] = df['clients_connectes'] * df['temperature_ext']\n",
    "        df['clients_weekend'] = df['clients_connectes'] * df['est_weekend']\n",
    "        df['clients_heure_cos'] = df['clients_connectes'] * df['heure_cos']\n",
    "\n",
    "    if 'tstats_intelligents_connectes' in df.columns:\n",
    "        df['tstats_temp'] = df['tstats_intelligents_connectes'] * df['temperature_ext']\n",
    "        df['ratio_tstats_clients'] = (\n",
    "            df['tstats_intelligents_connectes'] / (df['clients_connectes'] + 1))\n",
    "\n",
    "    if 'irradiance_solaire' in df.columns:\n",
    "        df['irradiance_temp'] = df['irradiance_solaire'] * df['temperature_ext']\n",
    "\n",
    "    return df\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. BUILD PER-POSTE DATA\n",
    "# -------------------------------------------------------------------\n",
    "postes = sorted(train['poste'].unique())\n",
    "train_parts = {}\n",
    "test_parts = {}\n",
    "\n",
    "for p in postes:\n",
    "    train_parts[p] = creer_caracteristiques_v3(train[train['poste'] == p])\n",
    "    test_parts[p] = creer_caracteristiques_v3(test[test['poste'] == p])\n",
    "\n",
    "# Full feature list for Ridge (same as v4)\n",
    "features_clean = [f for f in train_parts[postes[0]].select_dtypes(include=[np.number]).columns\n",
    "                  if f not in ['energie_kwh']]\n",
    "\n",
    "# Add P_pointe if classifier available\n",
    "try:\n",
    "    _feats_pointe = ['temperature_ext', 'humidite', 'vitesse_vent',\n",
    "                     'heure_sin', 'heure_cos', 'est_weekend', 'clients_connectes']\n",
    "    for p in postes:\n",
    "        train_parts[p]['P_pointe'] = clf_pointe.predict_proba(\n",
    "            train_parts[p][_feats_pointe].values)[:, 1]\n",
    "        test_parts[p]['P_pointe'] = clf_pointe.predict_proba(\n",
    "            test_parts[p][_feats_pointe].values)[:, 1]\n",
    "    if 'P_pointe' not in features_clean:\n",
    "        features_clean.append('P_pointe')\n",
    "    print(\"P_pointe added.\")\n",
    "except NameError:\n",
    "    print(\"clf_pointe not found, skipping P_pointe.\")\n",
    "\n",
    "# Reassemble full DataFrames (for Cell B compatibility)\n",
    "train_clean = pd.concat([train_parts[p] for p in postes])\n",
    "test_clean = pd.concat([test_parts[p] for p in postes])\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. MODEL CONFIGURATION PER POSTE\n",
    "# -------------------------------------------------------------------\n",
    "# KNN features for Poste C: WEATHER-ONLY (no infrastructure features!)\n",
    "# Infrastructure features (clients_connectes, tstats) drift over time and\n",
    "# cause KNN to find wrong neighbors. Weather features are seasonal = stable.\n",
    "features_knn_c = [\n",
    "    'temperature_ext', 'heure_sin', 'heure_cos',\n",
    "    'mois_sin', 'mois_cos', 'est_weekend',\n",
    "    'degres_jours_chauffage', 'degres_jours_clim',\n",
    "    'humidite', 'irradiance_solaire', 'temp_squared',\n",
    "]\n",
    "\n",
    "# Model type per poste\n",
    "model_types = {\n",
    "    'A': 'ridge',   # Ridge works well (RMSE=16.74, R¬≤=0.28)\n",
    "    'B': 'ridge',   # Ridge for B (KNN didn't improve B in testing)\n",
    "    'C': 'knn',     # KNN: finds summer 2022/2023 neighbors, avoids extrapolation\n",
    "}\n",
    "\n",
    "# Ridge configs (same as v4)\n",
    "alphas_grid = [0.01, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 5000]\n",
    "alphas_per_poste = {\n",
    "    'A': alphas_grid,\n",
    "    'B': alphas_grid,\n",
    "    'C': [500, 1000, 2000, 5000, 10000]  # not used for C in v5, kept for reference\n",
    "}\n",
    "\n",
    "# KNN k candidates ‚Äî wide range including large values\n",
    "# Large k acts as smoothed local average (Nadaraya-Watson style)\n",
    "k_candidates = [3, 5, 10, 20, 50, 100, 200, 500, 750, 1000, 1500, 2000, 2500, 3000]\n",
    "\n",
    "print(f\"Ridge features: {len(features_clean)}\")\n",
    "print(f\"KNN features (Poste C): {len(features_knn_c)}\")\n",
    "print(f\"Postes: {postes}\")\n",
    "print(f\"Model types: {model_types}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. KNN CV TUNING HELPER\n",
    "# -------------------------------------------------------------------\n",
    "def tune_knn_cv(X_tr, y_tr, k_candidates, n_splits):\n",
    "    \"\"\"Tune KNN k using TimeSeriesSplit CV. Returns best k and CV scores.\"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    best_k = k_candidates[0]\n",
    "    best_rmse = float('inf')\n",
    "    results = []\n",
    "\n",
    "    # Filter k candidates to be <= training size in smallest fold\n",
    "    max_k = len(X_tr) * (n_splits - 1) // n_splits  # approx smallest train fold\n",
    "    valid_ks = [k for k in k_candidates if k <= max_k]\n",
    "    if not valid_ks:\n",
    "        valid_ks = [min(k_candidates)]\n",
    "\n",
    "    for k in valid_ks:\n",
    "        fold_mses = []\n",
    "        for train_idx, val_idx in tscv.split(X_tr):\n",
    "            if k > len(train_idx):\n",
    "                continue\n",
    "            knn = KNeighborsRegressor(n_neighbors=k, weights='distance', metric='euclidean')\n",
    "            knn.fit(X_tr[train_idx], y_tr[train_idx])\n",
    "            preds = knn.predict(X_tr[val_idx])\n",
    "            fold_mses.append(mean_squared_error(y_tr[val_idx], preds))\n",
    "        if fold_mses:\n",
    "            cv_rmse = np.sqrt(np.mean(fold_mses))\n",
    "            results.append((k, cv_rmse))\n",
    "            if cv_rmse < best_rmse:\n",
    "                best_rmse = cv_rmse\n",
    "                best_k = k\n",
    "\n",
    "    return best_k, best_rmse, results\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. TRAIN PER-POSTE MODELS\n",
    "# -------------------------------------------------------------------\n",
    "models = {}\n",
    "scalers_per_poste = {}\n",
    "features_used = {}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Per-poste model training (v5: Ridge A/B + KNN C)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for p in postes:\n",
    "    mtype = model_types[p]\n",
    "    n_splits = min(5, max(2, len(train_parts[p]) // 50))\n",
    "\n",
    "    if mtype == 'ridge':\n",
    "        # --- RIDGE MODEL (same as v4) ---\n",
    "        feats_p = features_clean\n",
    "        alphas_p = alphas_per_poste.get(p, alphas_grid)\n",
    "\n",
    "        X_tr = train_parts[p][feats_p].values\n",
    "        y_tr = train_parts[p]['energie_kwh'].values\n",
    "\n",
    "        scaler_p = StandardScaler()\n",
    "        X_tr_s = scaler_p.fit_transform(X_tr)\n",
    "\n",
    "        tscv_p = TimeSeriesSplit(n_splits=n_splits)\n",
    "        ridge_p = RidgeCV(alphas=alphas_p, cv=tscv_p, scoring='neg_mean_squared_error')\n",
    "        ridge_p.fit(X_tr_s, y_tr)\n",
    "\n",
    "        X_te = test_parts[p][feats_p].values\n",
    "        y_te = test_parts[p]['energie_kwh'].values\n",
    "        X_te_s = scaler_p.transform(X_te)\n",
    "\n",
    "        y_pred_tr = ridge_p.predict(X_tr_s)\n",
    "        y_pred_te = ridge_p.predict(X_te_s)\n",
    "\n",
    "        rmse_tr = np.sqrt(mean_squared_error(y_tr, y_pred_tr))\n",
    "        rmse_te = np.sqrt(mean_squared_error(y_te, y_pred_te))\n",
    "        r2_tr = r2_score(y_tr, y_pred_tr)\n",
    "        r2_te = r2_score(y_te, y_pred_te) if len(y_te) > 1 else float('nan')\n",
    "\n",
    "        models[p] = ridge_p\n",
    "        scalers_per_poste[p] = scaler_p\n",
    "        features_used[p] = feats_p\n",
    "\n",
    "        print(f\"\\n  Poste {p} [RIDGE]: alpha={ridge_p.alpha_}, n_feat={len(feats_p)}, \"\n",
    "              f\"n_tr={len(y_tr)}, n_te={len(y_te)}\")\n",
    "        print(f\"    RMSE_tr={rmse_tr:.2f}, RMSE_te={rmse_te:.2f}, \"\n",
    "              f\"R2_tr={r2_tr:.4f}, R2_te={r2_te:.4f}\")\n",
    "\n",
    "    else:\n",
    "        # --- KNN MODEL (Poste C) ---\n",
    "        feats_p = [f for f in features_knn_c if f in train_parts[p].columns]\n",
    "\n",
    "        X_tr = train_parts[p][feats_p].values\n",
    "        y_tr = train_parts[p]['energie_kwh'].values\n",
    "\n",
    "        scaler_p = StandardScaler()\n",
    "        X_tr_s = scaler_p.fit_transform(X_tr)\n",
    "\n",
    "        # Tune k via TimeSeriesSplit CV\n",
    "        best_k, cv_rmse, k_results = tune_knn_cv(X_tr_s, y_tr, k_candidates, n_splits)\n",
    "\n",
    "        # Train final model with best k on all training data\n",
    "        knn_p = KNeighborsRegressor(n_neighbors=best_k, weights='distance', metric='euclidean')\n",
    "        knn_p.fit(X_tr_s, y_tr)\n",
    "\n",
    "        X_te = test_parts[p][feats_p].values\n",
    "        y_te = test_parts[p]['energie_kwh'].values\n",
    "        X_te_s = scaler_p.transform(X_te)\n",
    "\n",
    "        y_pred_tr = knn_p.predict(X_tr_s)\n",
    "        y_pred_te = knn_p.predict(X_te_s)\n",
    "\n",
    "        rmse_tr = np.sqrt(mean_squared_error(y_tr, y_pred_tr))\n",
    "        rmse_te = np.sqrt(mean_squared_error(y_te, y_pred_te))\n",
    "        r2_tr = r2_score(y_tr, y_pred_tr)\n",
    "        r2_te = r2_score(y_te, y_pred_te) if len(y_te) > 1 else float('nan')\n",
    "\n",
    "        models[p] = knn_p\n",
    "        scalers_per_poste[p] = scaler_p\n",
    "        features_used[p] = feats_p\n",
    "\n",
    "        print(f\"\\n  Poste {p} [KNN]:   best_k={best_k} (CV RMSE={cv_rmse:.2f}), \"\n",
    "              f\"n_feat={len(feats_p)}, n_tr={len(y_tr)}, n_te={len(y_te)}\")\n",
    "        print(f\"    RMSE_tr={rmse_tr:.2f}, RMSE_te={rmse_te:.2f}, \"\n",
    "              f\"R2_tr={r2_tr:.4f}, R2_te={r2_te:.4f}\")\n",
    "        print(f\"    k search results:\")\n",
    "        for k_val, k_rmse in k_results:\n",
    "            marker = \" <-- best\" if k_val == best_k else \"\"\n",
    "            print(f\"      k={k_val:5d}: CV_RMSE={k_rmse:.2f}{marker}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. PREDICT ON TEST\n",
    "# -------------------------------------------------------------------\n",
    "y_pred_test_c = pd.Series(index=test.index, dtype=float)\n",
    "\n",
    "for p in postes:\n",
    "    te_p = test_parts[p]\n",
    "    feats_p = features_used[p]\n",
    "    X_te = te_p[feats_p].values\n",
    "    X_te_s = scalers_per_poste[p].transform(X_te)\n",
    "    preds = np.maximum(models[p].predict(X_te_s), 0)\n",
    "    y_pred_test_c.loc[te_p.index] = preds\n",
    "\n",
    "y_pred_test_c = y_pred_test_c.values\n",
    "y_test_clean = test['energie_kwh'].values\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7. KAGGLE SCORE\n",
    "# -------------------------------------------------------------------\n",
    "rmse_sim = np.sqrt(mean_squared_error(y_test_clean, y_pred_test_c))\n",
    "r2_sim = r2_score(y_test_clean, y_pred_test_c)\n",
    "mae_sim = mean_absolute_error(y_test_clean, y_pred_test_c)\n",
    "\n",
    "# Build merged DataFrame (for Cell B)\n",
    "merged = pd.DataFrame({\n",
    "    'horodatage_local': test['horodatage_local'].values,\n",
    "    'poste': test['poste'].values,\n",
    "    'y_true': y_test_clean,\n",
    "    'y_pred': y_pred_test_c\n",
    "})\n",
    "\n",
    "y_sim_pred = y_pred_test_c.copy()\n",
    "\n",
    "try:\n",
    "    sample_sub = pd.read_csv('ift-3395-6390-prediction-energetique/sample_submission.csv')\n",
    "    expected_rows = len(sample_sub)\n",
    "except FileNotFoundError:\n",
    "    expected_rows = len(test)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 8. REPORT\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KAGGLE SCORE SIMULATION (v5: Ridge A/B + KNN C)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Models:      {', '.join(f'{p}={model_types[p].upper()}' for p in postes)}\")\n",
    "print(f\"  Features:    {', '.join(f'{p}={len(features_used[p])}' for p in postes)}\")\n",
    "print(f\"  Rows:        {len(merged)} (expected {expected_rows})\")\n",
    "print()\n",
    "print(f\"  RMSE:  {rmse_sim:.4f} kWh   <-- Kaggle RMSE\")\n",
    "print(f\"  MAE:   {mae_sim:.4f} kWh\")\n",
    "print(f\"  R2:    {r2_sim:.4f}\")\n",
    "print()\n",
    "print(f\"  Pred stats:  min={merged['y_pred'].min():.2f}, \"\n",
    "      f\"mean={merged['y_pred'].mean():.2f}, max={merged['y_pred'].max():.2f}\")\n",
    "print(f\"  Truth stats: min={merged['y_true'].min():.2f}, \"\n",
    "      f\"mean={merged['y_true'].mean():.2f}, max={merged['y_true'].max():.2f}\")\n",
    "\n",
    "print(f\"\\n  vs v4 (Ridge-only): RMSE was 63.51, R2 was 0.19\")\n",
    "delta_total = rmse_sim - 63.51\n",
    "print(f\"  Improvement: {delta_total:+.2f} kWh ({100*delta_total/63.51:+.1f}%)\")\n",
    "\n",
    "print(\"\\n  Per-poste RMSE (v4 ‚Üí v5):\")\n",
    "v4_rmses = {'A': 16.74, 'B': 44.55, 'C': 174.83}\n",
    "for p in postes:\n",
    "    mask = merged['poste'] == p\n",
    "    sub = merged[mask]\n",
    "    rmse_p = np.sqrt(mean_squared_error(sub['y_true'], sub['y_pred']))\n",
    "    r2_p = r2_score(sub['y_true'], sub['y_pred']) if len(sub) > 1 else float('nan')\n",
    "    bias_p = (sub['y_true'] - sub['y_pred']).mean()\n",
    "    delta = rmse_p - v4_rmses[p]\n",
    "    print(f\"    Poste {p} [{model_types[p].upper():5s}]: RMSE={rmse_p:.2f} (v4={v4_rmses[p]:.2f}, \"\n",
    "          f\"Œî={delta:+.2f}), R2={r2_p:.4f}, bias={bias_p:+.2f}, n={mask.sum()}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DIAGNOSTIC DUMP v5 -- COPY EVERYTHING BELOW THIS LINE\n",
      "================================================================================\n",
      "\n",
      "[1] DATASET OVERVIEW\n",
      "  train shape: (8246, 23)\n",
      "  test shape:  (1754, 23)\n",
      "  train period: 2022-01-01 05:00:00+00:00 -> 2024-01-31 21:00:00+00:00\n",
      "  test period:  2024-02-01 01:00:00+00:00 -> 2024-07-01 03:00:00+00:00\n",
      "\n",
      "  train energie_kwh: mean=215.91, std=212.95, min=13.01, max=11804.20\n",
      "  test energie_kwh:  mean=83.74, std=70.77, min=17.76, max=538.48\n",
      "  temperature_ext: train_mean=5.98, test_mean=9.51, shift=+58.9%\n",
      "  humidite: train_mean=72.96, test_mean=73.37, shift=+0.6%\n",
      "  vitesse_vent: train_mean=2.64, test_mean=2.95, shift=+12.1%\n",
      "  irradiance_solaire: train_mean=137.88, test_mean=196.62, shift=+42.6%\n",
      "  clients_connectes: train_mean=69.22, test_mean=47.05, shift=-32.0%\n",
      "\n",
      "[1B] PER-POSTE OVERVIEW\n",
      "  Poste A [RIDGE]: train n=1751, mean_kwh=82.73 | test n=474, mean_kwh=50.82\n",
      "  Poste B [RIDGE]: train n=366, mean_kwh=129.81 | test n=1126, mean_kwh=72.20\n",
      "  Poste C [KNN  ]: train n=6129, mean_kwh=259.10 | test n=154, mean_kwh=269.41\n",
      "\n",
      "[2] PER-POSTE MODEL CONFIGURATIONS\n",
      "  Poste A [RIDGE]: alpha=500.0, n_feat=43, intercept=82.7272\n",
      "  Poste B [RIDGE]: alpha=50.0, n_feat=43, intercept=129.8094\n",
      "  Poste C [KNN]:   k=1500, n_feat=11, weights=distance, metric=euclidean\n",
      "\n",
      "[3] MODEL DETAILS PER POSTE\n",
      "\n",
      "  --- Poste A [RIDGE] ---\n",
      "  Top 15 coefficients:\n",
      "     1. mois_cos                                   +10.4314\n",
      "     2. temp_mois_sin                               -9.8142\n",
      "     3. mois                                        -7.1632\n",
      "     4. degres_jours_chauffage                      +5.2769\n",
      "     5. ratio_tstats_clients                        -5.0680\n",
      "     6. temp_heure_cos                              +4.5078\n",
      "     7. clients_temp                                -4.4697\n",
      "     8. tstats_temp                                 -4.4058\n",
      "     9. temp_lag24                                  -4.2865\n",
      "    10. temp_lag1                                   -4.2144\n",
      "    11. heure_cos                                   -4.1372\n",
      "    12. temp_ressentie                              -4.0611\n",
      "    13. temperature_ext                             -4.0040\n",
      "    14. est_nuit                                    -3.9430\n",
      "    15. clients_heure_cos                           -3.8678\n",
      "\n",
      "  --- Poste B [RIDGE] ---\n",
      "  Top 15 coefficients:\n",
      "     1. evenement_pointe                           -15.9594\n",
      "     2. est_pointe_soir                             +6.5341\n",
      "     3. irradiance_solaire                          -5.7744\n",
      "     4. clients_heure_cos                           -5.0408\n",
      "     5. heure_cos                                   -4.8961\n",
      "     6. temp_lag24                                  -4.5456\n",
      "     7. heure                                       +4.0472\n",
      "     8. temp_weekend                                -3.8326\n",
      "     9. temp_lag1                                   -3.2887\n",
      "    10. temp_diff                                   +2.8837\n",
      "    11. est_nuit                                    -2.6955\n",
      "    12. temp_mois_cos                               -2.5695\n",
      "    13. heure_sin                                   -2.4752\n",
      "    14. temperature_ext                             -2.4044\n",
      "    15. degres_jours_chauffage                      +2.4044\n",
      "\n",
      "  --- Poste C [KNN] ---\n",
      "  KNN neighbor distances (to test points):\n",
      "    Mean nearest dist:  0.4465\n",
      "    Mean farthest dist: 3.1538\n",
      "    Max nearest dist:   0.9421\n",
      "  Neighbor month distribution:\n",
      "    Month  1: 65600 ( 28.4%)\n",
      "    Month  2: 37722 ( 16.3%)\n",
      "    Month  3: 41029 ( 17.8%)\n",
      "    Month  4: 20270 (  8.8%)\n",
      "    Month  5:  4658 (  2.0%)\n",
      "    Month  6:   399 (  0.2%)\n",
      "    Month  7:    15 (  0.0%)\n",
      "    Month  8:    30 (  0.0%)\n",
      "    Month  9:   285 (  0.1%)\n",
      "    Month 10:  4320 (  1.9%)\n",
      "    Month 11: 19164 (  8.3%)\n",
      "    Month 12: 37508 ( 16.2%)\n",
      "\n",
      "[4] KAGGLE SIMULATION RESULTS\n",
      "  RMSE:  50.3819\n",
      "  MAE:   35.8591\n",
      "  R2:    0.4929\n",
      "  Rows:  1754\n",
      "\n",
      "[5] RESIDUAL ANALYSIS\n",
      "  Residual mean:   -29.6118 (bias)\n",
      "  Residual std:    40.7612\n",
      "  |residual| P50: 28.60\n",
      "  |residual| P75: 48.07\n",
      "  |residual| P90: 65.21\n",
      "  |residual| P95: 103.01\n",
      "  |residual| P99: 173.36\n",
      "\n",
      "[5B] PER-POSTE RESIDUAL ANALYSIS\n",
      "  Poste A [RIDGE]: RMSE=16.77 (v4=16.74, delta=+0.03), bias=+2.40, MAE=13.07, R2=0.2812, n=474\n",
      "  Poste B [RIDGE]: RMSE=40.02 (v4=44.55, delta=-4.53), bias=-33.28, MAE=35.02, R2=-0.2285, n=1126\n",
      "  Poste C [KNN  ]: RMSE=127.80 (v4=174.83, delta=-47.03), bias=-101.32, MAE=112.13, R2=-1.1796, n=154\n",
      "\n",
      "[6] MAE BY HOUR\n",
      "  Hour  0: MAE= 59.19, bias=-58.36, mean_conso=128.61, n=20\n",
      "  Hour  1: MAE= 32.90, bias=-25.87, mean_conso= 62.57, n=72\n",
      "  Hour  2: MAE= 38.54, bias=-35.67, mean_conso= 53.10, n=67\n",
      "  Hour  3: MAE= 36.50, bias=-35.13, mean_conso= 56.71, n=67\n",
      "  Hour  4: MAE= 45.60, bias=-29.87, mean_conso= 80.81, n=81\n",
      "  Hour  5: MAE= 41.56, bias=-34.72, mean_conso= 69.55, n=91\n",
      "  Hour  6: MAE= 36.25, bias=-31.90, mean_conso= 66.80, n=71\n",
      "  Hour  7: MAE= 43.44, bias=-37.39, mean_conso= 94.65, n=73\n",
      "  Hour  8: MAE= 40.01, bias=-37.53, mean_conso= 93.40, n=78\n",
      "  Hour  9: MAE= 30.72, bias=-26.67, mean_conso= 80.01, n=69\n",
      "  Hour 10: MAE= 28.26, bias=-22.06, mean_conso= 81.52, n=67\n",
      "  Hour 11: MAE= 28.75, bias=-20.87, mean_conso= 88.37, n=83\n",
      "  Hour 12: MAE= 29.51, bias=-21.82, mean_conso= 85.76, n=78\n",
      "  Hour 13: MAE= 30.82, bias=-26.75, mean_conso= 76.41, n=72\n",
      "  Hour 14: MAE= 34.29, bias=-28.37, mean_conso= 84.57, n=76\n",
      "  Hour 15: MAE= 34.49, bias=-29.39, mean_conso= 74.14, n=67\n",
      "  Hour 16: MAE= 34.10, bias=-30.13, mean_conso= 86.81, n=77\n",
      "  Hour 17: MAE= 37.28, bias=-35.72, mean_conso= 88.73, n=77\n",
      "  Hour 18: MAE= 36.36, bias=-30.21, mean_conso=103.12, n=74\n",
      "  Hour 19: MAE= 31.85, bias=-23.23, mean_conso=105.02, n=82\n",
      "  Hour 20: MAE= 39.37, bias=-30.44, mean_conso=113.37, n=61\n",
      "  Hour 21: MAE= 27.55, bias=-15.05, mean_conso=101.96, n=85\n",
      "  Hour 22: MAE= 39.61, bias=-29.92, mean_conso= 91.70, n=84\n",
      "  Hour 23: MAE= 39.87, bias=-36.60, mean_conso= 69.24, n=82\n",
      "\n",
      "[7] MAE BY MONTH\n",
      "  Month  1: MAE= 71.50, bias=-71.50, n=4\n",
      "  Month  2: MAE= 57.77, bias=-50.16, n=371\n",
      "  Month  3: MAE= 28.02, bias=-27.27, n=236\n",
      "  Month  4: MAE= 41.92, bias=-40.53, n=256\n",
      "  Month  5: MAE= 29.81, bias=-22.24, n=467\n",
      "  Month  6: MAE= 23.60, bias=-13.91, n=420\n",
      "\n",
      "[7B] MAE BY MONTH x POSTE\n",
      "  --- Poste A [RIDGE] ---\n",
      "    Month  4: MAE= 18.47, bias=-10.59, mean_true= 68.72, mean_pred= 79.31, n=41\n",
      "    Month  5: MAE= 13.06, bias= +1.34, mean_true= 49.86, mean_pred= 48.52, n=226\n",
      "    Month  6: MAE= 12.01, bias= +6.13, mean_true= 48.33, mean_pred= 42.20, n=207\n",
      "  --- Poste B [RIDGE] ---\n",
      "    Month  2: MAE= 20.15, bias=-14.90, mean_true=117.51, mean_pred=132.41, n=221\n",
      "    Month  3: MAE= 28.02, bias=-27.27, mean_true= 90.62, mean_pred=117.89, n=236\n",
      "    Month  4: MAE= 46.40, bias=-46.24, mean_true= 62.08, mean_pred=108.32, n=215\n",
      "    Month  5: MAE= 45.51, bias=-44.36, mean_true= 44.65, mean_pred= 89.01, n=241\n",
      "    Month  6: MAE= 34.87, bias=-33.39, mean_true= 46.15, mean_pred= 79.54, n=213\n",
      "  --- Poste C [KNN] ---\n",
      "    Month  1: MAE= 71.50, bias=-71.50, mean_true=280.00, mean_pred=351.50, n=4\n",
      "    Month  2: MAE=113.21, bias=-102.12, mean_true=269.13, mean_pred=371.25, n=150\n",
      "\n",
      "[8] MAE BY TEMPERATURE BIN\n",
      "  (-30, -10]     : MAE= 63.98, bias=-44.18, n=64\n",
      "  (-10, 0]       : MAE= 45.82, bias=-41.46, n=294\n",
      "  (0, 10]        : MAE= 42.28, bias=-40.35, n=504\n",
      "  (10, 20]       : MAE= 28.72, bias=-23.62, n=610\n",
      "  (20, 40]       : MAE= 23.06, bias= -7.72, n=282\n",
      "\n",
      "[8B] MAE BY TEMPERATURE BIN x POSTE\n",
      "  --- Poste A [RIDGE] ---\n",
      "    (0, 10]        : MAE= 16.96, bias= -8.57, n=59\n",
      "    (10, 20]       : MAE= 11.28, bias= -0.19, n=274\n",
      "    (20, 40]       : MAE= 14.90, bias=+12.02, n=141\n",
      "  --- Poste B [RIDGE] ---\n",
      "    (-30, -10]     : MAE= 20.59, bias=-14.96, n=40\n",
      "    (-10, 0]       : MAE= 22.08, bias=-18.82, n=203\n",
      "    (0, 10]        : MAE= 37.69, bias=-36.52, n=406\n",
      "    (10, 20]       : MAE= 42.94, bias=-42.73, n=336\n",
      "    (20, 40]       : MAE= 31.22, bias=-27.45, n=141\n",
      "  --- Poste C [KNN] ---\n",
      "    (-30, -10]     : MAE=136.31, bias=-92.88, n=24\n",
      "    (-10, 0]       : MAE= 98.79, bias=-91.95, n=91\n",
      "    (0, 10]        : MAE=128.38, bias=-128.38, n=39\n",
      "\n",
      "[9] ERRORS: POINTE vs NORMAL\n",
      "  Normal  : MAE=34.47, bias=-28.19, n=1726\n",
      "  Pointe  : MAE=121.53, bias=-117.09, n=28\n",
      "\n",
      "[10] ERRORS: WEEKEND vs WEEKDAY\n",
      "  Weekday : MAE=37.18, bias=-31.06, n=1224\n",
      "  Weekend : MAE=32.81, bias=-26.26, n=530\n",
      "\n",
      "[11] TOP 15 WORST PREDICTIONS\n",
      "poste  heure  mois  temperature_ext     y_true     y_pred    abs_err\n",
      "    C      7     2            -10.5 121.254790 385.543194 264.288404\n",
      "    C      7     2            -12.0 133.924798 395.139966 261.215168\n",
      "    C      8     2            -12.2 138.578763 398.551660 259.972897\n",
      "    C      9     2             -7.9 136.374097 394.458491 258.084394\n",
      "    C      8     2             -5.8 117.738317 373.358113 255.619797\n",
      "    C      7     2             -6.6 127.493412 366.821337 239.327926\n",
      "    C      7     2            -14.1 151.874644 390.156731 238.282087\n",
      "    C      8     2            -11.1 163.117721 399.584528 236.466806\n",
      "    C      7     2            -10.2 151.408463 387.547039 236.138576\n",
      "    C      6     2             -6.0 135.589153 354.000251 218.411098\n",
      "    C      6     2             -9.8 168.323179 380.530660 212.207480\n",
      "    C      6     2            -10.5 166.406183 375.937061 209.530878\n",
      "    C     15     2              0.5 200.151868 396.261581 196.109713\n",
      "    C     15     2              3.9 193.126202 375.845597 182.719395\n",
      "    C     16     2              5.7 200.744262 382.196451 181.452189\n",
      "\n",
      "[12] TOP FEATURE CORRELATIONS PER POSTE\n",
      "\n",
      "  --- Poste A [RIDGE] (top 10) ---\n",
      "     1. mois_cos                                 r=+0.8478\n",
      "     2. degres_jours_chauffage                   r=+0.8248\n",
      "     3. temp_lag1                                r=-0.8053\n",
      "     4. temp_ressentie                           r=-0.8026\n",
      "     5. temperature_ext                          r=-0.7974\n",
      "     6. mois                                     r=-0.7920\n",
      "     7. tstats_temp                              r=-0.7907\n",
      "     8. clients_temp                             r=-0.7877\n",
      "     9. temp_lag24                               r=-0.7451\n",
      "    10. tstats_intelligents_connectes            r=-0.6586\n",
      "\n",
      "  --- Poste B [RIDGE] (top 10) ---\n",
      "     1. temp_lag1                                r=-0.5515\n",
      "     2. temp_ressentie                           r=-0.5448\n",
      "     3. temperature_ext                          r=-0.5320\n",
      "     4. degres_jours_chauffage                   r=+0.5320\n",
      "     5. tstats_temp                              r=-0.5303\n",
      "     6. clients_temp                             r=-0.5298\n",
      "     7. temp_mois_cos                            r=-0.5279\n",
      "     8. temp_mois_sin                            r=-0.4829\n",
      "     9. temp_weekend                             r=-0.4063\n",
      "    10. temp_squared                             r=+0.4031\n",
      "\n",
      "  --- Poste C [KNN] (top 10) ---\n",
      "     1. degres_jours_chauffage                   r=+0.4949\n",
      "     2. temperature_ext                          r=-0.4723\n",
      "     3. mois_cos                                 r=+0.4187\n",
      "     4. mois_sin                                 r=+0.2817\n",
      "     5. temp_squared                             r=-0.2071\n",
      "     6. irradiance_solaire                       r=-0.1395\n",
      "     7. heure_sin                                r=-0.1134\n",
      "     8. heure_cos                                r=-0.0962\n",
      "     9. degres_jours_clim                        r=-0.0865\n",
      "    10. est_weekend                              r=+0.0075\n",
      "\n",
      "[13] DATA PIPELINE HEALTH\n",
      "  train_clean shape: (8246, 46)\n",
      "  test_clean shape:  (1754, 46)\n",
      "  merged shape:      (1754, 4)\n",
      "  No NaN in features (good)\n",
      "\n",
      "[14] v4 vs v5 COMPARISON\n",
      "  v4 RMSE: 63.51   |  v5 RMSE: 50.38   |  Œî = -13.13\n",
      "  v4 R2:   0.1943  |  v5 R2:   0.4929  |  Œî = +0.2986\n",
      "  Poste A: v4=16.74 ‚Üí v5=16.77 (Œî=+0.03) [RIDGE]\n",
      "  Poste B: v4=44.55 ‚Üí v5=40.02 (Œî=-4.53) [RIDGE]\n",
      "  Poste C: v4=174.83 ‚Üí v5=127.80 (Œî=-47.03) [KNN]\n",
      "\n",
      "[15] SAMPLE PREDICTIONS (first 20 rows)\n",
      "   horodatage_local poste     y_true     y_pred\n",
      "2024-02-01 01:00:00     C 346.194094 372.219042\n",
      "2024-02-01 02:00:00     C 309.154285 359.115948\n",
      "2024-02-01 03:00:00     C 266.433972 345.320372\n",
      "2024-02-01 04:00:00     C 198.226732 329.355395\n",
      "2024-02-01 05:00:00     B  89.578432  92.095953\n",
      "2024-02-01 05:00:00     C 174.930624 317.900893\n",
      "2024-02-01 08:00:00     B  90.479965  94.991913\n",
      "2024-02-01 12:00:00     B 113.598491 116.552040\n",
      "2024-02-01 14:00:00     C 285.613710 357.932554\n",
      "2024-02-01 14:00:00     B 102.134532 116.242975\n",
      "2024-02-01 16:00:00     C 265.481538 370.815126\n",
      "2024-02-01 17:00:00     B  99.204076 124.146242\n",
      "2024-02-01 19:00:00     C 242.795316 385.766271\n",
      "2024-02-01 20:00:00     B  96.496717 123.027492\n",
      "2024-02-02 01:00:00     B 140.656220 134.940405\n",
      "2024-02-02 02:00:00     B 125.841899 116.350027\n",
      "2024-02-02 03:00:00     C 207.908615 336.788243\n",
      "2024-02-02 03:00:00     B 100.110316 114.370154\n",
      "2024-02-02 07:00:00     C 155.919302 308.757989\n",
      "2024-02-02 09:00:00     C 182.000114 314.859414\n",
      "\n",
      "================================================================================\n",
      "END OF DIAGNOSTIC DUMP v5 -- COPY EVERYTHING ABOVE THIS LINE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CELL B v5: DIAGNOSTIC OUTPUT (copy-paste the output to me)\n",
    "# ================================================================\n",
    "# Run AFTER Cell A v5. Handles mixed model types (Ridge + KNN).\n",
    "# Uses: train, test, postes, train_parts, test_parts,\n",
    "# features_clean, features_used, models, model_types, scalers_per_poste,\n",
    "# merged, rmse_sim, mae_sim\n",
    "# ================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DIAGNOSTIC DUMP v5 -- COPY EVERYTHING BELOW THIS LINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ---- SECTION 1: DATASET OVERVIEW ----\n",
    "print(\"\\n[1] DATASET OVERVIEW\")\n",
    "print(f\"  train shape: {train.shape}\")\n",
    "print(f\"  test shape:  {test.shape}\")\n",
    "print(f\"  train period: {train['horodatage_local'].min()} -> {train['horodatage_local'].max()}\")\n",
    "print(f\"  test period:  {test['horodatage_local'].min()} -> {test['horodatage_local'].max()}\")\n",
    "\n",
    "print(f\"\\n  train energie_kwh: mean={train['energie_kwh'].mean():.2f}, \"\n",
    "      f\"std={train['energie_kwh'].std():.2f}, \"\n",
    "      f\"min={train['energie_kwh'].min():.2f}, max={train['energie_kwh'].max():.2f}\")\n",
    "print(f\"  test energie_kwh:  mean={test['energie_kwh'].mean():.2f}, \"\n",
    "      f\"std={test['energie_kwh'].std():.2f}, \"\n",
    "      f\"min={test['energie_kwh'].min():.2f}, max={test['energie_kwh'].max():.2f}\")\n",
    "\n",
    "for col in ['temperature_ext', 'humidite', 'vitesse_vent', 'irradiance_solaire', 'clients_connectes']:\n",
    "    if col in train.columns and col in test.columns:\n",
    "        print(f\"  {col}: train_mean={train[col].mean():.2f}, test_mean={test[col].mean():.2f}, \"\n",
    "              f\"shift={100*(test[col].mean()-train[col].mean())/(train[col].mean()+1e-8):+.1f}%\")\n",
    "\n",
    "# ---- SECTION 1B: PER-POSTE OVERVIEW ----\n",
    "print(\"\\n[1B] PER-POSTE OVERVIEW\")\n",
    "for p in postes:\n",
    "    tr_mask = train['poste'] == p\n",
    "    te_mask = test['poste'] == p\n",
    "    mtype = model_types.get(p, 'ridge')\n",
    "    print(f\"  Poste {p} [{mtype.upper():5s}]: train n={tr_mask.sum()}, \"\n",
    "          f\"mean_kwh={train.loc[tr_mask, 'energie_kwh'].mean():.2f} | \"\n",
    "          f\"test n={te_mask.sum()}, mean_kwh={test.loc[te_mask, 'energie_kwh'].mean():.2f}\")\n",
    "\n",
    "# ---- SECTION 2: PER-POSTE MODEL CONFIGS ----\n",
    "print(\"\\n[2] PER-POSTE MODEL CONFIGURATIONS\")\n",
    "for p in postes:\n",
    "    mtype = model_types.get(p, 'ridge')\n",
    "    n_feat = len(features_used[p])\n",
    "    if mtype == 'ridge':\n",
    "        print(f\"  Poste {p} [RIDGE]: alpha={models[p].alpha_}, n_feat={n_feat}, \"\n",
    "              f\"intercept={models[p].intercept_:.4f}\")\n",
    "    else:\n",
    "        print(f\"  Poste {p} [KNN]:   k={models[p].n_neighbors}, n_feat={n_feat}, \"\n",
    "              f\"weights={models[p].weights}, metric={models[p].metric}\")\n",
    "\n",
    "# ---- SECTION 3: TOP COEFFICIENTS / KNN NEIGHBOR INFO ----\n",
    "print(\"\\n[3] MODEL DETAILS PER POSTE\")\n",
    "for p in postes:\n",
    "    mtype = model_types.get(p, 'ridge')\n",
    "    print(f\"\\n  --- Poste {p} [{mtype.upper()}] ---\")\n",
    "    if mtype == 'ridge':\n",
    "        feats_p = features_used[p]\n",
    "        coef_pairs = sorted(zip(feats_p, models[p].coef_),\n",
    "                            key=lambda x: abs(x[1]), reverse=True)\n",
    "        print(f\"  Top 15 coefficients:\")\n",
    "        for i, (feat, coef) in enumerate(coef_pairs[:15], 1):\n",
    "            print(f\"    {i:2d}. {feat:40s} {coef:+10.4f}\")\n",
    "    else:\n",
    "        # KNN: show neighbor distance stats and month distribution\n",
    "        feats_p = features_used[p]\n",
    "        X_te = test_parts[p][feats_p].values\n",
    "        X_te_s = scalers_per_poste[p].transform(X_te)\n",
    "        dists, indices = models[p].kneighbors(X_te_s)\n",
    "        print(f\"  KNN neighbor distances (to test points):\")\n",
    "        print(f\"    Mean nearest dist:  {dists[:, 0].mean():.4f}\")\n",
    "        print(f\"    Mean farthest dist: {dists[:, -1].mean():.4f}\")\n",
    "        print(f\"    Max nearest dist:   {dists[:, 0].max():.4f}\")\n",
    "        # What training months the neighbors come from\n",
    "        tr_months = train_parts[p]['mois'].values\n",
    "        neighbor_months = tr_months[indices.flatten()]\n",
    "        from collections import Counter\n",
    "        month_counts = Counter(neighbor_months)\n",
    "        total_n = sum(month_counts.values())\n",
    "        print(f\"  Neighbor month distribution:\")\n",
    "        for m in sorted(month_counts.keys()):\n",
    "            pct = 100 * month_counts[m] / total_n\n",
    "            print(f\"    Month {m:2d}: {month_counts[m]:5d} ({pct:5.1f}%)\")\n",
    "\n",
    "# ---- SECTION 4: KAGGLE RESULTS ----\n",
    "print(\"\\n[4] KAGGLE SIMULATION RESULTS\")\n",
    "print(f\"  RMSE:  {rmse_sim:.4f}\")\n",
    "print(f\"  MAE:   {mae_sim:.4f}\")\n",
    "print(f\"  R2:    {r2_sim:.4f}\")\n",
    "print(f\"  Rows:  {len(merged)}\")\n",
    "\n",
    "# ---- SECTION 5: RESIDUAL ANALYSIS ----\n",
    "print(\"\\n[5] RESIDUAL ANALYSIS\")\n",
    "residuals = merged['y_true'].values - merged['y_pred'].values\n",
    "abs_residuals = np.abs(residuals)\n",
    "merged_diag = merged.copy()\n",
    "merged_diag['residual'] = residuals\n",
    "merged_diag['abs_err'] = abs_residuals\n",
    "\n",
    "print(f\"  Residual mean:   {residuals.mean():.4f} (bias)\")\n",
    "print(f\"  Residual std:    {residuals.std():.4f}\")\n",
    "for p_val in [50, 75, 90, 95, 99]:\n",
    "    print(f\"  |residual| P{p_val}: {np.percentile(abs_residuals, p_val):.2f}\")\n",
    "\n",
    "# ---- SECTION 5B: PER-POSTE RESIDUAL ----\n",
    "print(\"\\n[5B] PER-POSTE RESIDUAL ANALYSIS\")\n",
    "v4_rmses = {'A': 16.74, 'B': 44.55, 'C': 174.83}\n",
    "for p in postes:\n",
    "    mask = merged_diag['poste'] == p\n",
    "    sub = merged_diag[mask]\n",
    "    rmse_p = np.sqrt((sub['residual']**2).mean())\n",
    "    r2_p = r2_score(sub['y_true'], sub['y_pred']) if len(sub) > 1 else float('nan')\n",
    "    mtype = model_types.get(p, 'ridge')\n",
    "    delta = rmse_p - v4_rmses.get(p, 0)\n",
    "    print(f\"  Poste {p} [{mtype.upper():5s}]: RMSE={rmse_p:.2f} (v4={v4_rmses.get(p,0):.2f}, \"\n",
    "          f\"delta={delta:+.2f}), bias={sub['residual'].mean():+.2f}, \"\n",
    "          f\"MAE={sub['abs_err'].mean():.2f}, R2={r2_p:.4f}, n={mask.sum()}\")\n",
    "\n",
    "# ---- SECTIONS 6-11: ERRORS BY CATEGORY ----\n",
    "try:\n",
    "    analysis = merged_diag.copy()\n",
    "    for c in ['heure', 'mois', 'temperature_ext', 'clients_connectes',\n",
    "              'evenement_pointe', 'est_weekend']:\n",
    "        if c in test.columns:\n",
    "            analysis[c] = test[c].values\n",
    "\n",
    "    print(\"\\n[6] MAE BY HOUR\")\n",
    "    for h in range(24):\n",
    "        mask = analysis['heure'] == h\n",
    "        if mask.sum() > 0:\n",
    "            print(f\"  Hour {h:2d}: MAE={analysis.loc[mask, 'abs_err'].mean():6.2f}, \"\n",
    "                  f\"bias={analysis.loc[mask, 'residual'].mean():+6.2f}, \"\n",
    "                  f\"mean_conso={analysis.loc[mask, 'y_true'].mean():6.2f}, n={mask.sum()}\")\n",
    "\n",
    "    print(\"\\n[7] MAE BY MONTH\")\n",
    "    for m in sorted(analysis['mois'].unique()):\n",
    "        mask = analysis['mois'] == m\n",
    "        if mask.sum() > 0:\n",
    "            print(f\"  Month {m:2d}: MAE={analysis.loc[mask, 'abs_err'].mean():6.2f}, \"\n",
    "                  f\"bias={analysis.loc[mask, 'residual'].mean():+6.2f}, n={mask.sum()}\")\n",
    "\n",
    "    print(\"\\n[7B] MAE BY MONTH x POSTE\")\n",
    "    for p in postes:\n",
    "        mtype = model_types.get(p, 'ridge')\n",
    "        print(f\"  --- Poste {p} [{mtype.upper()}] ---\")\n",
    "        mask_p = analysis['poste'] == p\n",
    "        for m in sorted(analysis.loc[mask_p, 'mois'].unique()):\n",
    "            mask = mask_p & (analysis['mois'] == m)\n",
    "            if mask.sum() > 0:\n",
    "                print(f\"    Month {m:2d}: MAE={analysis.loc[mask, 'abs_err'].mean():6.2f}, \"\n",
    "                      f\"bias={analysis.loc[mask, 'residual'].mean():+6.2f}, \"\n",
    "                      f\"mean_true={analysis.loc[mask, 'y_true'].mean():6.2f}, \"\n",
    "                      f\"mean_pred={analysis.loc[mask, 'y_pred'].mean():6.2f}, n={mask.sum()}\")\n",
    "\n",
    "    print(\"\\n[8] MAE BY TEMPERATURE BIN\")\n",
    "    _tbins = pd.cut(analysis['temperature_ext'], bins=[-30, -10, 0, 10, 20, 40])\n",
    "    for tb, grp in analysis.groupby(_tbins, observed=False):\n",
    "        if len(grp) > 0:\n",
    "            print(f\"  {str(tb):15s}: MAE={grp['abs_err'].mean():6.2f}, \"\n",
    "                  f\"bias={grp['residual'].mean():+6.2f}, n={len(grp)}\")\n",
    "\n",
    "    print(\"\\n[8B] MAE BY TEMPERATURE BIN x POSTE\")\n",
    "    for p in postes:\n",
    "        mtype = model_types.get(p, 'ridge')\n",
    "        print(f\"  --- Poste {p} [{mtype.upper()}] ---\")\n",
    "        mask_p = analysis['poste'] == p\n",
    "        sub_p = analysis[mask_p]\n",
    "        _tbins_p = pd.cut(sub_p['temperature_ext'], bins=[-30, -10, 0, 10, 20, 40])\n",
    "        for tb, grp in sub_p.groupby(_tbins_p, observed=False):\n",
    "            if len(grp) > 0:\n",
    "                print(f\"    {str(tb):15s}: MAE={grp['abs_err'].mean():6.2f}, \"\n",
    "                      f\"bias={grp['residual'].mean():+6.2f}, n={len(grp)}\")\n",
    "\n",
    "    print(\"\\n[9] ERRORS: POINTE vs NORMAL\")\n",
    "    for ev, label in [(0, 'Normal'), (1, 'Pointe')]:\n",
    "        mask = analysis['evenement_pointe'] == ev\n",
    "        if mask.sum() > 0:\n",
    "            print(f\"  {label:8s}: MAE={analysis.loc[mask, 'abs_err'].mean():.2f}, \"\n",
    "                  f\"bias={analysis.loc[mask, 'residual'].mean():+.2f}, n={mask.sum()}\")\n",
    "\n",
    "    print(\"\\n[10] ERRORS: WEEKEND vs WEEKDAY\")\n",
    "    for we, label in [(0, 'Weekday'), (1, 'Weekend')]:\n",
    "        mask = analysis['est_weekend'] == we\n",
    "        if mask.sum() > 0:\n",
    "            print(f\"  {label:8s}: MAE={analysis.loc[mask, 'abs_err'].mean():.2f}, \"\n",
    "                  f\"bias={analysis.loc[mask, 'residual'].mean():+.2f}, n={mask.sum()}\")\n",
    "\n",
    "    print(\"\\n[11] TOP 15 WORST PREDICTIONS\")\n",
    "    worst = analysis.nlargest(15, 'abs_err')\n",
    "    cols_show = ['poste', 'heure', 'mois', 'temperature_ext',\n",
    "                 'y_true', 'y_pred', 'abs_err']\n",
    "    cols_avail = [c for c in cols_show if c in worst.columns]\n",
    "    print(worst[cols_avail].to_string(index=False))\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"  Error in sections 6-11: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "# ---- SECTION 12: FEATURE CORRELATIONS PER POSTE ----\n",
    "print(\"\\n[12] TOP FEATURE CORRELATIONS PER POSTE\")\n",
    "for p in postes:\n",
    "    try:\n",
    "        _feats_avail = [f for f in features_used[p] if f in train_parts[p].columns]\n",
    "        _corrs = train_parts[p][_feats_avail + ['energie_kwh']].corr()['energie_kwh'].drop('energie_kwh')\n",
    "        _corrs_sorted = _corrs.abs().sort_values(ascending=False)\n",
    "        mtype = model_types.get(p, 'ridge')\n",
    "        print(f\"\\n  --- Poste {p} [{mtype.upper()}] (top 10) ---\")\n",
    "        for i, feat in enumerate(_corrs_sorted.head(10).index, 1):\n",
    "            print(f\"    {i:2d}. {feat:40s} r={_corrs[feat]:+.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error for poste {p}: {e}\")\n",
    "\n",
    "# ---- SECTION 13: PIPELINE HEALTH ----\n",
    "print(\"\\n[13] DATA PIPELINE HEALTH\")\n",
    "print(f\"  train_clean shape: {train_clean.shape}\")\n",
    "print(f\"  test_clean shape:  {test_clean.shape}\")\n",
    "print(f\"  merged shape:      {merged.shape}\")\n",
    "any_nan = False\n",
    "for p in postes:\n",
    "    feats_p = features_used[p]\n",
    "    nans = train_parts[p][feats_p].isnull().sum()\n",
    "    if nans.sum() > 0:\n",
    "        print(f\"  WARNING NaN in train poste {p}: {nans[nans>0].to_dict()}\")\n",
    "        any_nan = True\n",
    "    nans_te = test_parts[p][feats_p].isnull().sum()\n",
    "    if nans_te.sum() > 0:\n",
    "        print(f\"  WARNING NaN in test poste {p}: {nans_te[nans_te>0].to_dict()}\")\n",
    "        any_nan = True\n",
    "if not any_nan:\n",
    "    print(f\"  No NaN in features (good)\")\n",
    "\n",
    "# ---- SECTION 14: v4 vs v5 COMPARISON ----\n",
    "print(\"\\n[14] v4 vs v5 COMPARISON\")\n",
    "print(f\"  v4 RMSE: 63.51   |  v5 RMSE: {rmse_sim:.2f}   |  Œî = {rmse_sim - 63.51:+.2f}\")\n",
    "print(f\"  v4 R2:   0.1943  |  v5 R2:   {r2_sim:.4f}  |  Œî = {r2_sim - 0.1943:+.4f}\")\n",
    "for p in postes:\n",
    "    mask = merged['poste'] == p\n",
    "    sub = merged[mask]\n",
    "    rmse_now = np.sqrt(mean_squared_error(sub['y_true'], sub['y_pred']))\n",
    "    mtype = model_types.get(p, 'ridge')\n",
    "    print(f\"  Poste {p}: v4={v4_rmses[p]:.2f} ‚Üí v5={rmse_now:.2f} \"\n",
    "          f\"(Œî={rmse_now - v4_rmses[p]:+.2f}) [{mtype.upper()}]\")\n",
    "\n",
    "# ---- SECTION 15: SAMPLE PREDICTIONS ----\n",
    "print(\"\\n[15] SAMPLE PREDICTIONS (first 20 rows)\")\n",
    "print(merged[['horodatage_local', 'poste', 'y_true', 'y_pred']].head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"END OF DIAGNOSTIC DUMP v5 -- COPY EVERYTHING ABOVE THIS LINE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEEP DIAGNOSTIC v4 -- COPY EVERYTHING BELOW THIS LINE\n",
      "================================================================================\n",
      "\n",
      "[D1] TRAIN vs TEST FEATURE DISTRIBUTIONS PER POSTE\n",
      "\n",
      "  --- Poste A (train n=1751, test n=474) ---\n",
      "    temperature_ext                    : train=    5.18, test=   17.03, shift=+228.9%\n",
      "    clients_connectes                  : train=   25.28, test=   51.81, shift=+104.9%\n",
      "    irradiance_solaire                 : train=  156.37, test=  242.93, shift= +55.4%\n",
      "    tstats_intelligents_connectes      : train=  168.21, test=  304.60, shift= +81.1%\n",
      "\n",
      "  --- Poste B (train n=366, test n=1126) ---\n",
      "    temperature_ext                    : train=   -4.56, test=    8.18, shift=+279.2%\n",
      "    clients_connectes                  : train=   38.63, test=   37.30, shift=  -3.5%\n",
      "    irradiance_solaire                 : train=   46.70, test=  194.50, shift=+316.4%\n",
      "    tstats_intelligents_connectes      : train=  258.28, test=  253.52, shift=  -1.8%\n",
      "\n",
      "  --- Poste C (train n=6129, test n=154) ---\n",
      "    temperature_ext                    : train=    6.84, test=   -3.96, shift=-157.8%\n",
      "    clients_connectes                  : train=   83.60, test=  103.67, shift= +24.0%\n",
      "    irradiance_solaire                 : train=  138.04, test=   69.62, shift= -49.6%\n",
      "    tstats_intelligents_connectes      : train=  514.59, test=  655.53, shift= +27.4%\n",
      "\n",
      "[D2] INTERCEPT vs ACTUAL TEST MEAN (shows bias source)\n",
      "  Poste A: intercept=82.73, test_mean=50.82, gap=+31.91 kWh\n",
      "  Poste B: intercept=129.81, test_mean=72.20, gap=+57.61 kWh\n",
      "  Poste C: intercept=259.10, test_mean=269.41, gap=-10.32 kWh\n",
      "\n",
      "[D3] COEFFICIENT MAGNITUDE (L1 norm of coefs)\n",
      "  Poste A: alpha=500.0, L1=124.58, L2=24.65, max|coef|=10.38\n",
      "  Poste B: alpha=10.0, L1=130.95, L2=29.80, max|coef|=18.50\n",
      "  Poste C: alpha=1000, L1=464.47, L2=90.51, max|coef|=39.09\n",
      "\n",
      "[D4] FEATURE CONTRIBUTION ON TEST (mean contribution per feature)\n",
      "     Shows which features push predictions UP or DOWN vs intercept\n",
      "\n",
      "  --- Poste A ---\n",
      "    Intercept: 82.73\n",
      "    Total feature contribution: -34.74\n",
      "    Predicted mean: 47.98\n",
      "    Actual test mean: 50.82\n",
      "    Top contributors pushing prediction:\n",
      "      ratio_tstats_clients               :   +24.62 (UP)\n",
      "      mois_cos                           :    -9.50 (DOWN)\n",
      "      clients_temp                       :    -9.49 (DOWN)\n",
      "      tstats_temp                        :    -8.07 (DOWN)\n",
      "      temp_mois_sin                      :    -8.05 (DOWN)\n",
      "      tstats_intelligents_connectes      :    -4.96 (DOWN)\n",
      "      mois                               :    -4.89 (DOWN)\n",
      "      degres_jours_chauffage             :    -4.81 (DOWN)\n",
      "      clients_connectes                  :    +4.57 (UP)\n",
      "      temp_lag1                          :    -3.77 (DOWN)\n",
      "\n",
      "  --- Poste B ---\n",
      "    Intercept: 129.81\n",
      "    Total feature contribution: -28.31\n",
      "    Predicted mean: 101.50\n",
      "    Actual test mean: 72.20\n",
      "    Top contributors pushing prediction:\n",
      "      temp_squared                       :   +18.97 (UP)\n",
      "      irradiance_solaire                 :   -18.74 (DOWN)\n",
      "      mois_cos                           :   +18.30 (UP)\n",
      "      temp_lag24                         :   -11.56 (DOWN)\n",
      "      temp_lag1                          :    -9.47 (DOWN)\n",
      "      irradiance_temp                    :    -7.61 (DOWN)\n",
      "      temperature_ext                    :    -7.09 (DOWN)\n",
      "      temp_ressentie                     :    -6.89 (DOWN)\n",
      "      ratio_tstats_clients               :    +6.73 (UP)\n",
      "      degres_jours_chauffage             :    -6.62 (DOWN)\n",
      "\n",
      "  --- Poste C ---\n",
      "    Intercept: 259.10\n",
      "    Total feature contribution: +172.01\n",
      "    Predicted mean: 431.11\n",
      "    Actual test mean: 269.41\n",
      "    Top contributors pushing prediction:\n",
      "      clients_connectes                  :   +38.85 (UP)\n",
      "      tstats_intelligents_connectes      :   +36.76 (UP)\n",
      "      mois_sin                           :   +23.43 (UP)\n",
      "      evenement_pointe                   :   -19.33 (DOWN)\n",
      "      clients_temp                       :   +18.74 (UP)\n",
      "      tstats_temp                        :   +18.47 (UP)\n",
      "      degres_jours_chauffage             :   +17.68 (UP)\n",
      "      ratio_tstats_clients               :   -13.79 (DOWN)\n",
      "      temp_lag24                         :   +12.05 (UP)\n",
      "      temp_lag1                          :   +10.96 (UP)\n",
      "\n",
      "[D5] CLIENTS_CONNECTES vs CONSUMPTION PER POSTE\n",
      "     Tests if normalizing by clients would help\n",
      "  Poste A: train_kwh/client=3.527 (clients=25), test_kwh/client=0.981 (clients=52), shift=-72.2%\n",
      "  Poste B: train_kwh/client=3.363 (clients=39), test_kwh/client=1.926 (clients=37), shift=-42.7%\n",
      "  Poste C: train_kwh/client=3.196 (clients=84), test_kwh/client=2.600 (clients=104), shift=-18.6%\n",
      "\n",
      "[D6] PREDICTION RANGE vs TRUE RANGE PER POSTE\n",
      "  Poste A: pred=[11.3, 103.9], true=[19.9, 133.3], pred_std=16.43, true_std=19.80\n",
      "  Poste B: pred=[0.0, 203.8], true=[17.8, 210.9], pred_std=38.36, true_std=36.13\n",
      "  Poste C: pred=[159.9, 640.7], true=[117.7, 538.5], pred_std=86.99, true_std=86.85\n",
      "\n",
      "[D7] ERROR TREND BY WEEK (does model degrade over time?)\n",
      "  --- Poste A ---\n",
      "    Week 17: MAE= 18.57, bias=  -6.42, mean_true= 69.12, n=25\n",
      "    Week 18: MAE= 15.36, bias=  -0.87, mean_true= 63.99, n=50\n",
      "    Week 19: MAE= 14.05, bias=  -2.69, mean_true= 52.03, n=49\n",
      "    Week 20: MAE= 13.07, bias=  -1.12, mean_true= 45.38, n=53\n",
      "    Week 21: MAE= 13.73, bias= +10.94, mean_true= 48.69, n=45\n",
      "    Week 22: MAE= 12.64, bias=  +2.96, mean_true= 48.24, n=57\n",
      "    Week 23: MAE= 14.19, bias= +12.14, mean_true= 49.76, n=47\n",
      "    Week 24: MAE=  9.48, bias=  +1.17, mean_true= 46.03, n=50\n",
      "    Week 25: MAE= 13.25, bias= +10.58, mean_true= 50.66, n=52\n",
      "    Week 26: MAE=  8.64, bias=  -1.96, mean_true= 42.90, n=44\n",
      "    Week 27: MAE=  6.60, bias=  -6.60, mean_true= 52.27, n=2\n",
      "  --- Poste B ---\n",
      "    Week  5: MAE= 17.60, bias= -15.78, mean_true=116.98, n=28\n",
      "    Week  6: MAE= 21.14, bias= -18.87, mean_true=108.49, n=58\n",
      "    Week  7: MAE= 17.63, bias=  -9.23, mean_true=127.15, n=53\n",
      "    Week  8: MAE= 16.04, bias=  -5.26, mean_true=126.12, n=52\n",
      "    Week  9: MAE= 22.71, bias=  -5.41, mean_true=109.78, n=54\n",
      "    Week 10: MAE= 26.51, bias= -24.91, mean_true= 81.55, n=60\n",
      "    Week 11: MAE= 28.07, bias= -22.47, mean_true= 79.69, n=50\n",
      "    Week 12: MAE= 17.43, bias= -14.26, mean_true=113.15, n=48\n",
      "    Week 13: MAE= 29.00, bias= -18.39, mean_true= 79.56, n=51\n",
      "    Week 14: MAE= 30.47, bias= -28.06, mean_true= 83.87, n=55\n",
      "    Week 15: MAE= 49.78, bias= -45.43, mean_true= 52.69, n=60\n",
      "    Week 16: MAE= 44.77, bias= -37.40, mean_true= 56.53, n=45\n",
      "    Week 17: MAE= 46.05, bias= -40.64, mean_true= 56.86, n=45\n",
      "    Week 18: MAE= 56.89, bias= -56.02, mean_true= 48.44, n=61\n",
      "    Week 19: MAE= 48.75, bias= -40.66, mean_true= 42.57, n=55\n",
      "    Week 20: MAE= 50.89, bias= -35.80, mean_true= 41.33, n=51\n",
      "    Week 21: MAE= 45.91, bias= -28.95, mean_true= 45.16, n=48\n",
      "    Week 22: MAE= 49.62, bias= -36.56, mean_true= 46.59, n=51\n",
      "    Week 23: MAE= 47.19, bias= -38.27, mean_true= 47.01, n=62\n",
      "    Week 24: MAE= 50.17, bias= -43.33, mean_true= 43.01, n=60\n",
      "    Week 25: MAE= 49.28, bias= -41.13, mean_true= 51.67, n=42\n",
      "    Week 26: MAE= 44.14, bias= -31.69, mean_true= 44.72, n=36\n",
      "    Week 27: MAE= 36.59, bias= -36.59, mean_true= 33.45, n=1\n",
      "  --- Poste C ---\n",
      "    Week  5: MAE=185.06, bias=-185.06, mean_true=272.46, n=35\n",
      "    Week  6: MAE=174.32, bias=-174.32, mean_true=242.87, n=45\n",
      "    Week  7: MAE=154.24, bias=-150.65, mean_true=281.59, n=68\n",
      "    Week  8: MAE= 89.26, bias= -55.88, mean_true=312.72, n=6\n",
      "\n",
      "[D8] BASELINE COMPARISONS\n",
      "  Our model (v4):                    RMSE = 63.51 kWh\n",
      "  Baseline: per-poste train mean:    RMSE = 63.37 kWh\n",
      "  Baseline: global test mean:        RMSE = 70.75 kWh\n",
      "  Baseline: per-poste test mean*:    RMSE = 40.01 kWh  (* cheating)\n",
      "  Gap to close (v4 vs cheating):     23.50 kWh\n",
      "\n",
      "  Per-poste comparison (our model vs train-mean baseline):\n",
      "    Poste A: model=16.74, baseline=37.54, diff=-20.80 (BETTER)\n",
      "    Poste B: model=44.55, baseline=67.99, diff=-23.45 (BETTER)\n",
      "    Poste C: model=174.83, baseline=87.18, diff=+87.65 (WORSE)\n",
      "\n",
      "[D9] QUICK TEST: GradientBoosting vs Ridge (per poste)\n",
      "  Poste A: Ridge RMSE=16.74, GB RMSE=21.85, GB bias=+17.04 (Ridge WINS)\n",
      "  Poste B: Ridge RMSE=44.55, GB RMSE=47.46, GB bias=-39.50 (Ridge WINS)\n",
      "  Poste C: Ridge RMSE=174.83, GB RMSE=97.32, GB bias=-76.42 (GB WINS)\n",
      "\n",
      "  OVERALL: Ridge RMSE=63.51, GradientBoosting RMSE=49.06\n",
      "\n",
      "================================================================================\n",
      "END OF DEEP DIAGNOSTIC -- COPY EVERYTHING ABOVE THIS LINE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CELL C: DEEP DIAGNOSTIC ‚Äî Where is the model failing and why?\n",
    "# ================================================================\n",
    "# Run AFTER Cell A + Cell B\n",
    "# ================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DEEP DIAGNOSTIC v4 -- COPY EVERYTHING BELOW THIS LINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Rebuild analysis (timezone-safe)\n",
    "analysis = merged.copy()\n",
    "for c in ['heure', 'mois', 'temperature_ext', 'clients_connectes',\n",
    "          'evenement_pointe', 'est_weekend', 'irradiance_solaire',\n",
    "          'tstats_intelligents_connectes']:\n",
    "    if c in test.columns:\n",
    "        analysis[c] = test[c].values\n",
    "analysis['residual'] = analysis['y_true'] - analysis['y_pred']\n",
    "analysis['abs_err'] = analysis['residual'].abs()\n",
    "\n",
    "# ---- [D1] TRAIN vs TEST DISTRIBUTION SHIFT PER POSTE ----\n",
    "print(\"\\n[D1] TRAIN vs TEST FEATURE DISTRIBUTIONS PER POSTE\")\n",
    "shift_cols = ['temperature_ext', 'clients_connectes', 'irradiance_solaire',\n",
    "              'tstats_intelligents_connectes']\n",
    "for p in postes:\n",
    "    tr_p = train[train['poste'] == p]\n",
    "    te_p = test[test['poste'] == p]\n",
    "    print(f\"\\n  --- Poste {p} (train n={len(tr_p)}, test n={len(te_p)}) ---\")\n",
    "    for col in shift_cols:\n",
    "        if col in tr_p.columns and col in te_p.columns:\n",
    "            tr_mean = tr_p[col].mean()\n",
    "            te_mean = te_p[col].mean()\n",
    "            shift_pct = 100 * (te_mean - tr_mean) / (abs(tr_mean) + 1e-8)\n",
    "            print(f\"    {col:35s}: train={tr_mean:8.2f}, test={te_mean:8.2f}, shift={shift_pct:+6.1f}%\")\n",
    "\n",
    "# ---- [D2] INTERCEPT vs ACTUAL MEAN (bias source) ----\n",
    "print(\"\\n[D2] INTERCEPT vs ACTUAL TEST MEAN (shows bias source)\")\n",
    "for p in postes:\n",
    "    te_mean = test.loc[test['poste'] == p, 'energie_kwh'].mean()\n",
    "    intercept = models[p].intercept_\n",
    "    print(f\"  Poste {p}: intercept={intercept:.2f}, test_mean={te_mean:.2f}, \"\n",
    "          f\"gap={intercept - te_mean:+.2f} kWh\")\n",
    "\n",
    "# ---- [D3] COEFFICIENT MAGNITUDE & CONTRIBUTION ANALYSIS ----\n",
    "print(\"\\n[D3] COEFFICIENT MAGNITUDE (L1 norm of coefs)\")\n",
    "for p in postes:\n",
    "    feats_p = features_used[p]\n",
    "    coefs = models[p].coef_\n",
    "    l1_norm = np.abs(coefs).sum()\n",
    "    l2_norm = np.sqrt((coefs ** 2).sum())\n",
    "    max_coef = np.max(np.abs(coefs))\n",
    "    print(f\"  Poste {p}: alpha={models[p].alpha_}, L1={l1_norm:.2f}, L2={l2_norm:.2f}, \"\n",
    "          f\"max|coef|={max_coef:.2f}\")\n",
    "\n",
    "# ---- [D4] FEATURE CONTRIBUTION ON TEST (which features drive overprediction?) ----\n",
    "print(\"\\n[D4] FEATURE CONTRIBUTION ON TEST (mean contribution per feature)\")\n",
    "print(\"     Shows which features push predictions UP or DOWN vs intercept\")\n",
    "for p in postes:\n",
    "    print(f\"\\n  --- Poste {p} ---\")\n",
    "    feats_p = features_used[p]\n",
    "    te_p = test_parts[p]\n",
    "    X_te = te_p[feats_p].values\n",
    "    X_te_s = scalers_per_poste[p].transform(X_te)\n",
    "    coefs = models[p].coef_\n",
    "    # Mean contribution of each feature\n",
    "    mean_contrib = (X_te_s * coefs).mean(axis=0)\n",
    "    contrib_pairs = sorted(zip(feats_p, mean_contrib), key=lambda x: abs(x[1]), reverse=True)\n",
    "    total_contrib = sum(c for _, c in contrib_pairs)\n",
    "    print(f\"    Intercept: {models[p].intercept_:.2f}\")\n",
    "    print(f\"    Total feature contribution: {total_contrib:+.2f}\")\n",
    "    print(f\"    Predicted mean: {models[p].intercept_ + total_contrib:.2f}\")\n",
    "    print(f\"    Actual test mean: {te_p['energie_kwh'].mean():.2f}\")\n",
    "    print(f\"    Top contributors pushing prediction:\")\n",
    "    for feat, contrib in contrib_pairs[:10]:\n",
    "        direction = \"UP\" if contrib > 0 else \"DOWN\"\n",
    "        print(f\"      {feat:35s}: {contrib:+8.2f} ({direction})\")\n",
    "\n",
    "# ---- [D5] CLIENTS_CONNECTES ANALYSIS (key for Poste B/C) ----\n",
    "print(\"\\n[D5] CLIENTS_CONNECTES vs CONSUMPTION PER POSTE\")\n",
    "print(\"     Tests if normalizing by clients would help\")\n",
    "for p in postes:\n",
    "    tr_p = train[train['poste'] == p]\n",
    "    te_p = test[test['poste'] == p]\n",
    "    tr_kwh_per_client = (tr_p['energie_kwh'] / tr_p['clients_connectes']).mean()\n",
    "    te_kwh_per_client = (te_p['energie_kwh'] / te_p['clients_connectes']).mean()\n",
    "    tr_clients = tr_p['clients_connectes'].mean()\n",
    "    te_clients = te_p['clients_connectes'].mean()\n",
    "    print(f\"  Poste {p}: train_kwh/client={tr_kwh_per_client:.3f} (clients={tr_clients:.0f}), \"\n",
    "          f\"test_kwh/client={te_kwh_per_client:.3f} (clients={te_clients:.0f}), \"\n",
    "          f\"shift={100*(te_kwh_per_client-tr_kwh_per_client)/(tr_kwh_per_client+1e-8):+.1f}%\")\n",
    "\n",
    "# ---- [D6] PREDICTION SPREAD ANALYSIS ----\n",
    "print(\"\\n[D6] PREDICTION RANGE vs TRUE RANGE PER POSTE\")\n",
    "for p in postes:\n",
    "    mask = analysis['poste'] == p\n",
    "    sub = analysis[mask]\n",
    "    print(f\"  Poste {p}: pred=[{sub['y_pred'].min():.1f}, {sub['y_pred'].max():.1f}], \"\n",
    "          f\"true=[{sub['y_true'].min():.1f}, {sub['y_true'].max():.1f}], \"\n",
    "          f\"pred_std={sub['y_pred'].std():.2f}, true_std={sub['y_true'].std():.2f}\")\n",
    "\n",
    "# ---- [D7] TEMPORAL TREND ‚Äî Does error grow over time? ----\n",
    "print(\"\\n[D7] ERROR TREND BY WEEK (does model degrade over time?)\")\n",
    "analysis['week'] = pd.to_datetime(analysis['horodatage_local'].values).isocalendar().week.values\n",
    "for p in postes:\n",
    "    print(f\"  --- Poste {p} ---\")\n",
    "    mask_p = analysis['poste'] == p\n",
    "    sub_p = analysis[mask_p]\n",
    "    for w in sorted(sub_p['week'].unique()):\n",
    "        mask_w = sub_p['week'] == w\n",
    "        grp = sub_p[mask_w]\n",
    "        print(f\"    Week {w:2d}: MAE={grp['abs_err'].mean():6.2f}, \"\n",
    "              f\"bias={grp['residual'].mean():+7.2f}, \"\n",
    "              f\"mean_true={grp['y_true'].mean():6.2f}, n={len(grp)}\")\n",
    "\n",
    "# ---- [D8] SIMPLE BASELINES (how good is our model really?) ----\n",
    "print(\"\\n[D8] BASELINE COMPARISONS\")\n",
    "# Baseline 1: Predict train mean per poste\n",
    "pred_train_mean = test['poste'].map(\n",
    "    train.groupby('poste')['energie_kwh'].mean()).values\n",
    "rmse_b1 = np.sqrt(mean_squared_error(test['energie_kwh'], pred_train_mean))\n",
    "# Baseline 2: Predict test mean per poste (cheating ‚Äî best possible constant)\n",
    "pred_test_mean = test['poste'].map(\n",
    "    test.groupby('poste')['energie_kwh'].mean()).values\n",
    "rmse_b2 = np.sqrt(mean_squared_error(test['energie_kwh'], pred_test_mean))\n",
    "# Baseline 3: Just predict global test mean\n",
    "rmse_b3 = np.sqrt(mean_squared_error(test['energie_kwh'],\n",
    "    np.full(len(test), test['energie_kwh'].mean())))\n",
    "\n",
    "print(f\"  Our model (v4):                    RMSE = {rmse_sim:.2f} kWh\")\n",
    "print(f\"  Baseline: per-poste train mean:    RMSE = {rmse_b1:.2f} kWh\")\n",
    "print(f\"  Baseline: global test mean:        RMSE = {rmse_b3:.2f} kWh\")\n",
    "print(f\"  Baseline: per-poste test mean*:    RMSE = {rmse_b2:.2f} kWh  (* cheating)\")\n",
    "print(f\"  Gap to close (v4 vs cheating):     {rmse_sim - rmse_b2:.2f} kWh\")\n",
    "\n",
    "# Per-poste baseline comparison\n",
    "print(\"\\n  Per-poste comparison (our model vs train-mean baseline):\")\n",
    "for p in postes:\n",
    "    mask = test['poste'] == p\n",
    "    y_true_p = test.loc[mask, 'energie_kwh'].values\n",
    "    pred_mean_p = np.full(mask.sum(), train.loc[train['poste'] == p, 'energie_kwh'].mean())\n",
    "    rmse_baseline_p = np.sqrt(mean_squared_error(y_true_p, pred_mean_p))\n",
    "    rmse_model_p = np.sqrt(mean_squared_error(y_true_p, merged.loc[mask, 'y_pred'].values))\n",
    "    better = \"BETTER\" if rmse_model_p < rmse_baseline_p else \"WORSE\"\n",
    "    print(f\"    Poste {p}: model={rmse_model_p:.2f}, baseline={rmse_baseline_p:.2f}, \"\n",
    "          f\"diff={rmse_model_p-rmse_baseline_p:+.2f} ({better})\")\n",
    "\n",
    "# ---- [D9] ALTERNATIVE: WHAT IF WE USED GRADIENT BOOSTING? ----\n",
    "print(\"\\n[D9] QUICK TEST: GradientBoosting vs Ridge (per poste)\")\n",
    "try:\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    for p in postes:\n",
    "        feats_p = features_used[p]\n",
    "        X_tr = train_parts[p][feats_p].values\n",
    "        y_tr = train_parts[p]['energie_kwh'].values\n",
    "        X_te = test_parts[p][feats_p].values\n",
    "        y_te = test_parts[p]['energie_kwh'].values\n",
    "\n",
    "        gb = GradientBoostingRegressor(\n",
    "            n_estimators=200, max_depth=4, learning_rate=0.05,\n",
    "            subsample=0.8, random_state=42)\n",
    "        gb.fit(X_tr, y_tr)\n",
    "        y_pred_gb = np.maximum(gb.predict(X_te), 0)\n",
    "\n",
    "        rmse_ridge = np.sqrt(mean_squared_error(y_te, merged.loc[test['poste']==p, 'y_pred'].values))\n",
    "        rmse_gb = np.sqrt(mean_squared_error(y_te, y_pred_gb))\n",
    "        bias_gb = (y_te - y_pred_gb).mean()\n",
    "        better = \"GB WINS\" if rmse_gb < rmse_ridge else \"Ridge WINS\"\n",
    "        print(f\"  Poste {p}: Ridge RMSE={rmse_ridge:.2f}, GB RMSE={rmse_gb:.2f}, \"\n",
    "              f\"GB bias={bias_gb:+.2f} ({better})\")\n",
    "\n",
    "    # Overall GB RMSE\n",
    "    y_pred_gb_all = pd.Series(index=test.index, dtype=float)\n",
    "    for p in postes:\n",
    "        feats_p = features_used[p]\n",
    "        X_tr = train_parts[p][feats_p].values\n",
    "        y_tr = train_parts[p]['energie_kwh'].values\n",
    "        X_te = test_parts[p][feats_p].values\n",
    "        gb = GradientBoostingRegressor(\n",
    "            n_estimators=200, max_depth=4, learning_rate=0.05,\n",
    "            subsample=0.8, random_state=42)\n",
    "        gb.fit(X_tr, y_tr)\n",
    "        y_pred_gb_all.loc[test_parts[p].index] = np.maximum(gb.predict(X_te), 0)\n",
    "    rmse_gb_all = np.sqrt(mean_squared_error(test['energie_kwh'], y_pred_gb_all.values))\n",
    "    print(f\"\\n  OVERALL: Ridge RMSE={rmse_sim:.2f}, GradientBoosting RMSE={rmse_gb_all:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"  GradientBoosting test failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"END OF DEEP DIAGNOSTIC -- COPY EVERYTHING ABOVE THIS LINE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Soumission Kaggle\n",
    "\n",
    "G√©n√©rez votre fichier de soumission pour la comp√©tition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier de soumission cr√©√©: submission.csv (1754 lignes)\n",
      "Pr√©dictions: min=11.45, mean=113.35, max=418.95\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>energie_kwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>372.219042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>359.115948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>345.320372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>329.355395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>92.095953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  energie_kwh\n",
       "0   0   372.219042\n",
       "1   1   359.115948\n",
       "2   2   345.320372\n",
       "3   3   329.355395\n",
       "4   4    92.095953"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# G√©n√©rer les pr√©dictions pour Kaggle\n",
    "# Uses y_sim_pred from Cell A (predictions in same row order as test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(y_sim_pred)),\n",
    "    'energie_kwh': y_sim_pred\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"Fichier de soumission cr√©√©: submission.csv ({len(submission)} lignes)\")\n",
    "print(f\"Pr√©dictions: min={y_sim_pred.min():.2f}, mean={y_sim_pred.mean():.2f}, max={y_sim_pred.max():.2f}\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Questions de pr√©paration pour l'entrevue orale\n",
    "\n",
    "Pr√©parez-vous √† r√©pondre √† ces questions:\n",
    "\n",
    "### Fondamentaux\n",
    "1. D√©rivez la solution OLS sur le tableau.\n",
    "2. Pourquoi avez-vous utilis√© une division temporelle et non al√©atoire?\n",
    "3. Que voyez-vous dans vos r√©sidus?\n",
    "\n",
    "### R√©gularisation\n",
    "4. Pourquoi Ridge aide-t-il avec des caract√©ristiques corr√©l√©es?\n",
    "5. Comment avez-vous choisi Œª?\n",
    "6. Quel coefficient a √©t√© le plus r√©duit? Pourquoi?\n",
    "\n",
    "### Classification\n",
    "7. Quelle cible binaire avez-vous choisie? Justifiez.\n",
    "8. Votre classifieur donne P=0.7. Qu'est-ce que cela signifie?\n",
    "9. Pourquoi utiliser P(pointe) plut√¥t qu'un indicateur 0/1?\n",
    "\n",
    "### Th√©orie probabiliste\n",
    "10. Expliquez Ridge comme estimation MAP.\n",
    "11. Pourquoi la r√©gression logistique minimise-t-elle l'entropie crois√©e?\n",
    "\n",
    "### Synth√®se\n",
    "12. Parcourez votre mod√®le complet √©tape par √©tape.\n",
    "13. Quelle am√©lioration de R¬≤ √©tait la plus importante?\n",
    "14. Modifiez ce seuil en direct - que pr√©disez-vous?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
