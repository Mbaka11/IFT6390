{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet: Pr√©diction de la demande √©nerg√©tique\n",
    "\n",
    "**IFT3395/IFT6390 - Fondements de l'apprentissage machine**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pierrelux/mlbook/blob/main/exercises/projet_energie.ipynb)\n",
    "\n",
    "**Comp√©tition Kaggle:** [Rejoindre la comp√©tition](https://www.kaggle.com/t/72daeb9bff104caf912f9a0b0f42eb5a)\n",
    "\n",
    "---\n",
    "\n",
    "## Contexte\n",
    "\n",
    "Hydro-Qu√©bec publie des donn√©es ouvertes sur la consommation √©lectrique de clients participant √† un programme de gestion de la demande. Ces donn√©es incluent la consommation horaire, les conditions m√©t√©orologiques, et des indicateurs d'√©v√©nements de pointe.\n",
    "\n",
    "Votre mission: construire un mod√®le de pr√©diction de la consommation √©nerg√©tique en utilisant **uniquement** les m√©thodes vues dans les chapitres 1 √† 5 du cours.\n",
    "\n",
    "## Objectifs d'apprentissage\n",
    "\n",
    "√Ä la fin de ce projet, vous serez en mesure de:\n",
    "\n",
    "1. Impl√©menter les moindres carr√©s ordinaires (OLS) √† partir de z√©ro\n",
    "2. Impl√©menter la r√©gression logistique avec descente de gradient\n",
    "3. Appliquer la r√©gularisation Ridge et interpr√©ter ses effets\n",
    "4. Construire un mod√®le √† deux √©tages: classification ‚Üí r√©gression\n",
    "5. Utiliser les probabilit√©s pr√©dites comme caract√©ristiques\n",
    "\n",
    "## √âvaluation\n",
    "\n",
    "| Composante | Pond√©ration | Description |\n",
    "|------------|-------------|-------------|\n",
    "| **Entrevue orale** | **60%** | V√©rification de la compr√©hension |\n",
    "| Code soumis | 20% | Compl√©tion des parties 1-7 |\n",
    "| Kaggle | 10% | Position au classement |\n",
    "| Rapport √©crit | 10% | Analyse et r√©flexion |\n",
    "\n",
    "### Bar√®me de l'entrevue orale (60%)\n",
    "\n",
    "| Crit√®re | Points | Ce qu'on √©value |\n",
    "|---------|--------|-----------------|\n",
    "| D√©rivation OLS au tableau | 15 | Ma√Ætrise de la solution analytique |\n",
    "| Explication descente de gradient | 10 | Compr√©hension des mises √† jour |\n",
    "| Justification des choix | 15 | Pourquoi ces caract√©ristiques? Pourquoi TimeSeriesSplit? |\n",
    "| Questions th√©oriques | 10 | Ridge = MAP, entropie crois√©e, etc. |\n",
    "| Modifications en direct | 10 | Adapter le code et pr√©dire les effets |\n",
    "\n",
    "**Important**: L'entrevue orale est la composante principale de l'√©valuation. Vous devez √™tre capable d'expliquer et de justifier chaque ligne de code que vous soumettez.\n",
    "\n",
    "### ‚ö†Ô∏è Avertissement sur l'utilisation d'outils IA\n",
    "\n",
    "Les outils comme ChatGPT, Cursor, Copilot peuvent vous aider, **mais** :\n",
    "- Vous devez comprendre **chaque ligne** de code que vous soumettez\n",
    "- L'entrevue orale r√©v√©lera rapidement si vous comprenez ou non\n",
    "- **60% de la note** d√©pend de votre capacit√© √† expliquer votre travail\n",
    "\n",
    "**Conseil** : Utilisez ces outils pour apprendre, pas pour √©viter d'apprendre. Du code copi√© sans compr√©hension m√®ne √† l'√©chec √† l'entrevue orale.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 0: Configuration et chargement des donn√©es\n",
    "\n",
    "Ex√©cutez cette cellule pour importer les biblioth√®ques et charger les donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "print(\"Configuration termin√©e!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des donn√©es\n",
    "\n",
    "Les donn√©es proviennent du jeu de donn√©es ouvert [consommation-clients-evenements-pointe](https://donnees.hydroquebec.com/explore/dataset/consommation-clients-evenements-pointe/) d'Hydro-Qu√©bec. Nous les chargeons directement depuis GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs des donn√©es sur GitHub\n",
    "BASE_URL = \"https://raw.githubusercontent.com/pierrelux/mlbook/main/data/\"\n",
    "\n",
    "# Charger les donn√©es\n",
    "print(\"Chargement des donn√©es depuis GitHub...\")\n",
    "train = pd.read_csv(BASE_URL + \"energy_train.csv\", parse_dates=['horodatage_local'])\n",
    "\n",
    "# Pour l'√©valuation locale: test avec la cible (energie_kwh)\n",
    "test = pd.read_csv(BASE_URL + \"energy_test_avec_cible.csv\", parse_dates=['horodatage_local'])\n",
    "\n",
    "# Pour Kaggle: test sans la cible (pour g√©n√©rer les pr√©dictions)\n",
    "test_kaggle = pd.read_csv(BASE_URL + \"energy_test.csv\", parse_dates=['horodatage_local'])\n",
    "\n",
    "print(f\"Ensemble d'entra√Ænement: {len(train)} observations\")\n",
    "print(f\"Ensemble de test: {len(test)} observations\")\n",
    "print(f\"\\nP√©riode d'entra√Ænement: {train['horodatage_local'].min()} √† {train['horodatage_local'].max()}\")\n",
    "print(f\"P√©riode de test: {test['horodatage_local'].min()} √† {test['horodatage_local'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aper√ßu des donn√©es\n",
    "print(\"Colonnes disponibles:\")\n",
    "print(train.columns.tolist())\n",
    "print(f\"\\nProportion √©v√©nements de pointe (train): {train['evenement_pointe'].mean():.1%}\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description des variables\n",
    "\n",
    "Les donn√©es contiennent des mesures m√©t√©orologiques et temporelles pour pr√©dire la consommation √©nerg√©tique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description des variables\n",
    "print(\"Variables m√©t√©orologiques:\")\n",
    "print(\"  - temperature_ext: Temp√©rature ext√©rieure moyenne (¬∞C)\")\n",
    "print(\"  - humidite: Humidit√© relative moyenne (%)\")\n",
    "print(\"  - vitesse_vent: Vitesse du vent moyenne (km/h)\")\n",
    "print(\"  - neige: Pr√©cipitations de neige moyennes\")\n",
    "print(\"  - irradiance_solaire: Irradiance solaire moyenne\")\n",
    "\n",
    "print(\"\\nVariables temporelles:\")\n",
    "print(\"  - heure, mois, jour, jour_semaine: Composantes temporelles\")\n",
    "print(\"  - heure_sin, heure_cos, mois_sin, mois_cos: Encodage cyclique\")\n",
    "print(\"  - est_weekend, est_ferie: Indicateurs binaires\")\n",
    "\n",
    "print(\"\\nAutres:\")\n",
    "print(\"  - evenement_pointe: Indicateur d'√©v√©nement de pointe (classification)\")\n",
    "print(\"  - energie_kwh: Variable cible (consommation en kWh)\")\n",
    "\n",
    "print(f\"\\nStatistiques de base:\")\n",
    "train[['temperature_ext', 'humidite', 'energie_kwh']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Division temporelle d√©j√† effectu√©e\n",
    "# Les donn√©es de test couvrent la p√©riode √† partir du 1er f√©vrier 2024\n",
    "# NE PAS m√©langer les donn√©es - c'est une s√©rie temporelle!\n",
    "\n",
    "print(\"‚ö†Ô∏è  ATTENTION: Division temporelle\")\n",
    "print(\"Les ensembles train/test sont d√©j√† s√©par√©s chronologiquement.\")\n",
    "print(\"N'utilisez PAS de validation crois√©e al√©atoire (fuite d'information).\")\n",
    "print(\"\\nPour la validation, utilisez une division temporelle sur train:\")\n",
    "print(\"  - Ex: train[:6000] pour entra√Ænement, train[6000:] pour validation\")\n",
    "\n",
    "# Note: il y a un d√©calage de distribution entre train (hiver) et test (printemps/√©t√©)\n",
    "# C'est un d√©fi r√©aliste! Pensez √† utiliser des caract√©ristiques qui g√©n√©ralisent bien.\n",
    "print(\"\\nüìä D√©calage de distribution:\")\n",
    "print(f\"  Train: {train['energie_kwh'].mean():.1f} kWh (hiver)\")\n",
    "print(f\"  Test:  {test['energie_kwh'].mean():.1f} kWh (printemps/√©t√©)\")\n",
    "print(\"  ‚Üí Le mod√®le doit g√©n√©raliser √† travers les saisons!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration visuelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Consommation vs temp√©rature\n",
    "axes[0, 0].scatter(train['temperature_ext'], train['energie_kwh'], alpha=0.3, s=5)\n",
    "axes[0, 0].set_xlabel('Temp√©rature (¬∞C)')\n",
    "axes[0, 0].set_ylabel('√ânergie consomm√©e (kWh)')\n",
    "axes[0, 0].set_title('Consommation vs Temp√©rature')\n",
    "\n",
    "# Distribution de la consommation\n",
    "axes[0, 1].hist(train['energie_kwh'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('√ânergie (kWh)')\n",
    "axes[0, 1].set_ylabel('Fr√©quence')\n",
    "axes[0, 1].set_title('Distribution de la consommation')\n",
    "\n",
    "# Profil horaire\n",
    "profil_horaire = train.groupby('heure')['energie_kwh'].mean()\n",
    "axes[1, 0].bar(profil_horaire.index, profil_horaire.values)\n",
    "axes[1, 0].set_xlabel('Heure')\n",
    "axes[1, 0].set_ylabel('√ânergie moyenne (kWh)')\n",
    "axes[1, 0].set_title('Profil de consommation horaire')\n",
    "\n",
    "# √âv√©nements de pointe par heure\n",
    "pointe_horaire = train.groupby('heure')['evenement_pointe'].mean()\n",
    "axes[1, 1].bar(pointe_horaire.index, pointe_horaire.values)\n",
    "axes[1, 1].set_xlabel('Heure')\n",
    "axes[1, 1].set_ylabel('Proportion √©v√©nements de pointe')\n",
    "axes[1, 1].set_title('Fr√©quence des √©v√©nements de pointe')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ANALYSE EXPLORATOIRE APPROFONDIE\n",
    "# ============================================\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYSE STATISTIQUE D√âTAILL√âE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Matrice de corr√©lation des variables principales\n",
    "features_corr = ['temperature_ext', 'humidite', 'vitesse_vent', \n",
    "                 'irradiance_solaire', 'clients_connectes', 'energie_kwh']\n",
    "\n",
    "corr_matrix = train[features_corr].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Matrice de Corr√©lation - Variables Cl√©s', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 corr√©lations avec energie_kwh:\")\n",
    "corr_with_target = corr_matrix['energie_kwh'].sort_values(ascending=False)[1:6]\n",
    "print(corr_with_target)\n",
    "\n",
    "\n",
    "# 2. Distribution par saison et jour de la semaine\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Consommation par mois\n",
    "monthly_stats = train.groupby('mois')['energie_kwh'].agg(['mean', 'std'])\n",
    "axes[0, 0].bar(monthly_stats.index, monthly_stats['mean'], \n",
    "               yerr=monthly_stats['std'], capsize=5, alpha=0.7, color='steelblue')\n",
    "axes[0, 0].set_xlabel('Mois')\n",
    "axes[0, 0].set_ylabel('Consommation moyenne (kWh)')\n",
    "axes[0, 0].set_title('Variations Mensuelles (avec √©cart-type)')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Consommation par jour de la semaine\n",
    "weekly_stats = train.groupby('jour_semaine')['energie_kwh'].mean()\n",
    "days = ['Lun', 'Mar', 'Mer', 'Jeu', 'Ven', 'Sam', 'Dim']\n",
    "axes[0, 1].bar(range(7), weekly_stats, color='coral', alpha=0.7)\n",
    "axes[0, 1].set_xticks(range(7))\n",
    "axes[0, 1].set_xticklabels(days)\n",
    "axes[0, 1].set_ylabel('Consommation moyenne (kWh)')\n",
    "axes[0, 1].set_title('Variations Hebdomadaires')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Temp√©rature vs Consommation (avec r√©gression)\n",
    "axes[1, 0].scatter(train['temperature_ext'], train['energie_kwh'], \n",
    "                   alpha=0.2, s=3, c=train['heure'], cmap='viridis')\n",
    "# Ajouter courbe de tendance\n",
    "z = np.polyfit(train['temperature_ext'], train['energie_kwh'], 2)\n",
    "p = np.poly1d(z)\n",
    "temp_range = np.linspace(train['temperature_ext'].min(), \n",
    "                         train['temperature_ext'].max(), 100)\n",
    "axes[1, 0].plot(temp_range, p(temp_range), 'r-', linewidth=2, \n",
    "                label='Tendance polynomiale')\n",
    "axes[1, 0].set_xlabel('Temp√©rature (¬∞C)')\n",
    "axes[1, 0].set_ylabel('√ânergie (kWh)')\n",
    "axes[1, 0].set_title('Relation Temp√©rature-Consommation (color√© par heure)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# clients_connectes vs energie_kwh (tr√®s important!)\n",
    "axes[1, 1].scatter(train['clients_connectes'], train['energie_kwh'], \n",
    "                   alpha=0.3, s=5, c='green')\n",
    "axes[1, 1].set_xlabel('Nombre de clients connect√©s')\n",
    "axes[1, 1].set_ylabel('√ânergie (kWh)')\n",
    "axes[1, 1].set_title('Impact du Nombre de Clients (corr√©lation forte!)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 3. Analyse des √©v√©nements de pointe\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSE DES √âV√âNEMENTS DE POINTE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pointe_stats = train.groupby('evenement_pointe')['energie_kwh'].describe()\n",
    "print(\"\\nStatistiques consommation par type:\")\n",
    "print(pointe_stats)\n",
    "\n",
    "print(f\"\\nRatio consommation pointe/normal: \"\n",
    "      f\"{pointe_stats.loc[1, 'mean'] / pointe_stats.loc[0, 'mean']:.2f}x\")\n",
    "\n",
    "# Boxplot comparatif\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].boxplot([train[train['evenement_pointe']==0]['energie_kwh'],\n",
    "                 train[train['evenement_pointe']==1]['energie_kwh']],\n",
    "                labels=['Normal', 'Pointe'])\n",
    "axes[0].set_ylabel('Consommation (kWh)')\n",
    "axes[0].set_title('Distribution Consommation: Normal vs Pointe')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Heures de pointe par temp√©rature\n",
    "temp_bins = pd.cut(train['temperature_ext'], bins=5)\n",
    "pointe_by_temp = train.groupby([temp_bins, 'heure'])['evenement_pointe'].mean().unstack()\n",
    "\n",
    "im = axes[1].imshow(pointe_by_temp.T, cmap='YlOrRd', aspect='auto')\n",
    "axes[1].set_xlabel('Plage de temp√©rature')\n",
    "axes[1].set_ylabel('Heure')\n",
    "axes[1].set_title('Probabilit√© Pointe par Temp√©rature et Heure')\n",
    "plt.colorbar(im, ax=axes[1], label='P(pointe)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 4. Test de stationnarit√© (d√©calage train/test)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSE D√âCALAGE TRAIN/TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Variable': ['Temp√©rature', 'Humidit√©', 'Vent', '√ânergie', 'Pointe (%)'],\n",
    "    'Train (moyenne)': [\n",
    "        train['temperature_ext'].mean(),\n",
    "        train['humidite'].mean(),\n",
    "        train['vitesse_vent'].mean(),\n",
    "        train['energie_kwh'].mean(),\n",
    "        train['evenement_pointe'].mean()*100\n",
    "    ],\n",
    "    'Test (moyenne)': [\n",
    "        test['temperature_ext'].mean(),\n",
    "        test['humidite'].mean(),\n",
    "        test['vitesse_vent'].mean(),\n",
    "        test['energie_kwh'].mean(),\n",
    "        test['evenement_pointe'].mean()*100\n",
    "    ]\n",
    "})\n",
    "comparison['√âcart (%)'] = 100 * (comparison['Test (moyenne)'] - \n",
    "                                  comparison['Train (moyenne)']) / comparison['Train (moyenne)']\n",
    "\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"\\nObservation: Le d√©calage train/test est significatif!\")\n",
    "print(\"Strat√©gie: Utiliser features qui g√©n√©ralisent bien (degr√©-jours, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 1: Impl√©mentation OLS (10%)\n",
    "\n",
    "Avant d'utiliser scikit-learn, vous devez impl√©menter la solution analytique des moindres carr√©s ordinaires.\n",
    "\n",
    "**Rappel**: La solution OLS est donn√©e par:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "\n",
    "Pour des raisons de stabilit√© num√©rique, pr√©f√©rez `np.linalg.solve` √† l'inversion directe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_fit(X, y):\n",
    "    \"\"\"\n",
    "    Calcule les coefficients OLS.\n",
    "    \n",
    "    Param√®tres:\n",
    "        X : ndarray de forme (n, p) - matrice de caract√©ristiques (SANS colonne de 1)\n",
    "        y : ndarray de forme (n,) - vecteur cible\n",
    "    \n",
    "    Retourne:\n",
    "        beta : ndarray de forme (p+1,) - coefficients [intercept, coef1, coef2, ...]\n",
    "    \n",
    "    Indice: Ajoutez une colonne de 1 √† X pour l'intercept.\n",
    "    \"\"\"\n",
    "    # VOTRE CODE ICI\n",
    "    # 1. Ajouter une colonne de 1 pour l'intercept\n",
    "    n = X.shape[0]\n",
    "    X_with_intercept = np.column_stack((np.ones(n), X))\n",
    "    # 2. R√©soudre le syst√®me X^T X beta = X^T y\n",
    "    XTX = X_with_intercept.T @ X_with_intercept\n",
    "    XTy = X_with_intercept.T @ y\n",
    "    # 3. Retourner beta\n",
    "    beta = np.linalg.solve(XTX, XTy)\n",
    "    return beta    \n",
    "\n",
    "\n",
    "def ols_predict(X, beta):\n",
    "    \"\"\"\n",
    "    Pr√©dit avec les coefficients OLS.\n",
    "    \n",
    "    Param√®tres:\n",
    "        X : ndarray de forme (n, p) - caract√©ristiques (SANS colonne de 1)\n",
    "        beta : ndarray de forme (p+1,) - coefficients [intercept, coef1, ...]\n",
    "    \n",
    "    Retourne:\n",
    "        y_pred : ndarray de forme (n,)\n",
    "    \"\"\"\n",
    "    # VOTRE CODE ICI\n",
    "    n = X.shape[0]\n",
    "    X_with_intercept = np.column_stack((np.ones(n), X))\n",
    "    y_pred = X_with_intercept @ beta\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre impl√©mentation\n",
    "# Caract√©ristiques simples pour commencer\n",
    "features_base = ['temperature_ext', 'humidite', 'vitesse_vent']\n",
    "\n",
    "X_train_base = train[features_base].values\n",
    "y_train = train['energie_kwh'].values\n",
    "X_test_base = test[features_base].values\n",
    "y_test = test['energie_kwh'].values\n",
    "\n",
    "# Votre impl√©mentation\n",
    "beta_ols = ols_fit(X_train_base, y_train)\n",
    "y_pred_ols = ols_predict(X_test_base, beta_ols)\n",
    "\n",
    "# Validation avec sklearn\n",
    "model_sklearn = LinearRegression()\n",
    "model_sklearn.fit(X_train_base, y_train)\n",
    "y_pred_sklearn = model_sklearn.predict(X_test_base)\n",
    "\n",
    "# Comparaison\n",
    "print(\"Comparaison OLS impl√©ment√© vs sklearn:\")\n",
    "print(f\"  Intercept - Vous: {beta_ols[0]:.4f}, sklearn: {model_sklearn.intercept_:.4f}\")\n",
    "print(f\"  Coefficients proches: {np.allclose(beta_ols[1:], model_sklearn.coef_, atol=1e-4)}\")\n",
    "print(f\"\\nR¬≤ sur test: {r2_score(y_test, y_pred_ols):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE √Ä AJOUTER - Diagnostiques OLS\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIAGNOSTIQUES OLS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Analyse des r√©sidus\n",
    "residus_ols = y_test - y_pred_ols\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# R√©sidus vs pr√©dictions\n",
    "axes[0, 0].scatter(y_pred_ols, residus_ols, alpha=0.4, s=10)\n",
    "axes[0, 0].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Pr√©dictions')\n",
    "axes[0, 0].set_ylabel('R√©sidus')\n",
    "axes[0, 0].set_title('R√©sidus vs Pr√©dictions (h√©t√©rosc√©dasticit√©?)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# QQ-plot (normalit√© des r√©sidus)\n",
    "from scipy import stats\n",
    "stats.probplot(residus_ols, dist=\"norm\", plot=axes[0, 1])\n",
    "axes[0, 1].set_title('Q-Q Plot (Test de Normalit√©)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Histogramme r√©sidus\n",
    "axes[1, 0].hist(residus_ols, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(residus_ols.mean(), color='red', linestyle='--',\n",
    "                   label=f'Moyenne: {residus_ols.mean():.2f}')\n",
    "axes[1, 0].set_xlabel('R√©sidu')\n",
    "axes[1, 0].set_ylabel('Fr√©quence')\n",
    "axes[1, 0].set_title('Distribution des R√©sidus')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Autocorr√©lation des r√©sidus (important pour s√©ries temporelles!)\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(pd.Series(residus_ols), ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Autocorr√©lation des R√©sidus')\n",
    "axes[1, 1].set_xlabel('Lag')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tests statistiques\n",
    "print(f\"\\nStatistiques r√©sidus:\")\n",
    "print(f\"  Moyenne: {residus_ols.mean():.4f} (devrait √™tre ~0)\")\n",
    "print(f\"  √âcart-type: {residus_ols.std():.2f}\")\n",
    "print(f\"  Min: {residus_ols.min():.2f}, Max: {residus_ols.max():.2f}\")\n",
    "\n",
    "# Test de Shapiro-Wilk (normalit√©)\n",
    "if len(residus_ols) < 5000:  # Limitation du test\n",
    "    stat, p_value = stats.shapiro(residus_ols[:5000])\n",
    "    print(f\"  Test Shapiro-Wilk: p-value = {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"    ‚Üí R√©sidus NON normaux (mais OK pour grandes donn√©es)\")\n",
    "\n",
    "# 2. Coefficients OLS\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"INTERPR√âTATION DES COEFFICIENTS OLS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': features_base,\n",
    "    'Coefficient': beta_ols[1:],\n",
    "    '|Coefficient|': np.abs(beta_ols[1:])\n",
    "}).sort_values('|Coefficient|', ascending=False)\n",
    "\n",
    "print(coef_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nIntercept: {beta_ols[0]:.2f} kWh\")\n",
    "print(\"\\nInterpr√©tation exemple:\")\n",
    "print(f\"  - {features_base[0]}: coefficient = {beta_ols[1]:.2f}\")\n",
    "print(f\"    ‚Üí +1¬∞C ‚Üí {beta_ols[1]:+.2f} kWh de consommation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 2: R√©gression logistique avec descente de gradient (15%)\n",
    "\n",
    "Impl√©mentez la r√©gression logistique pour la classification binaire.\n",
    "\n",
    "**Rappels**:\n",
    "- Fonction sigmo√Øde: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "- Perte d'entropie crois√©e: $L = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]$\n",
    "- Gradient: $\\nabla L = \\frac{1}{n} \\mathbf{X}^\\top (\\sigma(\\mathbf{X}\\boldsymbol{\\beta}) - \\mathbf{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Fonction sigmo√Øde.\n",
    "    \n",
    "    Indice: Pour la stabilit√© num√©rique, clip z entre -500 et 500.\n",
    "    \"\"\"\n",
    "    # VOTRE CODE ICI\n",
    "    z_clipped = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z_clipped))\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Calcule la perte d'entropie crois√©e binaire.\n",
    "    \n",
    "    Indice: Clip les probabilit√©s pour √©viter log(0).\n",
    "    \"\"\"\n",
    "    # VOTRE CODE ICI\n",
    "    epsilon = 1e-15\n",
    "    y_pred_proba_clipped = np.clip(y_pred_proba, epsilon, 1 - epsilon)\n",
    "    loss = -np.mean(y_true * np.log(y_pred_proba_clipped) + (1 - y_true) * np.log(1 - y_pred_proba_clipped))\n",
    "    return loss\n",
    "\n",
    "def logistic_gradient(X, y, beta):\n",
    "    \"\"\"\n",
    "    Calcule le gradient de la perte d'entropie crois√©e.\n",
    "    \n",
    "    Param√®tres:\n",
    "        X : ndarray (n, p+1) - caract√©ristiques AVEC colonne de 1\n",
    "        y : ndarray (n,) - √©tiquettes binaires\n",
    "        beta : ndarray (p+1,) - coefficients actuels\n",
    "    \n",
    "    Retourne:\n",
    "        gradient : ndarray (p+1,)\n",
    "    \"\"\"\n",
    "    # VOTRE CODE ICI\n",
    "    n = len(y)\n",
    "    z = X @ beta\n",
    "    y_pred_proba = sigmoid(z)\n",
    "    error = y_pred_proba - y\n",
    "    gradient = (X.T @ error) / n\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "def logistic_fit_gd(X, y, lr=0.1, n_iter=1000, verbose=False):\n",
    "    \"\"\"\n",
    "    Entra√Æne la r√©gression logistique par descente de gradient.\n",
    "    \n",
    "    Param√®tres:\n",
    "        X : ndarray (n, p) - caract√©ristiques SANS colonne de 1\n",
    "        y : ndarray (n,) - √©tiquettes binaires (0 ou 1)\n",
    "        lr : float - taux d'apprentissage\n",
    "        n_iter : int - nombre d'it√©rations\n",
    "        verbose : bool - afficher la progression\n",
    "    \n",
    "    Retourne:\n",
    "        beta : ndarray (p+1,) - coefficients [intercept, coef1, ...]\n",
    "        losses : list - historique des pertes\n",
    "    \"\"\"\n",
    "    # VOTRE CODE ICI\n",
    "    n, p = X.shape\n",
    "    # 1. Ajouter colonne de 1 √† X\n",
    "    X_with_intercept = np.column_stack([np.ones(n), X])\n",
    "    # 2. Initialiser beta √† z√©ro\n",
    "    beta = np.zeros(p + 1)\n",
    "    losses = []\n",
    "    # 3. Boucle de descente de gradient\n",
    "    for i in range(n_iter):\n",
    "        Z = X_with_intercept @ beta\n",
    "        y_pred_proba = sigmoid(Z)\n",
    "        \n",
    "        loss = cross_entropy_loss(y, y_pred_proba)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        gradient = logistic_gradient(X_with_intercept, y, beta)\n",
    "        \n",
    "        beta -= lr * gradient\n",
    "        \n",
    "        if verbose and (i % 100 == 0 or i == n_iter - 1):\n",
    "            print(f\"Iteration {i+1}/{n_iter}, Loss: {loss:.4f}\")\n",
    "    # 4. Retourner beta et historique des pertes\n",
    "    return beta, losses\n",
    "\n",
    "\n",
    "def logistic_predict_proba(X, beta):\n",
    "    \"\"\"\n",
    "    Retourne les probabilit√©s P(Y=1|X).\n",
    "    \"\"\"\n",
    "    # VOTRE CODE ICI\n",
    "    n = X.shape[0]\n",
    "    X_with_intercept = np.column_stack([np.ones(n), X])\n",
    "    z = X_with_intercept @ beta\n",
    "    return sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sur la pr√©diction des √©v√©nements de pointe\n",
    "# Caract√©ristiques pour classification\n",
    "features_clf = ['temperature_ext', 'heure_sin', 'heure_cos', 'est_weekend']\n",
    "\n",
    "X_train_clf = train[features_clf].values\n",
    "y_train_clf = train['evenement_pointe'].values\n",
    "X_test_clf = test[features_clf].values\n",
    "y_test_clf = test['evenement_pointe'].values\n",
    "\n",
    "# Normaliser (recommand√© pour la descente de gradient)\n",
    "scaler = StandardScaler()\n",
    "X_train_clf_scaled = scaler.fit_transform(X_train_clf)\n",
    "X_test_clf_scaled = scaler.transform(X_test_clf)\n",
    "\n",
    "# Entra√Æner votre mod√®le\n",
    "beta_log, losses = logistic_fit_gd(X_train_clf_scaled, y_train_clf, lr=0.1, n_iter=500, verbose=True)\n",
    "\n",
    "# Tracer la courbe de convergence\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('It√©ration')\n",
    "plt.ylabel('Perte (entropie crois√©e)')\n",
    "plt.title('Convergence de la descente de gradient')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluation\n",
    "proba_train = logistic_predict_proba(X_train_clf_scaled, beta_log)\n",
    "proba_test = logistic_predict_proba(X_test_clf_scaled, beta_log)\n",
    "\n",
    "y_pred_train = (proba_train >= 0.5).astype(int)\n",
    "y_pred_test = (proba_test >= 0.5).astype(int)\n",
    "\n",
    "print(\"√âvaluation de votre r√©gression logistique:\")\n",
    "print(f\"  Accuracy (train): {accuracy_score(y_train_clf, y_pred_train):.4f}\")\n",
    "print(f\"  Accuracy (test): {accuracy_score(y_test_clf, y_pred_test):.4f}\")\n",
    "print(f\"\\nRapport de classification (test):\")\n",
    "print(classification_report(y_test_clf, y_pred_test, target_names=['Normal', 'Pointe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSE APPROFONDIE CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Matrice de confusion d√©taill√©e\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test_clf, y_pred_test)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Matrice de confusion\n",
    "im = axes[0].imshow(cm, cmap='Blues')\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_yticks([0, 1])\n",
    "axes[0].set_xticklabels(['Normal', 'Pointe'])\n",
    "axes[0].set_yticklabels(['Normal', 'Pointe'])\n",
    "axes[0].set_xlabel('Pr√©dit')\n",
    "axes[0].set_ylabel('R√©el')\n",
    "axes[0].set_title('Matrice de Confusion')\n",
    "\n",
    "# Annoter nombres\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = axes[0].text(j, i, cm[i, j],\n",
    "                           ha=\"center\", va=\"center\", color=\"black\", fontsize=16)\n",
    "\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "\n",
    "# Calculer m√©triques\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"\\nM√©triques d√©taill√©es:\")\n",
    "print(f\"  True Negatives:  {tn}\")\n",
    "print(f\"  False Positives: {fp} (fausses alarmes)\")\n",
    "print(f\"  False Negatives: {fn} (pointes manqu√©es)\")\n",
    "print(f\"  True Positives:  {tp}\")\n",
    "print(f\"\\n  Precision: {precision:.4f} (Quand on pr√©dit pointe, c'est vrai dans {precision*100:.1f}% cas)\")\n",
    "print(f\"  Recall:    {recall:.4f} (On d√©tecte {recall*100:.1f}% des vraies pointes)\")\n",
    "print(f\"  F1-score:  {f1:.4f}\")\n",
    "\n",
    "\n",
    "# 2. Courbe ROC et AUC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test_clf, proba_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "axes[1].plot(fpr, tpr, color='darkorange', lw=2,\n",
    "            label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',\n",
    "            label='Hasard')\n",
    "axes[1].set_xlabel('Taux Faux Positifs (FPR)')\n",
    "axes[1].set_ylabel('Taux Vrais Positifs (TPR)')\n",
    "axes[1].set_title('Courbe ROC')\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "print(f\"\\n  AUC-ROC: {roc_auc:.4f} (1.0 = parfait, 0.5 = hasard)\")\n",
    "\n",
    "\n",
    "# 3. Precision-Recall selon seuil\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision_vals, recall_vals, thresholds_pr = precision_recall_curve(y_test_clf, proba_test)\n",
    "\n",
    "axes[2].plot(recall_vals, precision_vals, color='green', lw=2)\n",
    "axes[2].set_xlabel('Recall')\n",
    "axes[2].set_ylabel('Precision')\n",
    "axes[2].set_title('Courbe Precision-Recall')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Marquer le seuil 0.5\n",
    "idx_05 = np.argmin(np.abs(thresholds_pr - 0.5))\n",
    "axes[2].plot(recall_vals[idx_05], precision_vals[idx_05], 'ro',\n",
    "            markersize=10, label='Seuil = 0.5')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 4. Analyse par seuil\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EFFET DU SEUIL DE D√âCISION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "seuils_test = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "results_seuils = []\n",
    "\n",
    "for seuil in seuils_test:\n",
    "    y_pred_seuil = (proba_test >= seuil).astype(int)\n",
    "    acc = accuracy_score(y_test_clf, y_pred_seuil)\n",
    "    cm_seuil = confusion_matrix(y_test_clf, y_pred_seuil)\n",
    "    tn, fp, fn, tp = cm_seuil.ravel()\n",
    "    prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    results_seuils.append({\n",
    "        'Seuil': seuil,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    })\n",
    "\n",
    "df_seuils = pd.DataFrame(results_seuils)\n",
    "print(df_seuils.to_string(index=False))\n",
    "\n",
    "print(\"\\nInterpr√©tation:\")\n",
    "print(\"  - Seuil bas (0.3): Plus de d√©tections ‚Üí Recall √©lev√©, mais plus de fausses alarmes\")\n",
    "print(\"  - Seuil √©lev√© (0.7): Moins de fausses alarmes ‚Üí Precision √©lev√©e, mais pointes manqu√©es\")\n",
    "print(\"  - Trade-off selon contexte m√©tier!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 3: Ing√©nierie des caract√©ristiques (15%)\n",
    "\n",
    "**√Ä partir de maintenant, vous pouvez utiliser scikit-learn.**\n",
    "\n",
    "Cr√©ez des caract√©ristiques temporelles pour am√©liorer le mod√®le de r√©gression.\n",
    "\n",
    "### Caract√©ristiques √† impl√©menter:\n",
    "\n",
    "1. **Retards (lags)**: consommation aux heures pr√©c√©dentes\n",
    "2. **Statistiques glissantes**: moyenne mobile, √©cart-type mobile\n",
    "3. **Interactions**: temp√©rature √ó heure, etc.\n",
    "\n",
    "Impl√©mentez **au moins 3 nouvelles caract√©ristiques**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creer_caracteristiques(df):\n",
    "    \"\"\"\n",
    "    Cr√©e des caract√©ristiques suppl√©mentaires.\n",
    "    \n",
    "    VOUS DEVEZ IMPL√âMENTER AU MOINS 3 NOUVELLES CARACT√âRISTIQUES.\n",
    "    \n",
    "    Id√©es:\n",
    "    - Retards: df['energie_kwh'].shift(1), shift(24)\n",
    "    - Moyennes mobiles: df['energie_kwh'].rolling(6).mean()\n",
    "    - Interactions: df['temperature_ext'] * df['heure_cos']\n",
    "    - Degr√©-jours de chauffage: np.maximum(18 - df['temperature_ext'], 0)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # VOTRE CODE ICI\n",
    "    # Exemple:\n",
    "    df['energie_lag1'] = df['energie_kwh'].shift(1)\n",
    "    df['energie_lag24'] = df['energie_kwh'].shift(24)\n",
    "    df['energie_lag168'] = df['energie_kwh'].shift(168)\n",
    "    \n",
    "    df['energie_rolling_6h'] = df['energie_kwh'].rolling(6).mean()\n",
    "    df['energie_rolling_24h'] = df['energie_kwh'].rolling(24).mean()\n",
    "    df['energie_rolling_std_24h'] = df['energie_kwh'].rolling(\n",
    "        window=24, \n",
    "        min_periods=1\n",
    "    ).std().fillna(0)\n",
    "    df['energie_rolling_max_12h'] = df['energie_kwh'].rolling(\n",
    "        window = 12,\n",
    "        min_periods=1\n",
    "    ).max()\n",
    "    \n",
    "    df['temp_heure_cos'] = df['temperature_ext'] * df['heure_cos']\n",
    "    df['temp_heure_sin'] = df['temperature_ext'] * df['heure_sin']\n",
    "    df['temp_weekend'] = df['temperature_ext'] * df['est_weekend']\n",
    "    df['temp_mois_sin'] = df['temperature_ext'] * df['mois_sin']\n",
    "    df['temp_mois_cos'] = df['temperature_ext'] * df['mois_cos']\n",
    "    \n",
    "    df['degres_jours_chauffage'] = np.maximum(18 - df['temperature_ext'], 0)\n",
    "    df['degres_jours_clim'] = np.maximum(df['temperature_ext'] - 22, 0)\n",
    "    df['temp_squared'] = df['temperature_ext'] ** 2\n",
    "    df['temp_ressentie'] = df['temperature_ext'] - 0.5 * df['vitesse_vent']\n",
    "    df['humidite_temp'] = df['humidite'] * np.abs(df['temperature_ext']) / 100\n",
    "    df['est_pointe_matin'] = ((df['heure'] >= 7) & (df['heure'] <= 9)).astype(int)\n",
    "    \n",
    "    df['est_pointe_soir'] = ((df['heure'] >= 17) & (df['heure'] <= 20)).astype(int)\n",
    "    df['est_nuit'] = ((df['heure'] >= 0) & (df['heure'] <= 6)).astype(int)\n",
    "    df['est_hiver'] = df['mois'].isin([12, 1, 2]).astype(int)\n",
    "    df['est_ete'] = df['mois'].isin([6, 7, 8]).astype(int)\n",
    "    \n",
    "    df['temp_rolling_mean_3h'] = df['temperature_ext'].rolling(\n",
    "        window=3, \n",
    "        min_periods=1\n",
    "    ).mean()\n",
    "    df['temp_diff'] = df['temperature_ext'].diff().fillna(0)\n",
    "    df['temp_amplitude_24h'] = (\n",
    "        df['temperature_ext'].rolling(window=24, min_periods=1).max() - \n",
    "        df['temperature_ext'].rolling(window=24, min_periods=1).min()\n",
    "    )\n",
    "    \n",
    "    if 'clients_connectes' in df.columns:\n",
    "        df['clients_temp'] = df['clients_connectes'] * df['temperature_ext']\n",
    "        df['energie_per_client'] = df['energie_kwh'] / (df['clients_connectes'] + 1)\n",
    "        df['clients_weekend'] = df['clients_connectes'] * df['est_weekend']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Appliquer aux donn√©es\n",
    "train_eng = creer_caracteristiques(train)\n",
    "test_eng = creer_caracteristiques(test)\n",
    "\n",
    "# Supprimer les lignes avec NaN (dues aux retards)\n",
    "train_eng = train_eng.dropna().reset_index(drop=True)\n",
    "test_eng = test_eng.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"Nouvelles colonnes: {[c for c in train_eng.columns if c not in train.columns]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_enrichi = creer_caracteristiques(train)\n",
    "test_enrichi = creer_caracteristiques(test)\n",
    "\n",
    "# Supprimer les NaN (dus aux lags/rolling)\n",
    "train_enrichi = train_enrichi.dropna().reset_index(drop=True)\n",
    "test_enrichi = test_enrichi.dropna().reset_index(drop=True)\n",
    "\n",
    "# V√©rifier les nouvelles colonnes\n",
    "nouvelles_cols = [c for c in train_enrichi.columns if c not in train.columns]\n",
    "print(f\"Nombre de nouvelles features: {len(nouvelles_cols)}\")\n",
    "print(f\"\\nNouvelles features cr√©√©es:\")\n",
    "for col in nouvelles_cols:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# V√©rifier corr√©lations avec la cible\n",
    "correlations = train_enrichi[nouvelles_cols + ['energie_kwh']].corr()['energie_kwh'].sort_values(ascending=False)\n",
    "print(f\"\\nTop 10 features par corr√©lation avec energie_kwh:\")\n",
    "print(correlations.head(10))\n",
    "\n",
    "features_to_use = [\n",
    "    # M√©t√©o de base\n",
    "    'temperature_ext', 'humidite', 'vitesse_vent', 'irradiance_solaire',\n",
    "    \n",
    "    # Temps cyclique\n",
    "    'heure_sin', 'heure_cos', 'mois_sin', 'mois_cos',\n",
    "    'jour_semaine_sin', 'jour_semaine_cos',\n",
    "    \n",
    "    # Indicateurs binaires\n",
    "    'est_weekend', 'est_ferie', 'est_pointe_matin', 'est_pointe_soir',\n",
    "    \n",
    "    # TR√àS IMPORTANT\n",
    "    'clients_connectes',\n",
    "    \n",
    "    # Lags (attention Kaggle!)\n",
    "    'energie_lag1', 'energie_lag24',\n",
    "    \n",
    "    # Rolling\n",
    "    'energie_rolling_mean_6h', 'energie_rolling_mean_24h',\n",
    "    \n",
    "    # Interactions\n",
    "    'temp_heure_cos', 'temp_weekend',\n",
    "    \n",
    "    # Transformations m√©t√©o\n",
    "    'degres_jours_chauffage', 'temp_squared'\n",
    "]\n",
    "\n",
    "# Filtrer celles qui existent vraiment\n",
    "features_disponibles = [f for f in features_to_use if f in train_enrichi.columns]\n",
    "\n",
    "print(f\"\\nFeatures s√©lectionn√©es: {len(features_disponibles)}\")\n",
    "\n",
    "X_train = train_enrichi[features_disponibles].values\n",
    "y_train = train_enrichi['energie_kwh'].values\n",
    "X_test = test_enrichi[features_disponibles].values\n",
    "y_test = test_enrichi['energie_kwh'].values\n",
    "\n",
    "# Entra√Æner un mod√®le simple pour tester\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print(f\"\\nR¬≤ avec features enrichies: {r2_score(y_test, model.predict(X_test)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ANALYSE D'IMPACT DES NOUVELLES FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Heatmap corr√©lations nouvelles features\n",
    "nouvelles_features_sample = nouvelles_cols[:15]  # Top 15\n",
    "corr_nouvelles = train_enrichi[nouvelles_features_sample + ['energie_kwh']].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_nouvelles, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, square=True, linewidths=0.5, cbar_kws={'label': 'Corr√©lation'})\n",
    "plt.title('Corr√©lations Nouvelles Features avec Cible', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Test incr√©mental d'ajout de features\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TEST INCR√âMENTAL - IMPACT PAR TYPE DE FEATURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Baseline: features de base\n",
    "features_baseline = ['temperature_ext', 'humidite', 'vitesse_vent',\n",
    "                     'heure_sin', 'heure_cos', 'mois_sin', 'mois_cos',\n",
    "                     'est_weekend', 'clients_connectes']\n",
    "\n",
    "# Test progressif\n",
    "feature_groups = {\n",
    "    'Baseline': features_baseline,\n",
    "    '+ Lags': features_baseline + ['energie_lag1', 'energie_lag24'],\n",
    "    '+ Rolling': features_baseline + ['energie_lag1', 'energie_lag24',\n",
    "                                      'energie_rolling_6h', 'energie_rolling_24h'],\n",
    "    '+ Interactions': features_baseline + ['energie_lag1', 'energie_lag24',\n",
    "                                           'energie_rolling_6h', 'energie_rolling_24h',\n",
    "                                           'temp_heure_cos', 'temp_weekend'],\n",
    "    '+ Transformations': features_baseline + ['energie_lag1', 'energie_lag24',\n",
    "                                               'energie_rolling_6h', 'energie_rolling_24h',\n",
    "                                               'temp_heure_cos', 'temp_weekend',\n",
    "                                               'degres_jours_chauffage', 'temp_squared']\n",
    "}\n",
    "\n",
    "results_incremental = []\n",
    "\n",
    "for name, feats in feature_groups.items():\n",
    "    feats_avail = [f for f in feats if f in train_enrichi.columns]\n",
    "    \n",
    "    X_tr = train_enrichi[feats_avail].values\n",
    "    y_tr = train_enrichi['energie_kwh'].values\n",
    "    X_te = test_enrichi[feats_avail].values\n",
    "    y_te = test_enrichi['energie_kwh'].values\n",
    "    \n",
    "    model_temp = Ridge(alpha=1.0)\n",
    "    model_temp.fit(X_tr, y_tr)\n",
    "    \n",
    "    r2_test = r2_score(y_te, model_temp.predict(X_te))\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_te, model_temp.predict(X_te)))\n",
    "    \n",
    "    results_incremental.append({\n",
    "        'Configuration': name,\n",
    "        'Nb Features': len(feats_avail),\n",
    "        'R¬≤ Test': r2_test,\n",
    "        'RMSE Test': rmse_test\n",
    "    })\n",
    "\n",
    "df_incremental = pd.DataFrame(results_incremental)\n",
    "print(df_incremental.to_string(index=False))\n",
    "\n",
    "# Visualiser am√©lioration\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_incremental['Configuration'], df_incremental['R¬≤ Test'],\n",
    "         'o-', linewidth=2, markersize=10, color='steelblue')\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('R¬≤ Test')\n",
    "plt.title('Impact Incr√©mental des Features', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAm√©lioration totale: +{(df_incremental.iloc[-1]['R¬≤ Test'] - df_incremental.iloc[0]['R¬≤ Test']):.4f} R¬≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 4: R√©gression Ridge (15%)\n",
    "\n",
    "Avec plusieurs caract√©ristiques corr√©l√©es, la r√©gularisation devient utile.\n",
    "\n",
    "1. Entra√Ænez un mod√®le Ridge avec validation crois√©e pour choisir Œª\n",
    "2. Comparez les performances avec OLS\n",
    "3. Analysez comment les coefficients changent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finissez vos caract√©ristiques pour la r√©gression\n",
    "# MODIFIEZ CETTE LISTE selon vos caract√©ristiques cr√©√©es en Partie 3\n",
    "# IMPORTANT: clients_connectes est une variable tr√®s importante!\n",
    "features_reg = [\n",
    "    'temperature_ext', 'humidite', 'vitesse_vent', 'irradiance_solaire',\n",
    "    'heure_sin', 'heure_cos', 'mois_sin', 'mois_cos',\n",
    "    'jour_semaine_sin', 'jour_semaine_cos',\n",
    "    'est_weekend', 'est_ferie',\n",
    "    'clients_connectes',  # Ne pas oublier!\n",
    "    # Ajoutez vos caract√©ristiques ici\n",
    "]\n",
    "\n",
    "features_reg += features_to_use\n",
    "\n",
    "# V√©rifier que toutes les colonnes existent\n",
    "features_disponibles = [f for f in features_reg if f in train_eng.columns]\n",
    "print(f\"Caract√©ristiques utilis√©es: {len(features_disponibles)}\")\n",
    "\n",
    "X_train_reg = train_eng[features_disponibles].values\n",
    "y_train_reg = train_eng['energie_kwh'].values\n",
    "X_test_reg = test_eng[features_disponibles].values\n",
    "y_test_reg = test_eng['energie_kwh'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mod√®le OLS (baseline)\n",
    "model_ols = LinearRegression()\n",
    "model_ols.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "y_pred_ols_test = model_ols.predict(X_test_reg)\n",
    "y_pred_ols_train = model_ols.predict(X_train_reg)\n",
    "\n",
    "r2_ols_train = r2_score(y_train_reg, y_pred_ols_train)\n",
    "r2_ols_test = r2_score(y_test_reg, y_pred_ols_test)\n",
    "rmse_ols_test = np.sqrt(mean_squared_error(y_test_reg, y_pred_ols_test))\n",
    "\n",
    "print(\"OLS (baseline):\")\n",
    "print(f\"  R¬≤ train: {r2_ols_train:.4f}\")\n",
    "print(f\"  R¬≤ test:  {r2_ols_test:.4f}\")\n",
    "print(f\"  RMSE test: {rmse_ols_test:.4f}\")\n",
    "\n",
    "if r2_ols_train - r2_ols_test > 0.1:\n",
    "    print(\" Attention: surapprentissage d√©tect√© avec OLS!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mod√®le Ridge avec validation crois√©e\n",
    "# ATTENTION: Utilisez TimeSeriesSplit pour les donn√©es temporelles!\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "alphas = [0.01, 0.1, 1, 10, 100, 1000]\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "model_ridge = RidgeCV(alphas=alphas, cv=tscv)\n",
    "model_ridge.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "y_pred_ridge_train = model_ridge.predict(X_train_reg)\n",
    "y_pred_ridge_test = model_ridge.predict(X_test_reg)\n",
    "\n",
    "r2_ridge_train = r2_score(y_train_reg, y_pred_ridge_train)\n",
    "r2_ridge_test = r2_score(y_test_reg, y_pred_ridge_test)\n",
    "rmse_ridge_test = np.sqrt(mean_squared_error(y_test_reg, y_pred_ridge_test))\n",
    "\n",
    "print(f\"\\nRidge (Œª={model_ridge.alpha_}):\")\n",
    "print(f\"  R¬≤ train: {r2_ridge_train:.4f}\")\n",
    "print(f\"  R¬≤ test:  {r2_ridge_test:.4f}\")\n",
    "print(f\"  RMSE test: {rmse_ridge_test:.4f}\")\n",
    "print(f\"  √âcart R¬≤ train-test: {r2_ridge_train - r2_ridge_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des coefficients OLS vs Ridge\n",
    "coef_comparison = pd.DataFrame({\n",
    "    'Caract√©ristique': features_disponibles,\n",
    "    'OLS': model_ols.coef_,\n",
    "    'Ridge': model_ridge.coef_\n",
    "})\n",
    "coef_comparison['R√©duction (%)'] = 100 * (1 - np.abs(coef_comparison['Ridge']) / (np.abs(coef_comparison['OLS']) + 1e-8))\n",
    "coef_comparison = coef_comparison.sort_values('R√©duction (%)', ascending=False)\n",
    "\n",
    "print(\"\\nComparaison des coefficients (tri√©s par r√©duction):\")\n",
    "print(coef_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Mod√®le': ['OLS', 'Ridge (Œª=1)', f'Ridge (Œª={model_ridge.alpha_})'],\n",
    "    'R¬≤ train': [r2_ols_train, r2_ridge_train, r2_ridge_train],\n",
    "    'R¬≤ test': [r2_ols_test, r2_ridge_test, r2_ridge_test],\n",
    "    'RMSE test': [rmse_ols_test, rmse_ridge_test, rmse_ridge_test],\n",
    "    '√âcart': [abs(r2_ols_train - r2_ols_test),\n",
    "              abs(r2_ridge_train - r2_ridge_test),\n",
    "              abs(r2_ridge_train - r2_ridge_test)]\n",
    "})\n",
    "\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Meilleur mod√®le\n",
    "best_idx = results['R¬≤ test'].idxmax()\n",
    "print(f\"\\n Meilleur mod√®le: {results.loc[best_idx, 'Mod√®le']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyse des coefficients du meilleur mod√®le\n",
    "coef_comparison = pd.DataFrame({\n",
    "    'Feature': features_disponibles,\n",
    "    'OLS': model_ols.coef_,\n",
    "    'Ridge': model_ridge.coef_\n",
    "})\n",
    "\n",
    "# Calculer r√©duction (shrinkage)\n",
    "coef_comparison['R√©duction (%)'] = 100 * (\n",
    "    1 - np.abs(coef_comparison['Ridge']) / (np.abs(coef_comparison['OLS']) + 1e-8)\n",
    ")\n",
    "\n",
    "# Trier par r√©duction\n",
    "coef_comparison = coef_comparison.sort_values('R√©duction (%)', ascending=False)\n",
    "\n",
    "print(coef_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n Observations:\")\n",
    "print(f\"  - R√©duction moyenne: {coef_comparison['R√©duction (%)'].mean():.1f}%\")\n",
    "print(f\"  - R√©duction max: {coef_comparison['R√©duction (%)'].max():.1f}%\")\n",
    "print(f\"  - Feature la plus r√©duite: {coef_comparison.iloc[0]['Feature']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester plusieurs Œª\n",
    "lambdas_test = np.logspace(-2, 4, 50)  # 0.01 √† 10000\n",
    "coefficients_path = []\n",
    "\n",
    "for lam in lambdas_test:\n",
    "    model_temp = Ridge(alpha=lam)\n",
    "    model_temp.fit(X_train, y_train)\n",
    "    coefficients_path.append(model_temp.coef_)\n",
    "\n",
    "coefficients_path = np.array(coefficients_path)\n",
    "\n",
    "# Tracer\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, feature in enumerate(features_disponibles[:10]):  # 10 premi√®res features\n",
    "    plt.plot(lambdas_test, coefficients_path[:, i], label=feature, linewidth=2)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Œª (√©chelle log)', fontsize=12)\n",
    "plt.ylabel('Coefficient', fontsize=12)\n",
    "plt.title('Chemin de R√©gularisation Ridge', fontsize=14, fontweight='bold')\n",
    "plt.axvline(model_ridge.alpha_, color='red', linestyle='--',\n",
    "            linewidth=2, label=f'Œª optimal = {model_ridge.alpha_}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train_list = []\n",
    "r2_test_list = []\n",
    "\n",
    "for lam in lambdas_test:\n",
    "    model_temp = Ridge(alpha=lam)\n",
    "    model_temp.fit(X_train, y_train)\n",
    "\n",
    "    r2_train_list.append(r2_score(y_train, model_temp.predict(X_train)))\n",
    "    r2_test_list.append(r2_score(y_test, model_temp.predict(X_test)))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lambdas_test, r2_train_list, label='R¬≤ train', linewidth=2, color='blue')\n",
    "plt.plot(lambdas_test, r2_test_list, label='R¬≤ test', linewidth=2, color='orange')\n",
    "plt.axvline(model_ridge.alpha_, color='red', linestyle='--',\n",
    "            linewidth=2, label=f'Œª optimal = {model_ridge.alpha_}')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Œª (√©chelle log)', fontsize=12)\n",
    "plt.ylabel('R¬≤', fontsize=12)\n",
    "plt.title('Courbe de Validation Ridge', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions pour l'entrevue orale**:\n",
    "- Pourquoi Ridge aide-t-il quand les caract√©ristiques sont corr√©l√©es?\n",
    "- Quelle caract√©ristique a √©t√© la plus r√©duite? Pourquoi?\n",
    "- Comment interpr√©ter Ridge comme estimation MAP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 5: Sous-t√¢che de classification (15%)\n",
    "\n",
    "Entra√Ænez un classifieur pour pr√©dire les √©v√©nements de pointe, puis utilisez la probabilit√© pr√©dite comme caract√©ristique pour la r√©gression.\n",
    "\n",
    "**√âtapes**:\n",
    "1. Entra√Æner LogisticRegression sur `evenement_pointe`\n",
    "2. Extraire `P(pointe)` pour chaque observation\n",
    "3. Ajouter cette probabilit√© comme caract√©ristique pour Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caract√©ristiques pour la classification\n",
    "# Utilisez des caract√©ristiques qui ne \"trichent\" pas (pas de consommation pass√©e pour pr√©dire la pointe)\n",
    "features_pointe = ['temperature_ext', 'humidite', 'vitesse_vent', 'heure_sin', 'heure_cos', 'est_weekend', 'clients_connectes']\n",
    "\n",
    "X_train_pointe = train_eng[features_pointe].values\n",
    "y_train_pointe = train_eng['evenement_pointe'].values\n",
    "X_test_pointe = test_eng[features_pointe].values\n",
    "y_test_pointe = test_eng['evenement_pointe'].values\n",
    "\n",
    "# Entra√Æner le classifieur\n",
    "clf_pointe = LogisticRegression(max_iter=1000)\n",
    "clf_pointe.fit(X_train_pointe, y_train_pointe)\n",
    "\n",
    "# √âvaluation\n",
    "print(\"Classification des √©v√©nements de pointe:\")\n",
    "print(f\"  Accuracy (train): {clf_pointe.score(X_train_pointe, y_train_pointe):.4f}\")\n",
    "print(f\"  Accuracy (test): {clf_pointe.score(X_test_pointe, y_test_pointe):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les probabilit√©s\n",
    "train_eng['P_pointe'] = clf_pointe.predict_proba(X_train_pointe)[:, 1]\n",
    "test_eng['P_pointe'] = clf_pointe.predict_proba(X_test_pointe)[:, 1]\n",
    "\n",
    "print(f\"Distribution de P(pointe):\")\n",
    "print(f\"  Train: moyenne={train_eng['P_pointe'].mean():.3f}, std={train_eng['P_pointe'].std():.3f}\")\n",
    "print(f\"  Test:  moyenne={test_eng['P_pointe'].mean():.3f}, std={test_eng['P_pointe'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question pour l'entrevue**: Pourquoi utiliser P(pointe) au lieu d'un indicateur 0/1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ANALYSE CALIBRATION ET DISTRIBUTION P(pointe)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Calibration plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Diviser en bins de probabilit√©\n",
    "n_bins = 10\n",
    "bins = np.linspace(0, 1, n_bins + 1)\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "# Calculer fraction r√©elle par bin\n",
    "bin_indices = np.digitize(proba_test, bins) - 1\n",
    "bin_indices = np.clip(bin_indices, 0, n_bins - 1)\n",
    "\n",
    "fraction_positive = np.zeros(n_bins)\n",
    "count_per_bin = np.zeros(n_bins)\n",
    "\n",
    "for i in range(n_bins):\n",
    "    mask = bin_indices == i\n",
    "    if mask.sum() > 0:\n",
    "        fraction_positive[i] = y_test_clf[mask].mean()\n",
    "        count_per_bin[i] = mask.sum()\n",
    "\n",
    "# Calibration curve\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Parfaitement calibr√©')\n",
    "axes[0].plot(bin_centers, fraction_positive, 'o-', linewidth=2,\n",
    "            label='Notre mod√®le', markersize=8)\n",
    "axes[0].set_xlabel('Probabilit√© pr√©dite')\n",
    "axes[0].set_ylabel('Fraction r√©elle de pointes')\n",
    "axes[0].set_title('Courbe de Calibration')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution P(pointe) par classe r√©elle\n",
    "axes[1].hist(proba_test[y_test_clf==0], bins=30, alpha=0.5,\n",
    "            label='Normal (classe 0)', color='blue', density=True)\n",
    "axes[1].hist(proba_test[y_test_clf==1], bins=30, alpha=0.5,\n",
    "            label='Pointe (classe 1)', color='red', density=True)\n",
    "axes[1].axvline(0.5, color='black', linestyle='--', label='Seuil 0.5')\n",
    "axes[1].set_xlabel('P(pointe)')\n",
    "axes[1].set_ylabel('Densit√©')\n",
    "axes[1].set_title('Distribution P(pointe) par Classe R√©elle')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Histogramme 2D\n",
    "axes[2].hist2d(test_eng['temperature_ext'], test_eng['P_pointe'],\n",
    "              bins=30, cmap='YlOrRd')\n",
    "axes[2].set_xlabel('Temp√©rature (¬∞C)')\n",
    "axes[2].set_ylabel('P(pointe)')\n",
    "axes[2].set_title('P(pointe) vs Temp√©rature')\n",
    "plt.colorbar(axes[2].collections[0], ax=axes[2], label='Fr√©quence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Analyse segment√©e\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"P(pointe) PAR SEGMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "segments = {\n",
    "    'Train': train_eng['P_pointe'],\n",
    "    'Test': test_eng['P_pointe'],\n",
    "    'Pointe r√©elle': test_eng[test_eng['evenement_pointe']==1]['P_pointe'],\n",
    "    'Normal r√©el': test_eng[test_eng['evenement_pointe']==0]['P_pointe']\n",
    "}\n",
    "\n",
    "for name, data in segments.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Min:  {data.min():.3f}\")\n",
    "    print(f\"  Q25:  {data.quantile(0.25):.3f}\")\n",
    "    print(f\"  M√©diane: {data.median():.3f}\")\n",
    "    print(f\"  Q75:  {data.quantile(0.75):.3f}\")\n",
    "    print(f\"  Max:  {data.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 6: Mod√®le combin√© (10%)\n",
    "\n",
    "Assemblez le mod√®le final en ajoutant `P_pointe` comme caract√©ristique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caract√©ristiques finales (avec P_pointe)\n",
    "features_final = features_disponibles + ['P_pointe']\n",
    "\n",
    "X_train_final = train_eng[features_final].values\n",
    "y_train_final = train_eng['energie_kwh'].values\n",
    "X_test_final = test_eng[features_final].values\n",
    "y_test_final = test_eng['energie_kwh'].values\n",
    "\n",
    "# Mod√®le Ridge final\n",
    "model_final = RidgeCV(alphas=[0.1, 1, 10, 100], cv=TimeSeriesSplit(n_splits=5))\n",
    "model_final.fit(X_train_final, y_train_final)\n",
    "y_pred_final = model_final.predict(X_test_final)\n",
    "\n",
    "print(\"Mod√®le final (Ridge + P_pointe):\")\n",
    "print(f\"  Œª s√©lectionn√©: {model_final.alpha_}\")\n",
    "print(f\"  R¬≤ train: {model_final.score(X_train_final, y_train_final):.4f}\")\n",
    "print(f\"  R¬≤ test:  {r2_score(y_test_final, y_pred_final):.4f}\")\n",
    "print(f\"  RMSE test: {np.sqrt(mean_squared_error(y_test_final, y_pred_final)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des r√©sidus\n",
    "residus = y_test_final - y_pred_final\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Histogramme des r√©sidus\n",
    "axes[0].hist(residus, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(0, color='red', linestyle='--', label='Z√©ro')\n",
    "axes[0].set_xlabel('R√©sidu')\n",
    "axes[0].set_ylabel('Fr√©quence')\n",
    "axes[0].set_title('Distribution des r√©sidus')\n",
    "axes[0].legend()\n",
    "\n",
    "# Pr√©dictions vs r√©el\n",
    "axes[1].scatter(y_test_final, y_pred_final, alpha=0.3, s=5)\n",
    "axes[1].plot([y_test_final.min(), y_test_final.max()], \n",
    "             [y_test_final.min(), y_test_final.max()], 'r--', label='Parfait')\n",
    "axes[1].set_xlabel('√ânergie r√©elle (kWh)')\n",
    "axes[1].set_ylabel('√ânergie pr√©dite (kWh)')\n",
    "axes[1].set_title('Pr√©dictions vs R√©el')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ANALYSE APPROFONDIE DES ERREURS\")\n",
    "print(\"=\"*60)\n",
    "print(\"asdassadsad\")\n",
    "\n",
    "# 1. Erreurs par segment\n",
    "residus_final = y_test_final - y_pred_final\n",
    "test_analysis = test_eng.copy()\n",
    "test_analysis['residus'] = residus_final\n",
    "test_analysis['erreur_abs'] = np.abs(residus_final)\n",
    "test_analysis['erreur_pct'] = 100 * np.abs(residus_final) / (y_test_final + 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Erreurs par heure\n",
    "erreurs_heure = test_analysis.groupby('heure')['erreur_abs'].mean()\n",
    "axes[0, 0].bar(erreurs_heure.index, erreurs_heure.values, color='coral')\n",
    "axes[0, 0].set_xlabel('Heure')\n",
    "axes[0, 0].set_ylabel('Erreur absolue moyenne (kWh)')\n",
    "axes[0, 0].set_title('Erreurs par Heure de la Journ√©e')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Erreurs par temp√©rature\n",
    "temp_bins = pd.cut(test_analysis['temperature_ext'], bins=8)\n",
    "erreurs_temp = test_analysis.groupby(temp_bins)['erreur_abs'].mean()\n",
    "axes[0, 1].bar(range(len(erreurs_temp)), erreurs_temp.values, color='steelblue')\n",
    "axes[0, 1].set_xlabel('Plage de temp√©rature')\n",
    "axes[0, 1].set_ylabel('Erreur absolue moyenne (kWh)')\n",
    "axes[0, 1].set_title('Erreurs par Temp√©rature')\n",
    "axes[0, 1].set_xticklabels([f\"{int(i.left)}-{int(i.right)}\" \n",
    "                             for i in erreurs_temp.index], rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Erreurs par type (pointe vs normal)\n",
    "erreurs_type = test_analysis.groupby('evenement_pointe')['erreur_abs'].mean()\n",
    "axes[0, 2].bar(['Normal', 'Pointe'], erreurs_type.values, color=['green', 'red'])\n",
    "axes[0, 2].set_ylabel('Erreur absolue moyenne (kWh)')\n",
    "axes[0, 2].set_title('Erreurs: Normal vs Pointe')\n",
    "axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "print(f\"\\nErreurs moyennes:\")\n",
    "print(f\"  Normal: {erreurs_type[0]:.2f} kWh\")\n",
    "print(f\"  Pointe: {erreurs_type[1]:.2f} kWh\")\n",
    "print(f\"  Ratio pointe/normal: {erreurs_type[1]/erreurs_type[0]:.2f}x\")\n",
    "\n",
    "# R√©sidus vs features importantes\n",
    "axes[1, 0].scatter(test_analysis['temperature_ext'], residus_final, \n",
    "                   alpha=0.4, s=10, c=test_analysis['heure'], cmap='viridis')\n",
    "axes[1, 0].axhline(0, color='red', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Temp√©rature (¬∞C)')\n",
    "axes[1, 0].set_ylabel('R√©sidu (kWh)')\n",
    "axes[1, 0].set_title('R√©sidus vs Temp√©rature (color√© par heure)')\n",
    "plt.colorbar(axes[1, 0].collections[0], ax=axes[1, 0], label='Heure')\n",
    "\n",
    "axes[1, 1].scatter(test_analysis['clients_connectes'], residus_final,\n",
    "                   alpha=0.4, s=10, color='purple')\n",
    "axes[1, 1].axhline(0, color='red', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Clients connect√©s')\n",
    "axes[1, 1].set_ylabel('R√©sidu (kWh)')\n",
    "axes[1, 1].set_title('R√©sidus vs Nombre de Clients')\n",
    "\n",
    "# Top 10 pires pr√©dictions\n",
    "axes[1, 2].axis('off')\n",
    "pires = test_analysis.nlargest(10, 'erreur_abs')[\n",
    "    ['heure', 'temperature_ext', 'evenement_pointe', 'erreur_abs']\n",
    "]\n",
    "table_text = \"TOP 10 PIRES PR√âDICTIONS\\n\\n\"\n",
    "table_text += pires.to_string(index=False, float_format='%.1f')\n",
    "axes[1, 2].text(0.1, 0.5, table_text, fontsize=9, \n",
    "                family='monospace', verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. M√©triques par quantile de consommation\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PERFORMANCE PAR NIVEAU DE CONSOMMATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_analysis['consommation_level'] = pd.qcut(y_test_final, q=4,\n",
    "                                               labels=['Faible', 'Moyen', '√âlev√©', 'Tr√®s √©lev√©'])\n",
    "\n",
    "perf_by_level = test_analysis.groupby('consommation_level').agg({\n",
    "    'erreur_abs': 'mean',\n",
    "    'erreur_pct': 'mean',\n",
    "    'residus': lambda x: r2_score(y_test_final[x.index], \n",
    "                                   y_pred_final[x.index])\n",
    "}).round(2)\n",
    "perf_by_level.columns = ['MAE (kWh)', 'MAPE (%)', 'R¬≤']\n",
    "\n",
    "print(perf_by_level)\n",
    "\n",
    "print(\"\\nConstat: Le mod√®le est-il meilleur sur certaines plages?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Partie 7: Extension (10%) - Choisir UNE option\n",
    "\n",
    "### Option A: Donn√©es m√©t√©orologiques externes\n",
    "Utilisez la biblioth√®que `meteostat` pour ajouter des donn√©es m√©t√©o suppl√©mentaires (ex: pression atmosph√©rique, point de ros√©e).\n",
    "\n",
    "### Option B: Classification multiclasse\n",
    "Au lieu de binaire (pointe/normal), cr√©ez 3+ classes de consommation (faible/moyenne/√©lev√©e) et utilisez softmax.\n",
    "\n",
    "### Option C: Analyse d'erreur approfondie\n",
    "Identifiez quand le mod√®le fait le plus d'erreurs et proposez des am√©liorations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOTRE EXTENSION ICI\n",
    "# Indiquez quelle option vous avez choisie et pourquoi.\n",
    "\n",
    "# Option choisie: ___\n",
    "# Justification: ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Kaggle Score Simulation & Diagnostics\n",
    "\n",
    "The two cells below serve two purposes:\n",
    "1. **Kaggle Simulation**: Replicate exactly what happens when Kaggle evaluates your submission (RMSE on `energy_test.csv` targets). This uses the clean model only (no energy lags).\n",
    "2. **Diagnostic Output**: Print a structured text block with all the information needed to diagnose performance issues. Copy-paste the output for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_pointe added.\n",
      "Features per model: 44\n",
      "Postes: ['A', 'B', 'C']\n",
      "\n",
      "Per-poste model training:\n",
      "  Poste A: alpha=500.0, n_tr=1751, n_te=474, RMSE_tr=25.55, RMSE_te=16.74, R2_tr=0.8339, R2_te=0.2834\n",
      "  Poste B: alpha=10.0, n_tr=366, n_te=1126, RMSE_tr=17.85, RMSE_te=44.83, R2_tr=0.7666, R2_te=-0.5415\n",
      "  Poste C: alpha=10.0, n_tr=6129, n_te=154, RMSE_tr=171.32, RMSE_te=186.61, R2_tr=0.4401, R2_te=-3.6472\n",
      "\n",
      "======================================================================\n",
      "KAGGLE SCORE SIMULATION (v3: Per-Poste Ridge Models)\n",
      "======================================================================\n",
      "  Models:      Ridge per poste (A, B, C)\n",
      "  Features:    44 per model (no poste dummies needed)\n",
      "  Rows:        1754 (expected 1754)\n",
      "\n",
      "  RMSE:  66.3865 kWh   <-- Kaggle score\n",
      "  MAE:   42.6369 kWh\n",
      "  R2:    0.1196\n",
      "\n",
      "  Pred stats:  min=0.00, mean=116.87, max=702.43\n",
      "  Truth stats: min=17.76, mean=83.74, max=538.48\n",
      "\n",
      "  Expected rows by Kaggle: 1754\n",
      "  Your submission rows:    1754\n",
      "\n",
      "  Per-poste RMSE:\n",
      "    Poste A: RMSE=16.74, R2=0.2834, bias=+2.84, n=474\n",
      "    Poste B: RMSE=44.55, R2=-0.5218, bias=-29.51, n=1126\n",
      "    Poste C: RMSE=186.61, R2=-3.6472, bias=-170.35, n=154\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CELL A v3: KAGGLE SCORE SIMULATION ‚Äî Per-Poste Ridge Models\n",
    "# ================================================================\n",
    "# Root cause of bad RMSE: Postes A/B/C have very different consumption\n",
    "#   Poste A: ~50 kWh (test)   ‚Äî small\n",
    "#   Poste B: ~72 kWh (test)   ‚Äî medium (64% of test!)\n",
    "#   Poste C: ~269 kWh (test)  ‚Äî large  (74% of train!)\n",
    "# A single model learns an intercept ~216 (train mean) and can't\n",
    "# shift enough per poste. Solution: one Ridge model per poste.\n",
    "# Also: weather lags are now computed per-poste (shift across postes\n",
    "# was giving wrong values).\n",
    "# Requires: train, test, clf_pointe (from earlier cells)\n",
    "# ================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. FEATURE ENGINEERING (applied per-poste, so lags are correct)\n",
    "# -------------------------------------------------------------------\n",
    "def creer_caracteristiques_v3(df):\n",
    "    \"\"\"\n",
    "    Feature engineering for a SINGLE poste's data.\n",
    "    Data is sorted by time so shift/rolling operate correctly.\n",
    "    Original DataFrame index is preserved for alignment.\n",
    "    \"\"\"\n",
    "    df = df.sort_values('horodatage_local').copy()\n",
    "\n",
    "    # Weather x time interactions\n",
    "    df['temp_heure_cos'] = df['temperature_ext'] * df['heure_cos']\n",
    "    df['temp_heure_sin'] = df['temperature_ext'] * df['heure_sin']\n",
    "    df['temp_weekend'] = df['temperature_ext'] * df['est_weekend']\n",
    "    df['temp_mois_sin'] = df['temperature_ext'] * df['mois_sin']\n",
    "    df['temp_mois_cos'] = df['temperature_ext'] * df['mois_cos']\n",
    "\n",
    "    # Degree-days\n",
    "    df['degres_jours_chauffage'] = np.maximum(18 - df['temperature_ext'], 0)\n",
    "    df['degres_jours_clim'] = np.maximum(df['temperature_ext'] - 22, 0)\n",
    "\n",
    "    # Weather transforms\n",
    "    df['temp_squared'] = df['temperature_ext'] ** 2\n",
    "    df['temp_ressentie'] = df['temperature_ext'] - 0.5 * df['vitesse_vent']\n",
    "    df['humidite_temp'] = df['humidite'] * np.abs(df['temperature_ext']) / 100\n",
    "\n",
    "    # Time indicators\n",
    "    df['est_pointe_matin'] = ((df['heure'] >= 7) & (df['heure'] <= 9)).astype(int)\n",
    "    df['est_pointe_soir'] = ((df['heure'] >= 17) & (df['heure'] <= 20)).astype(int)\n",
    "    df['est_nuit'] = ((df['heure'] >= 0) & (df['heure'] <= 6)).astype(int)\n",
    "\n",
    "    # Weather lags (correct: single-poste, sorted by time)\n",
    "    df['temp_lag1'] = df['temperature_ext'].shift(1).fillna(df['temperature_ext'].iloc[0])\n",
    "    df['temp_lag24'] = df['temperature_ext'].shift(24).fillna(df['temperature_ext'].iloc[0])\n",
    "    df['temp_diff'] = df['temperature_ext'].diff().fillna(0)\n",
    "    df['temp_amplitude_24h'] = (\n",
    "        df['temperature_ext'].rolling(window=24, min_periods=1).max() -\n",
    "        df['temperature_ext'].rolling(window=24, min_periods=1).min()\n",
    "    )\n",
    "\n",
    "    # Infrastructure interactions\n",
    "    if 'clients_connectes' in df.columns:\n",
    "        df['clients_temp'] = df['clients_connectes'] * df['temperature_ext']\n",
    "        df['clients_weekend'] = df['clients_connectes'] * df['est_weekend']\n",
    "        df['clients_heure_cos'] = df['clients_connectes'] * df['heure_cos']\n",
    "\n",
    "    if 'tstats_intelligents_connectes' in df.columns:\n",
    "        df['tstats_temp'] = df['tstats_intelligents_connectes'] * df['temperature_ext']\n",
    "        df['ratio_tstats_clients'] = (\n",
    "            df['tstats_intelligents_connectes'] / (df['clients_connectes'] + 1))\n",
    "\n",
    "    if 'irradiance_solaire' in df.columns:\n",
    "        df['irradiance_temp'] = df['irradiance_solaire'] * df['temperature_ext']\n",
    "\n",
    "    return df\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. BUILD PER-POSTE DATA\n",
    "# -------------------------------------------------------------------\n",
    "postes = sorted(train['poste'].unique())\n",
    "train_parts = {}\n",
    "test_parts = {}\n",
    "\n",
    "for p in postes:\n",
    "    train_parts[p] = creer_caracteristiques_v3(train[train['poste'] == p])\n",
    "    test_parts[p] = creer_caracteristiques_v3(test[test['poste'] == p])\n",
    "\n",
    "# Feature list (same for all postes ‚Äî same function applied)\n",
    "features_clean = [f for f in train_parts[postes[0]].select_dtypes(include=[np.number]).columns\n",
    "                  if f not in ['energie_kwh']]\n",
    "\n",
    "# Add P_pointe if classifier available\n",
    "try:\n",
    "    _feats_pointe = ['temperature_ext', 'humidite', 'vitesse_vent',\n",
    "                     'heure_sin', 'heure_cos', 'est_weekend', 'clients_connectes']\n",
    "    for p in postes:\n",
    "        train_parts[p]['P_pointe'] = clf_pointe.predict_proba(\n",
    "            train_parts[p][_feats_pointe].values)[:, 1]\n",
    "        test_parts[p]['P_pointe'] = clf_pointe.predict_proba(\n",
    "            test_parts[p][_feats_pointe].values)[:, 1]\n",
    "    if 'P_pointe' not in features_clean:\n",
    "        features_clean.append('P_pointe')\n",
    "    print(\"P_pointe added.\")\n",
    "except NameError:\n",
    "    print(\"clf_pointe not found, skipping P_pointe.\")\n",
    "\n",
    "print(f\"Features per model: {len(features_clean)}\")\n",
    "print(f\"Postes: {postes}\")\n",
    "\n",
    "# Reassemble full DataFrames (for Cell B compatibility)\n",
    "train_clean = pd.concat([train_parts[p] for p in postes])\n",
    "test_clean = pd.concat([test_parts[p] for p in postes])\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. TRAIN PER-POSTE RIDGE MODELS\n",
    "# -------------------------------------------------------------------\n",
    "alphas_grid = [0.01, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 5000]\n",
    "models = {}\n",
    "scalers_per_poste = {}\n",
    "\n",
    "print(\"\\nPer-poste model training:\")\n",
    "for p in postes:\n",
    "    X_tr = train_parts[p][features_clean].values\n",
    "    y_tr = train_parts[p]['energie_kwh'].values\n",
    "\n",
    "    scaler_p = StandardScaler()\n",
    "    X_tr_s = scaler_p.fit_transform(X_tr)\n",
    "\n",
    "    n_splits = min(5, max(2, len(X_tr) // 50))\n",
    "    tscv_p = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    ridge_p = RidgeCV(alphas=alphas_grid, cv=tscv_p, scoring='neg_mean_squared_error')\n",
    "    ridge_p.fit(X_tr_s, y_tr)\n",
    "\n",
    "    # Evaluate\n",
    "    X_te = test_parts[p][features_clean].values\n",
    "    y_te = test_parts[p]['energie_kwh'].values\n",
    "    X_te_s = scaler_p.transform(X_te)\n",
    "\n",
    "    y_pred_tr = ridge_p.predict(X_tr_s)\n",
    "    y_pred_te = ridge_p.predict(X_te_s)\n",
    "\n",
    "    rmse_tr = np.sqrt(mean_squared_error(y_tr, y_pred_tr))\n",
    "    rmse_te = np.sqrt(mean_squared_error(y_te, y_pred_te))\n",
    "    r2_tr = r2_score(y_tr, y_pred_tr)\n",
    "    r2_te = r2_score(y_te, y_pred_te) if len(y_te) > 1 else float('nan')\n",
    "\n",
    "    models[p] = ridge_p\n",
    "    scalers_per_poste[p] = scaler_p\n",
    "\n",
    "    print(f\"  Poste {p}: alpha={ridge_p.alpha_}, n_tr={len(y_tr)}, n_te={len(y_te)}, \"\n",
    "          f\"RMSE_tr={rmse_tr:.2f}, RMSE_te={rmse_te:.2f}, \"\n",
    "          f\"R2_tr={r2_tr:.4f}, R2_te={r2_te:.4f}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. PREDICT ON TEST (preserving original row order)\n",
    "# -------------------------------------------------------------------\n",
    "y_pred_test_c = pd.Series(index=test.index, dtype=float)\n",
    "\n",
    "for p in postes:\n",
    "    te_p = test_parts[p]\n",
    "    X_te = te_p[features_clean].values\n",
    "    X_te_s = scalers_per_poste[p].transform(X_te)\n",
    "    preds = np.maximum(models[p].predict(X_te_s), 0)\n",
    "    y_pred_test_c.loc[te_p.index] = preds\n",
    "\n",
    "y_pred_test_c = y_pred_test_c.values\n",
    "y_test_clean = test['energie_kwh'].values\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. KAGGLE SCORE = TEST SCORE\n",
    "# -------------------------------------------------------------------\n",
    "# test IS energy_test_avec_cible.csv, so test RMSE = Kaggle RMSE\n",
    "rmse_sim = np.sqrt(mean_squared_error(y_test_clean, y_pred_test_c))\n",
    "r2_sim = r2_score(y_test_clean, y_pred_test_c)\n",
    "mae_sim = mean_absolute_error(y_test_clean, y_pred_test_c)\n",
    "\n",
    "# Build merged DataFrame (for Cell B)\n",
    "merged = pd.DataFrame({\n",
    "    'horodatage_local': test['horodatage_local'].values,\n",
    "    'poste': test['poste'].values,\n",
    "    'y_true': y_test_clean,\n",
    "    'y_pred': y_pred_test_c\n",
    "})\n",
    "\n",
    "# y_sim_pred for submission (same order as test)\n",
    "y_sim_pred = y_pred_test_c.copy()\n",
    "\n",
    "try:\n",
    "    sample_sub = pd.read_csv('ift-3395-6390-prediction-energetique/sample_submission.csv')\n",
    "    expected_rows = len(sample_sub)\n",
    "except FileNotFoundError:\n",
    "    expected_rows = len(test)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. REPORT\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KAGGLE SCORE SIMULATION (v3: Per-Poste Ridge Models)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Models:      Ridge per poste ({', '.join(postes)})\")\n",
    "print(f\"  Features:    {len(features_clean)} per model (no poste dummies needed)\")\n",
    "print(f\"  Rows:        {len(merged)} (expected {expected_rows})\")\n",
    "print()\n",
    "print(f\"  RMSE:  {rmse_sim:.4f} kWh   <-- Kaggle score\")\n",
    "print(f\"  MAE:   {mae_sim:.4f} kWh\")\n",
    "print(f\"  R2:    {r2_sim:.4f}\")\n",
    "print()\n",
    "print(f\"  Pred stats:  min={merged['y_pred'].min():.2f}, \"\n",
    "      f\"mean={merged['y_pred'].mean():.2f}, max={merged['y_pred'].max():.2f}\")\n",
    "print(f\"  Truth stats: min={merged['y_true'].min():.2f}, \"\n",
    "      f\"mean={merged['y_true'].mean():.2f}, max={merged['y_true'].max():.2f}\")\n",
    "print()\n",
    "print(f\"  Expected rows by Kaggle: {expected_rows}\")\n",
    "print(f\"  Your submission rows:    {len(merged)}\")\n",
    "\n",
    "print(\"\\n  Per-poste RMSE:\")\n",
    "for p in postes:\n",
    "    mask = merged['poste'] == p\n",
    "    sub = merged[mask]\n",
    "    rmse_p = np.sqrt(mean_squared_error(sub['y_true'], sub['y_pred']))\n",
    "    r2_p = r2_score(sub['y_true'], sub['y_pred']) if len(sub) > 1 else float('nan')\n",
    "    bias_p = (sub['y_true'] - sub['y_pred']).mean()\n",
    "    print(f\"    Poste {p}: RMSE={rmse_p:.2f}, R2={r2_p:.4f}, bias={bias_p:+.2f}, n={mask.sum()}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# CELL B v3: DIAGNOSTIC OUTPUT (copy-paste the output to me)\n",
    "# ================================================================\n",
    "# Run AFTER Cell A. Uses: train, test, postes, train_parts, test_parts,\n",
    "# features_clean, models, scalers_per_poste, merged, rmse_sim, mae_sim\n",
    "# ================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DIAGNOSTIC DUMP v3 -- COPY EVERYTHING BELOW THIS LINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ---- SECTION 1: DATASET OVERVIEW ----\n",
    "print(\"\\n[1] DATASET OVERVIEW\")\n",
    "print(f\"  train shape: {train.shape}\")\n",
    "print(f\"  test shape:  {test.shape}\")\n",
    "print(f\"  train period: {train['horodatage_local'].min()} -> {train['horodatage_local'].max()}\")\n",
    "print(f\"  test period:  {test['horodatage_local'].min()} -> {test['horodatage_local'].max()}\")\n",
    "\n",
    "print(f\"\\n  train energie_kwh: mean={train['energie_kwh'].mean():.2f}, \"\n",
    "      f\"std={train['energie_kwh'].std():.2f}, \"\n",
    "      f\"min={train['energie_kwh'].min():.2f}, max={train['energie_kwh'].max():.2f}\")\n",
    "print(f\"  test energie_kwh:  mean={test['energie_kwh'].mean():.2f}, \"\n",
    "      f\"std={test['energie_kwh'].std():.2f}, \"\n",
    "      f\"min={test['energie_kwh'].min():.2f}, max={test['energie_kwh'].max():.2f}\")\n",
    "\n",
    "for col in ['temperature_ext', 'humidite', 'vitesse_vent', 'irradiance_solaire', 'clients_connectes']:\n",
    "    if col in train.columns and col in test.columns:\n",
    "        print(f\"  {col}: train_mean={train[col].mean():.2f}, test_mean={test[col].mean():.2f}, \"\n",
    "              f\"shift={100*(test[col].mean()-train[col].mean())/(train[col].mean()+1e-8):+.1f}%\")\n",
    "\n",
    "# ---- SECTION 1B: PER-POSTE OVERVIEW ----\n",
    "print(\"\\n[1B] PER-POSTE OVERVIEW\")\n",
    "for p in postes:\n",
    "    tr_mask = train['poste'] == p\n",
    "    te_mask = test['poste'] == p\n",
    "    print(f\"  Poste {p}: train n={tr_mask.sum()}, mean_kwh={train.loc[tr_mask, 'energie_kwh'].mean():.2f} | \"\n",
    "          f\"test n={te_mask.sum()}, mean_kwh={test.loc[te_mask, 'energie_kwh'].mean():.2f}\")\n",
    "\n",
    "# ---- SECTION 2: PER-POSTE MODEL CONFIGS ----\n",
    "print(\"\\n[2] PER-POSTE MODEL CONFIGURATIONS\")\n",
    "print(f\"  Number of features: {len(features_clean)}\")\n",
    "for p in postes:\n",
    "    print(f\"  Poste {p}: alpha={models[p].alpha_}, intercept={models[p].intercept_:.4f}\")\n",
    "\n",
    "# ---- SECTION 3: TOP COEFFICIENTS PER POSTE ----\n",
    "print(\"\\n[3] TOP 15 COEFFICIENTS PER POSTE\")\n",
    "for p in postes:\n",
    "    print(f\"\\n  --- Poste {p} ---\")\n",
    "    coef_pairs = sorted(zip(features_clean, models[p].coef_),\n",
    "                        key=lambda x: abs(x[1]), reverse=True)\n",
    "    for i, (feat, coef) in enumerate(coef_pairs[:15], 1):\n",
    "        print(f\"    {i:2d}. {feat:40s} {coef:+10.4f}\")\n",
    "\n",
    "# ---- SECTION 4: KAGGLE RESULTS ----\n",
    "print(\"\\n[4] KAGGLE SIMULATION RESULTS\")\n",
    "print(f\"  RMSE:  {rmse_sim:.4f}\")\n",
    "print(f\"  MAE:   {mae_sim:.4f}\")\n",
    "print(f\"  R2:    {r2_sim:.4f}\")\n",
    "print(f\"  Rows:  {len(merged)}\")\n",
    "\n",
    "# ---- SECTION 5: RESIDUAL ANALYSIS ----\n",
    "print(\"\\n[5] RESIDUAL ANALYSIS\")\n",
    "residuals = merged['y_true'].values - merged['y_pred'].values\n",
    "abs_residuals = np.abs(residuals)\n",
    "merged_diag = merged.copy()\n",
    "merged_diag['residual'] = residuals\n",
    "merged_diag['abs_err'] = abs_residuals\n",
    "\n",
    "print(f\"  Residual mean:   {residuals.mean():.4f} (bias)\")\n",
    "print(f\"  Residual std:    {residuals.std():.4f}\")\n",
    "for p_val in [50, 75, 90, 95, 99]:\n",
    "    print(f\"  |residual| P{p_val}: {np.percentile(abs_residuals, p_val):.2f}\")\n",
    "\n",
    "# ---- SECTION 5B: PER-POSTE RESIDUAL ----\n",
    "print(\"\\n[5B] PER-POSTE RESIDUAL ANALYSIS\")\n",
    "for p in postes:\n",
    "    mask = merged_diag['poste'] == p\n",
    "    sub = merged_diag[mask]\n",
    "    rmse_p = np.sqrt((sub['residual']**2).mean())\n",
    "    r2_p = r2_score(sub['y_true'], sub['y_pred']) if len(sub) > 1 else float('nan')\n",
    "    print(f\"  Poste {p}: RMSE={rmse_p:.2f}, bias={sub['residual'].mean():+.2f}, \"\n",
    "          f\"MAE={sub['abs_err'].mean():.2f}, R2={r2_p:.4f}, n={mask.sum()}\")\n",
    "\n",
    "# ---- SECTIONS 6-11: ERRORS BY CATEGORY ----\n",
    "try:\n",
    "    _info_cols = ['horodatage_local', 'poste', 'heure', 'mois', 'temperature_ext',\n",
    "                  'clients_connectes', 'evenement_pointe', 'est_weekend']\n",
    "    _info_avail = [c for c in _info_cols if c in test.columns]\n",
    "    analysis = merged_diag.merge(test[_info_avail], on=['horodatage_local', 'poste'], how='left')\n",
    "\n",
    "    print(\"\\n[6] MAE BY HOUR\")\n",
    "    for h in range(24):\n",
    "        mask = analysis['heure'] == h\n",
    "        if mask.sum() > 0:\n",
    "            print(f\"  Hour {h:2d}: MAE={analysis.loc[mask, 'abs_err'].mean():6.2f}, \"\n",
    "                  f\"mean_conso={analysis.loc[mask, 'y_true'].mean():6.2f}, n={mask.sum()}\")\n",
    "\n",
    "    print(\"\\n[7] MAE BY MONTH\")\n",
    "    for m in sorted(analysis['mois'].unique()):\n",
    "        mask = analysis['mois'] == m\n",
    "        if mask.sum() > 0:\n",
    "            print(f\"  Month {m:2d}: MAE={analysis.loc[mask, 'abs_err'].mean():6.2f}, n={mask.sum()}\")\n",
    "\n",
    "    print(\"\\n[8] MAE BY TEMPERATURE BIN\")\n",
    "    _tbins = pd.cut(analysis['temperature_ext'], bins=[-30, -10, 0, 10, 20, 40])\n",
    "    for tb, grp in analysis.groupby(_tbins):\n",
    "        if len(grp) > 0:\n",
    "            print(f\"  {str(tb):15s}: MAE={grp['abs_err'].mean():6.2f}, \"\n",
    "                  f\"bias={grp['residual'].mean():+6.2f}, n={len(grp)}\")\n",
    "\n",
    "    print(\"\\n[9] ERRORS: POINTE vs NORMAL\")\n",
    "    for ev, label in [(0, 'Normal'), (1, 'Pointe')]:\n",
    "        mask = analysis['evenement_pointe'] == ev\n",
    "        if mask.sum() > 0:\n",
    "            print(f\"  {label:8s}: MAE={analysis.loc[mask, 'abs_err'].mean():.2f}, \"\n",
    "                  f\"bias={analysis.loc[mask, 'residual'].mean():+.2f}, n={mask.sum()}\")\n",
    "\n",
    "    print(\"\\n[10] ERRORS: WEEKEND vs WEEKDAY\")\n",
    "    for we, label in [(0, 'Weekday'), (1, 'Weekend')]:\n",
    "        mask = analysis['est_weekend'] == we\n",
    "        if mask.sum() > 0:\n",
    "            print(f\"  {label:8s}: MAE={analysis.loc[mask, 'abs_err'].mean():.2f}, \"\n",
    "                  f\"bias={analysis.loc[mask, 'residual'].mean():+.2f}, n={mask.sum()}\")\n",
    "\n",
    "    print(\"\\n[11] TOP 15 WORST PREDICTIONS\")\n",
    "    worst = analysis.nlargest(15, 'abs_err')\n",
    "    cols_show = ['horodatage_local', 'poste', 'heure', 'mois', 'temperature_ext',\n",
    "                 'y_true', 'y_pred', 'abs_err']\n",
    "    cols_avail = [c for c in cols_show if c in worst.columns]\n",
    "    print(worst[cols_avail].to_string(index=False))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"  Error in sections 6-11: {e}\")\n",
    "\n",
    "# ---- SECTION 12: FEATURE CORRELATIONS PER POSTE ----\n",
    "print(\"\\n[12] TOP FEATURE CORRELATIONS PER POSTE\")\n",
    "for p in postes:\n",
    "    try:\n",
    "        _feats_avail = [f for f in features_clean if f in train_parts[p].columns]\n",
    "        _corrs = train_parts[p][_feats_avail + ['energie_kwh']].corr()['energie_kwh'].drop('energie_kwh')\n",
    "        _corrs_sorted = _corrs.abs().sort_values(ascending=False)\n",
    "        print(f\"\\n  --- Poste {p} (top 10) ---\")\n",
    "        for i, feat in enumerate(_corrs_sorted.head(10).index, 1):\n",
    "            print(f\"    {i:2d}. {feat:40s} r={_corrs[feat]:+.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error for poste {p}: {e}\")\n",
    "\n",
    "# ---- SECTION 13: PIPELINE HEALTH ----\n",
    "print(\"\\n[13] DATA PIPELINE HEALTH\")\n",
    "print(f\"  train_clean shape: {train_clean.shape}\")\n",
    "print(f\"  test_clean shape:  {test_clean.shape}\")\n",
    "print(f\"  merged shape:      {merged.shape}\")\n",
    "any_nan = False\n",
    "for p in postes:\n",
    "    nans = train_parts[p][features_clean].isnull().sum()\n",
    "    if nans.sum() > 0:\n",
    "        print(f\"  WARNING NaN in train poste {p}: {nans[nans>0].to_dict()}\")\n",
    "        any_nan = True\n",
    "    nans_te = test_parts[p][features_clean].isnull().sum()\n",
    "    if nans_te.sum() > 0:\n",
    "        print(f\"  WARNING NaN in test poste {p}: {nans_te[nans_te>0].to_dict()}\")\n",
    "        any_nan = True\n",
    "if not any_nan:\n",
    "    print(f\"  No NaN in features (good)\")\n",
    "\n",
    "# ---- SECTION 15: SAMPLE PREDICTIONS ----\n",
    "print(\"\\n[15] SAMPLE PREDICTIONS (first 20 rows)\")\n",
    "print(merged[['horodatage_local', 'poste', 'y_true', 'y_pred']].head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"END OF DIAGNOSTIC DUMP v3 -- COPY EVERYTHING ABOVE THIS LINE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Soumission Kaggle\n",
    "\n",
    "G√©n√©rez votre fichier de soumission pour la comp√©tition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier de soumission cr√©√©: submission.csv (1754 lignes)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>energie_kwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>398.873974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>349.478473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>338.598378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>326.020949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>27.960704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  energie_kwh\n",
       "0   0   398.873974\n",
       "1   1   349.478473\n",
       "2   2   338.598378\n",
       "3   3   326.020949\n",
       "4   4    27.960704"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# G√©n√©rer les pr√©dictions pour Kaggle\n",
    "# Uses y_sim_pred from Cell A (predictions in same row order as test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(y_sim_pred)),\n",
    "    'energie_kwh': y_sim_pred\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f\"Fichier de soumission cr√©√©: submission.csv ({len(submission)} lignes)\")\n",
    "print(f\"Pr√©dictions: min={y_sim_pred.min():.2f}, mean={y_sim_pred.mean():.2f}, max={y_sim_pred.max():.2f}\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Questions de pr√©paration pour l'entrevue orale\n",
    "\n",
    "Pr√©parez-vous √† r√©pondre √† ces questions:\n",
    "\n",
    "### Fondamentaux\n",
    "1. D√©rivez la solution OLS sur le tableau.\n",
    "2. Pourquoi avez-vous utilis√© une division temporelle et non al√©atoire?\n",
    "3. Que voyez-vous dans vos r√©sidus?\n",
    "\n",
    "### R√©gularisation\n",
    "4. Pourquoi Ridge aide-t-il avec des caract√©ristiques corr√©l√©es?\n",
    "5. Comment avez-vous choisi Œª?\n",
    "6. Quel coefficient a √©t√© le plus r√©duit? Pourquoi?\n",
    "\n",
    "### Classification\n",
    "7. Quelle cible binaire avez-vous choisie? Justifiez.\n",
    "8. Votre classifieur donne P=0.7. Qu'est-ce que cela signifie?\n",
    "9. Pourquoi utiliser P(pointe) plut√¥t qu'un indicateur 0/1?\n",
    "\n",
    "### Th√©orie probabiliste\n",
    "10. Expliquez Ridge comme estimation MAP.\n",
    "11. Pourquoi la r√©gression logistique minimise-t-elle l'entropie crois√©e?\n",
    "\n",
    "### Synth√®se\n",
    "12. Parcourez votre mod√®le complet √©tape par √©tape.\n",
    "13. Quelle am√©lioration de R¬≤ √©tait la plus importante?\n",
    "14. Modifiez ce seuil en direct - que pr√©disez-vous?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
