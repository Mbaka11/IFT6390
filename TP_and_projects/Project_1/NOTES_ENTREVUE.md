# üìö Notes de Pr√©paration - Entrevue Orale (60%)

## ‚ö†Ô∏è RAPPEL: L'entrevue orale vaut 60% de la note totale!

---

## üîç Questions sur les Donn√©es et Features Engineering

### Q1: Pourquoi utilise-t-on un encodage cyclique pour les heures et les mois?

**Contexte:**
Dans le dataset, on a:

- Variables simples: `heure` (0-23), `mois` (1-12)
- Variables cycliques: `heure_sin`, `heure_cos`, `mois_sin`, `mois_cos`

**Le probl√®me avec l'encodage num√©rique simple:**

Si on utilise juste `heure = 0, 1, 2, ..., 23`:

```
Distance entre 0h et 1h:   |1 - 0| = 1
Distance entre 23h et 0h:  |0 - 23| = 23  ‚ùå FAUX!
```

Le mod√®le penserait que 23h et 0h sont **tr√®s √©loign√©es** alors qu'elles sont s√©par√©es d'**une seule heure**!

**La solution: Encodage cyclique avec sin/cos**

On transforme l'heure en coordonn√©es sur un cercle:

```python
heure_sin = sin(2œÄ √ó heure/24)
heure_cos = cos(2œÄ √ó heure/24)
```

**Visualisation mentale:**

```
        12h (midi)
         |
    9h --|-- 15h
         |
    6h --|-- 18h
         |
        0h (minuit)
         ‚Üì
       23h ‚Üê Proche de 0h! ‚úÖ
```

**Pourquoi sin ET cos (pas juste sin)?**

- Avec **seulement sin**: sin(0¬∞) = sin(360¬∞) = 0 ‚Üí 0h et 12h auraient la m√™me valeur!
- Avec **sin + cos**: Chaque heure a une combinaison **unique** (x, y) sur le cercle

**Exemple num√©rique:**

```
0h:  sin=0.00,  cos=1.00
6h:  sin=1.00,  cos=0.00
12h: sin=0.00,  cos=-1.00
18h: sin=-1.00, cos=0.00
23h: sin=0.26,  cos=0.97  ‚Üê Proche de 0h ‚úÖ
```

**Avantages principaux:**

1. ‚úÖ **Pr√©serve la proximit√©**: 23h et 0h sont proches dans l'espace des features
2. ‚úÖ **Continuit√©**: Pas de "saut" artificiel entre 23h et 0h
3. ‚úÖ **G√©n√©ralisation**: Le mod√®le apprend que les comportements √† 23h peuvent ressembler √† ceux de 0h
4. ‚úÖ **Applicabilit√©**: Fonctionne pour toute variable cyclique (jour de la semaine, mois, saison, angle, etc.)

**Pour l'entrevue, soyez pr√™t √†:**

- ‚úèÔ∏è Dessiner un cercle et placer quelques heures dessus
- üßÆ Expliquer pourquoi on a besoin de DEUX dimensions (sin + cos)
- üí° Donner un exemple concret: "La consommation √† 23h est proche de celle √† 0h (nuit)"

**Formule math√©matique √† conna√Ætre:**
$$\text{feature\_sin} = \sin\left(\frac{2\pi \times \text{valeur}}{\text{p√©riode}}\right)$$
$$\text{feature\_cos} = \cos\left(\frac{2\pi \times \text{valeur}}{\text{p√©riode}}\right)$$

O√π p√©riode = 24 pour les heures, 12 pour les mois.

---

## üìù Questions √† Pr√©parer pour l'Entrevue

### Fondamentaux

- [ ] D√©rivez la solution OLS sur le tableau
- [ ] Pourquoi division temporelle et non al√©atoire?
- [ ] Que voyez-vous dans vos r√©sidus?

### R√©gularisation

- [ ] Pourquoi Ridge aide avec des features corr√©l√©es?
- [ ] Comment choisir Œª?
- [ ] Quel coefficient a √©t√© le plus r√©duit? Pourquoi?

### Classification

- [ ] Quelle cible binaire avez-vous choisie? Justifiez.
- [ ] Le classifieur donne P=0.7. Signification?
- [ ] Pourquoi utiliser P(pointe) plut√¥t qu'un indicateur 0/1?

### Th√©orie probabiliste

- [ ] Expliquez Ridge comme estimation MAP
- [ ] Pourquoi la r√©gression logistique minimise l'entropie crois√©e?

### Synth√®se

- [ ] Parcourez votre mod√®le complet √©tape par √©tape
- [ ] Quelle am√©lioration de R¬≤ √©tait la plus importante?
- [ ] Modifiez ce seuil en direct - pr√©disez les effets

---

## üìä Concepts Cl√©s √† Ma√Ætriser

_(Cette section sera compl√©t√©e au fur et √† mesure)_

### 1. OLS (Ordinary Least Squares) - MA√éTRISER POUR L'ENTREVUE! ‚≠ê

## üìê Th√©orie Math√©matique Compl√®te

### Le Probl√®me

On cherche √† pr√©dire une variable cible $y$ (consommation √©nerg√©tique) √† partir de caract√©ristiques $\mathbf{X}$ (temp√©rature, humidit√©, etc.).

**Mod√®le lin√©aire:**
$$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip} + \epsilon_i$$

Ou en notation matricielle:
$$\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$$

O√π:

- $\mathbf{y}$ : vecteur de taille $(n, 1)$ - les valeurs cibles
- $\mathbf{X}$ : matrice de taille $(n, p+1)$ - les caract√©ristiques (avec une colonne de 1 pour l'intercept)
- $\boldsymbol{\beta}$ : vecteur de taille $(p+1, 1)$ - les coefficients √† trouver
- $\boldsymbol{\epsilon}$ : vecteur d'erreurs (bruit)

### Objectif: Minimiser l'Erreur Quadratique

On veut trouver $\hat{\boldsymbol{\beta}}$ qui minimise:
$$L(\boldsymbol{\beta}) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2$$

## üìù D√©rivation √âtape par √âtape (IMPORTANT pour l'entrevue!)

**√âtape 1: √âcrire la fonction de perte**
$$L(\boldsymbol{\beta}) = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})$$

**√âtape 2: D√©velopper**
$$L(\boldsymbol{\beta}) = \mathbf{y}^T\mathbf{y} - \mathbf{y}^T\mathbf{X}\boldsymbol{\beta} - \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y} + \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}$$

Comme $\mathbf{y}^T\mathbf{X}\boldsymbol{\beta}$ est un scalaire, $\mathbf{y}^T\mathbf{X}\boldsymbol{\beta} = \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y}$

$$L(\boldsymbol{\beta}) = \mathbf{y}^T\mathbf{y} - 2\boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y} + \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}$$

**√âtape 3: Calculer le gradient**

Rappels d'alg√®bre lin√©aire:

- $\frac{\partial}{\partial \boldsymbol{\beta}}(\mathbf{A}\boldsymbol{\beta}) = \mathbf{A}^T$
- $\frac{\partial}{\partial \boldsymbol{\beta}}(\boldsymbol{\beta}^T\mathbf{A}\boldsymbol{\beta}) = 2\mathbf{A}\boldsymbol{\beta}$ (si $\mathbf{A}$ sym√©trique)

$$\nabla_{\boldsymbol{\beta}} L = -2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}$$

**√âtape 4: √âgaler √† z√©ro (condition n√©cessaire pour un minimum)**
$$-2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} = 0$$

$$\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} = \mathbf{X}^T\mathbf{y}$$

C'est l'**√©quation normale** !

**√âtape 5: R√©soudre pour Œ≤**

Si $\mathbf{X}^T\mathbf{X}$ est inversible:
$$\boxed{\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}}$$

C'est la **solution analytique OLS** ! üéØ

## üé® Interpr√©tation G√©om√©trique

### Vision 1: Projection orthogonale

- $\mathbf{y}$ est un vecteur dans $\mathbb{R}^n$
- $\mathbf{X}\boldsymbol{\beta}$ cr√©e un sous-espace de dimension $p$
- OLS trouve la **projection orthogonale** de $\mathbf{y}$ sur ce sous-espace
- Le r√©sidu $\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}}$ est perpendiculaire au sous-espace

```
        y (vrai)
        |
        |     r√©sidu (erreur)
        |    /
        |   /
        |  /
        | /
        |/
        ≈∑ = XŒ≤ (pr√©diction)
    ------------------- (sous-espace g√©n√©r√© par X)
```

### Vision 2: Minimisation de distance

OLS trouve le point dans le sous-espace g√©n√©r√© par $\mathbf{X}$ le plus proche de $\mathbf{y}$ (au sens de la distance euclidienne).

## üíª Impl√©mentation Python avec Commentaires D√©taill√©s

```python
import numpy as np

def ols_fit(X, y):
    """
    Calcule les coefficients OLS via la solution analytique.

    Param√®tres:
        X : ndarray de forme (n, p) - matrice de caract√©ristiques SANS colonne de 1
        y : ndarray de forme (n,) - vecteur cible

    Retourne:
        beta : ndarray de forme (p+1,) - coefficients [intercept, coef1, coef2, ...]

    Points cl√©s pour l'entrevue:
    - Pourquoi ajouter une colonne de 1? Pour mod√©liser l'intercept Œ≤‚ÇÄ
    - Pourquoi np.linalg.solve et non l'inverse? Stabilit√© num√©rique + efficacit√©
    - Que faire si X^TX n'est pas inversible? Ridge / r√©gularisation
    """

    # √âTAPE 1: Ajouter colonne de 1 pour l'intercept
    # X devient (n, p+1) avec X[:, 0] = 1
    n = X.shape[0]
    X_with_intercept = np.column_stack([np.ones(n), X])
    # √âquivalent: X_with_intercept = np.c_[np.ones(n), X]

    # √âTAPE 2: Calculer X^T X (matrice de Gram)
    # Forme: (p+1, p+1)
    XTX = X_with_intercept.T @ X_with_intercept

    # √âTAPE 3: Calculer X^T y
    # Forme: (p+1,)
    XTy = X_with_intercept.T @ y

    # √âTAPE 4: R√©soudre le syst√®me X^T X Œ≤ = X^T y
    # IMPORTANT: On utilise solve() plut√¥t que inv() pour:
    #   - Stabilit√© num√©rique (√©vite erreurs d'arrondi)
    #   - Efficacit√© (O(n¬≥) vs O(n¬≥) mais avec meilleure constante)
    #   - Robustesse (g√®re mieux les matrices mal conditionn√©es)
    beta = np.linalg.solve(XTX, XTy)

    # Alternative (NON recommand√©e):
    # beta = np.linalg.inv(XTX) @ XTy  # ‚ùå Moins stable!

    return beta
    # beta[0] = intercept (Œ≤‚ÇÄ)
    # beta[1:] = coefficients des features (Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤‚Çö)


def ols_predict(X, beta):
    """
    Pr√©dit les valeurs avec les coefficients OLS.

    Param√®tres:
        X : ndarray de forme (n, p) - caract√©ristiques SANS colonne de 1
        beta : ndarray de forme (p+1,) - coefficients [intercept, coef1, ...]

    Retourne:
        y_pred : ndarray de forme (n,) - pr√©dictions

    Points pour l'entrevue:
    - Comment s√©parer intercept et coefficients? beta[0] vs beta[1:]
    - Forme matricielle: y = X @ w + b, o√π w=beta[1:] et b=beta[0]
    """

    # M√âTHODE 1: Ajouter colonne de 1 et multiplier
    n = X.shape[0]
    X_with_intercept = np.column_stack([np.ones(n), X])
    y_pred = X_with_intercept @ beta

    # M√âTHODE 2 (√©quivalente): S√©parer intercept et coefficients
    # y_pred = beta[0] + X @ beta[1:]

    return y_pred


# ============================================
# EXEMPLE D'UTILISATION AVEC EXPLICATIONS
# ============================================

# Supposons qu'on a:
# - n = 8760 observations (1 an de donn√©es horaires)
# - p = 3 features: temp√©rature, humidit√©, vitesse_vent

# CHARGEMENT DES DONN√âES
# X_train shape: (8760, 3)
# y_train shape: (8760,)

# 1. ENTRA√éNEMENT
beta_ols = ols_fit(X_train, y_train)
# beta_ols shape: (4,)
# beta_ols[0] = intercept = consommation de base
# beta_ols[1] = coefficient temp√©rature
# beta_ols[2] = coefficient humidit√©
# beta_ols[3] = coefficient vitesse_vent

print(f"Intercept (Œ≤‚ÇÄ): {beta_ols[0]:.2f} kWh")
print(f"Coefficient temp√©rature (Œ≤‚ÇÅ): {beta_ols[1]:.2f} kWh/¬∞C")
print(f"Coefficient humidit√© (Œ≤‚ÇÇ): {beta_ols[2]:.2f} kWh/%")
print(f"Coefficient vent (Œ≤‚ÇÉ): {beta_ols[3]:.2f} kWh/(km/h)")

# INTERPR√âTATION (pour l'entrevue):
# Si Œ≤‚ÇÅ = -5.2, cela signifie:
# "Pour chaque degr√© de temp√©rature en plus, la consommation
#  diminue de 5.2 kWh (moins de chauffage)"

# 2. PR√âDICTION
y_pred_train = ols_predict(X_train, beta_ols)
y_pred_test = ols_predict(X_test, beta_ols)

# 3. √âVALUATION
from sklearn.metrics import r2_score, mean_squared_error

r2_train = r2_score(y_train, y_pred_train)
r2_test = r2_score(y_test, y_pred_test)
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))

print(f"\nPerformance:")
print(f"  R¬≤ train: {r2_train:.4f}")
print(f"  R¬≤ test: {r2_test:.4f}")
print(f"  RMSE test: {rmse_test:.2f} kWh")

# Pour l'entrevue: Soyez pr√™t √† expliquer R¬≤!
# R¬≤ = 0.75 signifie: "Le mod√®le explique 75% de la variance de y"
# R¬≤ = 1.0 ‚Üí Pr√©diction parfaite
# R¬≤ < 0 ‚Üí Mod√®le pire qu'une simple moyenne


# ============================================
# COMPARAISON AVEC SKLEARN (Validation)
# ============================================

from sklearn.linear_model import LinearRegression

model_sklearn = LinearRegression()
model_sklearn.fit(X_train, y_train)

print("\n=== Validation avec sklearn ===")
print(f"Intercept - Vous: {beta_ols[0]:.6f}")
print(f"Intercept - sklearn: {model_sklearn.intercept_:.6f}")
print(f"Coefficients identiques: {np.allclose(beta_ols[1:], model_sklearn.coef_)}")
# Devrait afficher True si impl√©mentation correcte!
```

## ‚ö†Ô∏è Points Critiques pour l'Entrevue

### 1. Pourquoi ajouter une colonne de 1?

**Question type:** "Pourquoi ajoutez-vous np.ones dans votre code?"

**R√©ponse:**

- Sans intercept: $y = \beta_1 x_1 + \beta_2 x_2$ ‚Üí la droite passe par l'origine (0, 0)
- Avec intercept: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2$ ‚Üí plus flexible
- La colonne de 1 permet de traiter $\beta_0$ comme les autres coefficients dans le calcul matriciel

### 2. Pourquoi np.linalg.solve plut√¥t que l'inverse?

**Question type:** "Vous n'utilisez pas l'inverse explicitement, pourquoi?"

**R√©ponse:**

- `solve(A, b)` r√©sout $Ax = b$ directement via d√©composition LU/Cholesky
- `inv(A) @ b` calcule d'abord $A^{-1}$ puis multiplie ‚Üí 2 op√©rations co√ªteuses
- `solve()` est **num√©riquement plus stable** (moins d'erreurs d'arrondi)
- Exemple: Si $X^TX$ est mal conditionn√©e, inverse peut √©chouer

### 3. Quand OLS √©choue-t-il?

**Question type:** "Dans quelles situations OLS pose-t-il probl√®me?"

**R√©ponse:**

1. **$X^TX$ non inversible** (collin√©arit√© parfaite)
   - Exemple: temp√©rature_celsius et temp√©rature_fahrenheit
   - Solution: Ridge r√©gularisation
2. **Mal conditionn√©e** (features tr√®s corr√©l√©es)
   - Coefficients instables (variance √©lev√©e)
   - Solution: Ridge ou PCA
3. **p > n** (plus de features que d'observations)
   - Syst√®me sous-d√©termin√©
   - Solution: Ridge ou Lasso

4. **Outliers** (valeurs extr√™mes)
   - OLS sensible aux outliers (erreur quadratique)
   - Solution: R√©gression robuste (Huber loss)

### 4. Complexit√© computationnelle

**Question type:** "Quelle est la complexit√© de OLS?"

**R√©ponse:**

- Calcul de $X^TX$: $O(np^2)$ o√π n=#observations, p=#features
- R√©solution du syst√®me: $O(p^3)$
- **Total: $O(np^2 + p^3)$**
- Dominant: $O(np^2)$ si $n >> p$ (cas typique)

### 5. Hypoth√®ses de OLS

**Pour l'entrevue, conna√Ætre les hypoth√®ses (mais pas besoin de les v√©rifier pour ce projet):**

1. **Lin√©arit√©**: La relation est lin√©aire
2. **Ind√©pendance**: Les observations sont ind√©pendantes
3. **Homosc√©dasticit√©**: Variance constante des erreurs
4. **Normalit√©**: Les erreurs suivent une loi normale (pour l'inf√©rence)
5. **Pas de multicolin√©arit√©**: Les features ne sont pas trop corr√©l√©es

## üéØ Checklist pour l'Entrevue OLS

Pratiquez ces exercices:

- [ ] D√©river $\hat{\beta} = (X^TX)^{-1}X^Ty$ au tableau en 5 minutes
- [ ] Expliquer pourquoi on minimise l'erreur quadratique (et non absolue)
- [ ] Dessiner l'interpr√©tation g√©om√©trique (projection)
- [ ] Coder ols_fit() de m√©moire en 3 minutes
- [ ] Expliquer np.linalg.solve vs inv
- [ ] Interpr√©ter un coefficient: "Œ≤‚ÇÅ = -5.2 signifie..."
- [ ] Expliquer R¬≤ √† votre grand-m√®re
- [ ] Donner 3 situations o√π OLS √©choue

### 2. R√©gression Logistique + Descente de Gradient - MA√éTRISER POUR L'ENTREVUE! ‚≠ê

## üéØ Pourquoi la R√©gression Logistique?

**Rappel du contexte:** On veut pr√©dire si une heure donn√©e sera un **√©v√©nement de pointe** (1) ou **normale** (0).

- **OLS (Partie 1):** Pr√©dit des valeurs continues (kWh)
- **R√©gression Logistique (Partie 2):** Pr√©dit des **probabilit√©s** entre 0 et 1

## üìê Th√©orie Math√©matique Compl√®te

### Le Probl√®me de Classification Binaire

On a:

- $y_i \in \{0, 1\}$ : √©tiquette binaire (0 = normal, 1 = pointe)
- $\mathbf{x}_i \in \mathbb{R}^p$ : vecteur de caract√©ristiques (temp√©rature, heure, etc.)

**Objectif:** Mod√©liser $P(y=1 | \mathbf{x})$ (probabilit√© d'√™tre en pointe sachant les features)

### Fonction Sigmo√Øde (Logistique)

Pour transformer $z \in \mathbb{R}$ en probabilit√© $p \in [0, 1]$:

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**Propri√©t√©s importantes:**

- $\sigma(0) = 0.5$ (point d'inflexion)
- $\lim_{z \to +\infty} \sigma(z) = 1$
- $\lim_{z \to -\infty} \sigma(z) = 0$
- $\sigma'(z) = \sigma(z)(1 - \sigma(z))$ (d√©riv√©e √©l√©gante!)

**Visualisation mentale:**

```
p
1.0 |           ___________
    |         /
0.5 |       /    ‚Üê Point d'inflexion (z=0)
    |     /
0.0 |___/
    |___|___|___|___|___|___> z
       -5   0   5
```

### Mod√®le de R√©gression Logistique

$$z_i = \beta_0 + \beta_1 x_{i1} + ... + \beta_p x_{ip} = \boldsymbol{\beta}^T \mathbf{x}_i$$

$$P(y_i = 1 | \mathbf{x}_i) = \sigma(z_i) = \frac{1}{1 + e^{-\boldsymbol{\beta}^T \mathbf{x}_i}}$$

**Interpr√©tation:**

- Si $z > 0$ ‚Üí $p > 0.5$ ‚Üí Classe 1 (pointe)
- Si $z < 0$ ‚Üí $p < 0.5$ ‚Üí Classe 0 (normal)
- Si $z = 0$ ‚Üí $p = 0.5$ ‚Üí Fronti√®re de d√©cision

### Fonction de Perte: Entropie Crois√©e (Cross-Entropy)

**Pourquoi pas MSE (Mean Squared Error)?**

- OLS utilise MSE: $L = \sum (y - \hat{y})^2$
- Avec sigmo√Øde, MSE ‚Üí **fonction non-convexe** ‚Üí pleins de minima locaux ‚ùå

**Entropie Crois√©e Binaire:**

Pour **une seule observation** $(x_i, y_i)$:

$$\mathcal{L}_i = -\left[ y_i \log(p_i) + (1-y_i) \log(1-p_i) \right]$$

O√π $p_i = \sigma(\boldsymbol{\beta}^T \mathbf{x}_i)$

**Intuition:**

- Si $y_i = 1$ : perte = $-\log(p_i)$
  - Si $p_i \to 1$ ‚Üí perte $\to 0$ ‚úÖ (bonne pr√©diction)
  - Si $p_i \to 0$ ‚Üí perte $\to +\infty$ ‚ùå (tr√®s mauvaise pr√©diction)
- Si $y_i = 0$ : perte = $-\log(1-p_i)$
  - Si $p_i \to 0$ ‚Üí perte $\to 0$ ‚úÖ
  - Si $p_i \to 1$ ‚Üí perte $\to +\infty$ ‚ùå

**Pour l'ensemble du dataset:**

$$L(\boldsymbol{\beta}) = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1-y_i) \log(1-p_i) \right]$$

O√π $p_i = \sigma(\boldsymbol{\beta}^T \mathbf{x}_i)$

## üìù D√©rivation du Gradient (IMPORTANT pour l'entrevue!)

**Objectif:** Calculer $\nabla_{\boldsymbol{\beta}} L$ pour la descente de gradient

### Calcul pour une observation

Posons $p = \sigma(z)$ o√π $z = \boldsymbol{\beta}^T \mathbf{x}$

$$\mathcal{L} = -\left[ y \log(p) + (1-y) \log(1-p) \right]$$

**√âtape 1: D√©river par rapport √† p**

$$\frac{\partial \mathcal{L}}{\partial p} = -\frac{y}{p} + \frac{1-y}{1-p}$$

$$= \frac{-y(1-p) + (1-y)p}{p(1-p)} = \frac{p - y}{p(1-p)}$$

**√âtape 2: D√©river p par rapport √† z (r√®gle de d√©rivation sigmo√Øde)**

$$\frac{\partial p}{\partial z} = \frac{\partial \sigma(z)}{\partial z} = \sigma(z)(1 - \sigma(z)) = p(1-p)$$

**√âtape 3: Cha√Æne pour d√©river par rapport √† z**

$$\frac{\partial \mathcal{L}}{\partial z} = \frac{\partial \mathcal{L}}{\partial p} \cdot \frac{\partial p}{\partial z} = \frac{p - y}{p(1-p)} \cdot p(1-p) = p - y$$

**R√©sultat magique!** ‚ú® Le gradient simplifie √©norm√©ment!

**√âtape 4: D√©river par rapport √† Œ≤**

Comme $z = \boldsymbol{\beta}^T \mathbf{x}$, on a $\frac{\partial z}{\partial \boldsymbol{\beta}} = \mathbf{x}$

$$\frac{\partial \mathcal{L}}{\partial \boldsymbol{\beta}} = \frac{\partial \mathcal{L}}{\partial z} \cdot \frac{\partial z}{\partial \boldsymbol{\beta}} = (p - y) \mathbf{x}$$

### Pour tout le dataset (notation matricielle)

$$\nabla_{\boldsymbol{\beta}} L = \frac{1}{n} \sum_{i=1}^{n} (p_i - y_i) \mathbf{x}_i = \frac{1}{n} \mathbf{X}^T (\mathbf{p} - \mathbf{y})$$

O√π:

- $\mathbf{X}$ : matrice $(n, p+1)$ des caract√©ristiques
- $\mathbf{p}$ : vecteur $(n,)$ des probabilit√©s pr√©dites $\sigma(\mathbf{X}\boldsymbol{\beta})$
- $\mathbf{y}$ : vecteur $(n,)$ des √©tiquettes vraies

**Formule finale:**

$$\boxed{\nabla_{\boldsymbol{\beta}} L = \frac{1}{n} \mathbf{X}^T \left(\sigma(\mathbf{X}\boldsymbol{\beta}) - \mathbf{y}\right)}$$

## üîÑ Descente de Gradient

**L'id√©e:** Pas de solution analytique comme OLS ‚Üí on it√®re!

### Algorithme

```
1. Initialiser Œ≤ ‚Üê 0 (ou al√©atoire)
2. Pour k = 1 √† n_iter:
     a. Calculer les pr√©dictions: p = œÉ(XŒ≤)
     b. Calculer le gradient: g = (1/n) X^T (p - y)
     c. Mise √† jour: Œ≤ ‚Üê Œ≤ - Œ±¬∑g
     d. (Optionnel) Calculer et stocker la perte pour suivre convergence
3. Retourner Œ≤
```

**Param√®tres:**

- $\alpha$ (alpha ou lr) : **taux d'apprentissage** (learning rate)
- $n_{iter}$ : nombre d'it√©rations

### Choix du Taux d'Apprentissage (Learning Rate)

**Question cl√© d'entrevue:** "Comment avez-vous choisi le learning rate?"

**Si Œ± trop petit (ex: 0.0001):**

- ‚úÖ Convergence garantie (si fonction convexe)
- ‚ùå **Tr√®s lent** (des milliers d'it√©rations)
- Graphe loss: descente lisse mais tr√®s graduelle

**Si Œ± trop grand (ex: 10.0):**

- ‚ùå **Divergence!** (oscille de plus en plus)
- ‚ùå Peut sauter par-dessus le minimum
- Graphe loss: zigzag, mont√©e au lieu de descendre

**Optimal (ex: 0.1 - 1.0 avec normalisation):**

- ‚úÖ Convergence rapide en ~100-500 it√©rations
- ‚úÖ Stable
- Graphe loss: descente rapide puis plateau

**Conseil pratique:**

1. **Toujours normaliser les features** (StandardScaler) ‚Üí permet d'utiliser Œ± plus grand
2. Tester plusieurs valeurs: 0.001, 0.01, 0.1, 1.0
3. Tracer la courbe de perte pour v√©rifier la convergence
4. Si la perte augmente ‚Üí r√©duire Œ±

## üíª Impl√©mentation Python avec Commentaires D√©taill√©s

```python
import numpy as np

# ============================================
# FONCTIONS DE BASE
# ============================================

def sigmoid(z):
    """
    Fonction sigmo√Øde (logistique).

    Param√®tres:
        z : ndarray de n'importe quelle forme

    Retourne:
        sigma(z) : ndarray de m√™me forme, valeurs entre 0 et 1

    Points pour l'entrevue:
    - Pourquoi clip? Pour √©viter overflow avec exp(-z) quand z est tr√®s n√©gatif
    - exp(-500) ‚âà 0, donc œÉ(500) ‚âà 1
    - exp(500) ‚Üí overflow, mais exp(-(-500)) = exp(500) ‚Üí probl√®me!
    - Clip z ‚àà [-500, 500] garantit stabilit√© num√©rique
    """
    # Clip pour stabilit√© num√©rique
    z = np.clip(z, -500, 500)
    return 1 / (1 + np.exp(-z))


def cross_entropy_loss(y_true, y_pred_proba):
    """
    Calcule la perte d'entropie crois√©e binaire.

    Param√®tres:
        y_true : ndarray (n,) - √©tiquettes vraies (0 ou 1)
        y_pred_proba : ndarray (n,) - probabilit√©s pr√©dites P(Y=1)

    Retourne:
        loss : float - perte moyenne

    Points pour l'entrevue:
    - Pourquoi clip les probabilit√©s? log(0) = -‚àû ‚Üí erreur num√©rique
    - eps = 1e-15 √©vite log(0) et log(1) exactement
    - Formule: -mean[ y¬∑log(p) + (1-y)¬∑log(1-p) ]
    """
    # Clip pour √©viter log(0)
    eps = 1e-15
    y_pred_proba = np.clip(y_pred_proba, eps, 1 - eps)

    # Calcul de l'entropie crois√©e
    loss = -np.mean(
        y_true * np.log(y_pred_proba) +
        (1 - y_true) * np.log(1 - y_pred_proba)
    )

    return loss


def logistic_gradient(X, y, beta):
    """
    Calcule le gradient de la perte d'entropie crois√©e.

    Param√®tres:
        X : ndarray (n, p+1) - caract√©ristiques AVEC colonne de 1
        y : ndarray (n,) - √©tiquettes binaires (0 ou 1)
        beta : ndarray (p+1,) - coefficients actuels

    Retourne:
        gradient : ndarray (p+1,) - gradient ‚àáL

    Formule: ‚àáL = (1/n) X^T (œÉ(XŒ≤) - y)

    Points pour l'entrevue:
    - Pourquoi cette formule simple? Gr√¢ce √† la d√©riv√©e de œÉ!
    - Interpr√©tation: gradient = moyenne des erreurs pond√©r√©es par les features
    - Si p_i > y_i (sur-pr√©diction) ‚Üí gradient positif ‚Üí diminuer Œ≤
    """
    n = len(y)

    # Pr√©dictions: p = œÉ(XŒ≤)
    z = X @ beta  # Combinaison lin√©aire
    p = sigmoid(z)  # Probabilit√©s

    # Erreur: p - y
    error = p - y

    # Gradient: (1/n) X^T (p - y)
    gradient = (1/n) * (X.T @ error)

    return gradient


# ============================================
# ENTRA√éNEMENT PAR DESCENTE DE GRADIENT
# ============================================

def logistic_fit_gd(X, y, lr=0.1, n_iter=1000, verbose=False):
    """
    Entra√Æne la r√©gression logistique par descente de gradient.

    Param√®tres:
        X : ndarray (n, p) - caract√©ristiques SANS colonne de 1
        y : ndarray (n,) - √©tiquettes binaires (0 ou 1)
        lr : float - taux d'apprentissage (learning rate)
        n_iter : int - nombre d'it√©rations
        verbose : bool - afficher progression tous les 100 iter

    Retourne:
        beta : ndarray (p+1,) - coefficients optimaux
        losses : list - historique des pertes (pour tracer convergence)

    Points pour l'entrevue:
    - Pourquoi initialiser Œ≤ √† 0? Simple et fonctionne bien (fonction convexe)
    - Alternative: initialisation al√©atoire (peu d'impact ici)
    - Crit√®re d'arr√™t: nombre d'it√©rations fixe (pourrait √™tre convergence)
    """
    n, p = X.shape

    # √âTAPE 1: Ajouter colonne de 1 pour l'intercept
    X_with_intercept = np.column_stack([np.ones(n), X])
    # Shape devient (n, p+1)

    # √âTAPE 2: Initialiser Œ≤ √† z√©ro
    beta = np.zeros(p + 1)

    # √âTAPE 3: Historique des pertes (pour analyser convergence)
    losses = []

    # √âTAPE 4: Boucle de descente de gradient
    for iteration in range(n_iter):
        # a. Calculer les probabilit√©s actuelles
        z = X_with_intercept @ beta
        p = sigmoid(z)

        # b. Calculer la perte (pour monitoring)
        loss = cross_entropy_loss(y, p)
        losses.append(loss)

        # c. Calculer le gradient
        gradient = logistic_gradient(X_with_intercept, y, beta)

        # d. Mise √† jour des param√®tres
        beta = beta - lr * gradient
        # Œ≤_new = Œ≤_old - Œ±¬∑‚àáL

        # e. Affichage optionnel
        if verbose and (iteration % 100 == 0 or iteration == n_iter - 1):
            print(f"It√©ration {iteration:4d} | Loss: {loss:.6f}")

    return beta, losses


def logistic_predict_proba(X, beta):
    """
    Retourne les probabilit√©s P(Y=1|X).

    Param√®tres:
        X : ndarray (n, p) - caract√©ristiques SANS colonne de 1
        beta : ndarray (p+1,) - coefficients [intercept, coef1, ...]

    Retourne:
        proba : ndarray (n,) - probabilit√©s entre 0 et 1

    Points pour l'entrevue:
    - Diff√©rence avec OLS: on retourne des probabilit√©s, pas des valeurs continues
    - Pour classification: seuil = 0.5 ‚Üí classe 1 si proba >= 0.5
    - Pourquoi retourner proba et non classe? Plus d'information!
      On peut ajuster le seuil selon le contexte (0.3, 0.5, 0.7...)
    """
    n = X.shape[0]

    # Ajouter colonne de 1
    X_with_intercept = np.column_stack([np.ones(n), X])

    # Calculer z = XŒ≤
    z = X_with_intercept @ beta

    # Appliquer sigmo√Øde pour obtenir probabilit√©s
    proba = sigmoid(z)

    return proba


def logistic_predict_class(X, beta, threshold=0.5):
    """
    Retourne les classes pr√©dites (0 ou 1).

    Param√®tres:
        threshold : float - seuil de d√©cision (par d√©faut 0.5)

    Points pour l'entrevue:
    - Pourquoi threshold=0.5? C'est la fronti√®re naturelle (z=0)
    - Peut ajuster selon co√ªt d'erreur:
      ‚Ä¢ √âviter faux n√©gatifs ‚Üí threshold = 0.3 (plus sensible)
      ‚Ä¢ √âviter faux positifs ‚Üí threshold = 0.7 (plus conservateur)
    """
    proba = logistic_predict_proba(X, beta)
    return (proba >= threshold).astype(int)


# ============================================
# EXEMPLE COMPLET D'UTILISATION
# ============================================

# DONN√âES
# X_train: (8760, 4) - temp√©rature, heure_sin, heure_cos, weekend
# y_train: (8760,) - √©v√©nement de pointe (0 ou 1)

# IMPORTANT: Normaliser les features pour la descente de gradient!
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Pourquoi normaliser?
# - Features avec des √©chelles diff√©rentes (temp√©rature: -20 √† 30, humidit√©: 0 √† 100)
# - Gradient descent converge plus vite avec features normalis√©es
# - Permet d'utiliser un learning rate plus √©lev√©

# 1. ENTRA√éNEMENT
print("=== Entra√Ænement R√©gression Logistique ===")
beta_log, losses = logistic_fit_gd(
    X_train_scaled,
    y_train,
    lr=0.1,        # Taux d'apprentissage
    n_iter=500,    # 500 it√©rations
    verbose=True   # Afficher progression
)

# beta_log shape: (5,)  ‚Üí  [intercept, Œ≤_temp, Œ≤_hsin, Œ≤_hcos, Œ≤_weekend]

print(f"\nCoefficients appris:")
print(f"  Intercept (Œ≤‚ÇÄ): {beta_log[0]:.4f}")
features_names = ['temp√©rature', 'heure_sin', 'heure_cos', 'weekend']
for i, name in enumerate(features_names):
    print(f"  Œ≤_{name}: {beta_log[i+1]:.4f}")

# INTERPR√âTATION (pour l'entrevue):
# Si Œ≤_temp < 0: temp√©ratures √©lev√©es ‚Üí moins de probabilit√© de pointe
# Si Œ≤_weekend < 0: weekend ‚Üí moins de probabilit√© de pointe (consommation plus faible)


# 2. VISUALISER CONVERGENCE
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(losses)
plt.xlabel('It√©ration')
plt.ylabel('Perte (Entropie Crois√©e)')
plt.title('Convergence de la Descente de Gradient')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Pour l'entrevue: expliquer la forme de la courbe
# - Descente rapide au d√©but (loin du minimum)
# - Plateau ensuite (proche du minimum)
# - Si oscillations ‚Üí learning rate trop grand


# 3. PR√âDICTIONS
proba_train = logistic_predict_proba(X_train_scaled, beta_log)
proba_test = logistic_predict_proba(X_test_scaled, beta_log)

# Classes (avec seuil 0.5)
y_pred_train = (proba_train >= 0.5).astype(int)
y_pred_test = (proba_test >= 0.5).astype(int)


# 4. √âVALUATION
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

print("\n=== √âvaluation ===")
print(f"Accuracy train: {accuracy_score(y_train, y_pred_train):.4f}")
print(f"Accuracy test: {accuracy_score(y_test, y_pred_test):.4f}")

print("\nRapport de classification:")
print(classification_report(y_test, y_pred_test,
                          target_names=['Normal', 'Pointe']))

# Matrice de confusion
cm = confusion_matrix(y_test, y_pred_test)
print("\nMatrice de confusion:")
print(f"                Pr√©dit Normal  Pr√©dit Pointe")
print(f"Vrai Normal          {cm[0,0]:5d}          {cm[0,1]:5d}")
print(f"Vrai Pointe          {cm[1,0]:5d}          {cm[1,1]:5d}")


# 5. COMPARAISON AVEC SKLEARN
from sklearn.linear_model import LogisticRegression

model_sklearn = LogisticRegression()
model_sklearn.fit(X_train_scaled, y_train)

print("\n=== Comparaison avec sklearn ===")
print(f"Intercept - Vous: {beta_log[0]:.6f}")
print(f"Intercept - sklearn: {model_sklearn.intercept_[0]:.6f}")
print(f"Coefficients proches: {np.allclose(beta_log[1:], model_sklearn.coef_[0], atol=0.1)}")
# Note: sklearn utilise optimiseur diff√©rent (L-BFGS) ‚Üí petites diff√©rences OK

```

## ‚ö†Ô∏è Points Critiques pour l'Entrevue

### 1. Pourquoi utiliser l'entropie crois√©e et non MSE?

**Question type:** "Pourquoi pas Mean Squared Error comme pour OLS?"

**R√©ponse:**

- Avec sigmo√Øde + MSE ‚Üí **fonction non-convexe** (plusieurs minima locaux)
- Gradient descent peut rester bloqu√© dans minimum local ‚ùå
- Entropie crois√©e + sigmo√Øde ‚Üí **fonction convexe** (un seul minimum global) ‚úÖ
- Bonus: d√©rivation math√©matique plus √©l√©gante (gradient = erreur √ó features)

### 2. Interpr√©ter la sigmo√Øde

**Question type:** "Expliquez ce que fait la fonction sigmo√Øde."

**R√©ponse:**

- Transforme $z \in (-\infty, +\infty)$ en $p \in (0, 1)$
- Forme en "S" ‚Üí transition douce entre 0 et 1
- $z = 0$ ‚Üí $p = 0.5$ ‚Üí fronti√®re de d√©cision
- Si $z$ tr√®s n√©gatif ‚Üí presque s√ªr classe 0
- Si $z$ tr√®s positif ‚Üí presque s√ªr classe 1

**Au tableau:** Dessiner la courbe sigmo√Øde!

### 3. Pourquoi retourner des probabilit√©s?

**Question type:** "Pourquoi pr√©dire des probabilit√©s plut√¥t que directement la classe?"

**R√©ponse:**

1. **Plus d'information**: P=0.51 vs P=0.99 ‚Üí tous deux classe 1, mais confiance diff√©rente!
2. **Flexibilit√© du seuil**: Peut ajuster selon le contexte
   - D√©tection fraude: seuil = 0.3 (tol√©rer faux positifs)
   - Diagnostic m√©dical: seuil = 0.7 (√©viter faux positifs)
3. **Utilisable comme feature**: Dans Partie 4, on utilise P(pointe) comme variable!

### 4. Comment choisir le learning rate?

**Question type:** "Vous avez utilis√© lr=0.1, pourquoi?"

**R√©ponse pratique:**

1. Normaliser d'abord les features (StandardScaler)
2. Tester [0.001, 0.01, 0.1, 1.0]
3. Tracer la courbe de perte:
   - Descend bien ‚Üí bon choix ‚úÖ
   - Oscille/diverge ‚Üí trop grand ‚ùå
   - Plateau trop lent ‚Üí trop petit ‚ùå
4. Pour ce dataset normalis√©: lr=0.1 converge en ~500 iter

### 5. Diff√©rences OLS vs R√©gression Logistique

**Question de synth√®se importante!**

| Aspect          | OLS                           | R√©gression Logistique           |
| --------------- | ----------------------------- | ------------------------------- |
| **Type**        | R√©gression                    | Classification                  |
| **Cible**       | Continue ($y \in \mathbb{R}$) | Binaire ($y \in \{0,1\}$)       |
| **Pr√©diction**  | Valeur r√©elle                 | Probabilit√©                     |
| **Fonction**    | Lin√©aire $y = X\beta$         | Sigmo√Øde $p = \sigma(X\beta)$   |
| **Perte**       | MSE (erreur quadratique)      | Entropie crois√©e                |
| **Solution**    | Analytique: $(X^TX)^{-1}X^Ty$ | It√©rative: descente de gradient |
| **Convergence** | Instantan√©e (1 calcul)        | Progressive (~500 iter)         |
| **Hypoth√®se**   | Erreurs normales              | Distribution de Bernoulli       |

## üéØ Checklist pour l'Entrevue R√©gression Logistique

Pratiquez ces exercices:

- [ ] Dessiner la fonction sigmo√Øde au tableau
- [ ] Expliquer pourquoi œÉ'(z) = œÉ(z)(1-œÉ(z))
- [ ] D√©river le gradient de l'entropie crois√©e (5 √©tapes)
- [ ] Coder sigmoid() de m√©moire
- [ ] Expliquer l'algorithme de descente de gradient en 30 secondes
- [ ] Interpr√©ter un coefficient: "Œ≤‚ÇÅ = -2.3 pour temp√©rature signifie..."
- [ ] Tracer + commenter une courbe de convergence
- [ ] Expliquer quand utiliser seuil ‚â† 0.5
- [ ] Diff√©rencier r√©gression vs classification
- [ ] Expliquer pourquoi normaliser les features

---

## PARTIE 3: Ing√©nierie des Caract√©ristiques (Feature Engineering) ‚≠ê

### üéØ Pourquoi l'Ing√©nierie des Caract√©ristiques?

**Citation c√©l√®bre en ML:**

> "Les algorithmes viennent et vont, mais les features bien con√ßues restent." - Andrew Ng

**R√©alit√©:**

- Un mod√®le simple (Ridge) avec **bonnes features** > mod√®le complexe (Deep Learning) avec features basiques
- **80% du travail en ML** = comprendre les donn√©es et cr√©er de bonnes features
- **20% du travail** = choisir et optimiser l'algorithme

**Pour ce projet:**

- OLS/Ridge sont des mod√®les **lin√©aires** ‚Üí pas tr√®s flexibles
- Solution: cr√©er des features qui **capturent les patterns non-lin√©aires**
- Exemple: interaction temp√©rature √ó heure capture "il fait froid la nuit"

## üìä Contexte: S√©rie Temporelle d'√ânergie

### Caract√©ristiques des Donn√©es √ânerg√©tiques

**Patterns typiques:**

1. **Saisonnalit√© horaire**: Pointe le matin (7-9h) et soir (17-19h)
2. **Saisonnalit√© journali√®re**: Weekend < Semaine
3. **Saisonnalit√© mensuelle**: Hiver > √ât√© (chauffage)
4. **D√©pendance temporelle**: Consommation √† t ‚âà consommation √† t-1
5. **D√©pendance m√©t√©o**: Froid ‚Üí plus de chauffage

**D√©fi:**

- Train = hiver (haute consommation)
- Test = printemps/√©t√© (basse consommation)
- Mod√®le doit **g√©n√©raliser √† travers les saisons**!

## üß∞ Types de Caract√©ristiques √† Cr√©er

### 1. Retards (Lags) - Autocorr√©lation

**Id√©e:** La consommation pass√©e aide √† pr√©dire la consommation future

**Exemples:**

```python
# Retard de 1 heure
df['energie_lag1'] = df['energie_kwh'].shift(1)

# Retard de 24 heures (m√™me heure hier)
df['energie_lag24'] = df['energie_kwh'].shift(24)

# Retard de 168 heures (m√™me heure la semaine derni√®re)
df['energie_lag168'] = df['energie_kwh'].shift(168)
```

**Intuition:**

- Si consommation √† 8h hier = 150 kWh ‚Üí probable aujourd'hui aussi
- Capture les **patterns qui se r√©p√®tent** quotidiennement/hebdomadairement

**‚ö†Ô∏è ATTENTION - Fuite de donn√©es (Data Leakage):**

**MAUVAIS (fuite):**

```python
# NE PAS FAIRE: utiliser energie_lag1 pour pr√©dire energie_kwh sur test_kaggle
# Probl√®me: Sur Kaggle, on n'a PAS la consommation pass√©e du test!
```

**BON (pas de fuite):**

```python
# OK pour validation locale (on a les vraies valeurs)
# Mais pour Kaggle, il faut soit:
# 1. Ne pas utiliser de lags
# 2. Pr√©dire de fa√ßon autor√©gressive (t ‚Üí t+1 ‚Üí t+2)
```

**Pour ce projet:**

- Utilisez les lags pour **am√©liorer le mod√®le local** (train/test avec cible)
- Pour Kaggle: soit enlever les lags, soit pr√©dire r√©cursivement

### 2. Statistiques Glissantes (Rolling Statistics)

**Id√©e:** Moyennes/√©carts-types sur une fen√™tre temporelle

**Exemples:**

```python
# Moyenne mobile sur 6 heures
df['energie_rolling_mean_6h'] = df['energie_kwh'].rolling(window=6).mean()

# √âcart-type mobile sur 24 heures
df['energie_rolling_std_24h'] = df['energie_kwh'].rolling(window=24).std()

# Min/Max sur les 12 derni√®res heures
df['energie_rolling_min_12h'] = df['energie_kwh'].rolling(window=12).min()
df['energie_rolling_max_12h'] = df['energie_kwh'].rolling(window=12).max()
```

**Intuition:**

- Moyenne mobile = lisse les fluctuations, capture la tendance
- √âcart-type mobile = mesure la volatilit√©/stabilit√© de la consommation
- Min/Max = d√©tecte les extr√™mes r√©cents

**Avantage vs Lags simples:**

- Moins sensible aux outliers ponctuels
- Capture des **tendances locales**

### 3. Interactions entre Variables

**Id√©e:** Combiner deux features pour capturer des effets conjoints

**Exemples:**

```python
# Temp√©rature √ó heure (froid + nuit = beaucoup de chauffage)
df['temp_heure_interaction'] = df['temperature_ext'] * df['heure_cos']

# Temp√©rature √ó weekend (comportement diff√©rent)
df['temp_weekend'] = df['temperature_ext'] * df['est_weekend']

# Temp√©rature au carr√© (effet non-lin√©aire)
df['temp_squared'] = df['temperature_ext'] ** 2
```

**Intuition:**

- **Lin√©aire:** $y = \beta_1 \cdot temp + \beta_2 \cdot heure$ ‚Üí effets s√©par√©s
- **Interaction:** $y = \beta_3 \cdot (temp \times heure)$ ‚Üí effet conjoint!

**Exemple concret:**

- √Ä 20¬∞C, l'heure importe peu (pas de chauffage)
- √Ä -10¬∞C, l'heure importe beaucoup (chauffage la nuit)
- Interaction capture cette **d√©pendance conditionnelle**

### 4. Transformations M√©t√©orologiques

**Degr√©-jours de chauffage (Heating Degree Days):**

```python
# Si < 18¬∞C, besoin de chauffage
df['degres_jours_chauffage'] = np.maximum(18 - df['temperature_ext'], 0)
```

**Intuition:**

- √Ä 20¬∞C: degr√©-jours = 0 (pas de chauffage)
- √Ä 10¬∞C: degr√©-jours = 8 (chauffage mod√©r√©)
- √Ä -10¬∞C: degr√©-jours = 28 (chauffage intense)
- Relation **plus lin√©aire** avec la consommation que temp√©rature brute

**Ressentie (Wind Chill):**

```python
# Temp√©rature ressentie avec le vent
df['temp_ressentie'] = df['temperature_ext'] - 0.5 * df['vitesse_vent']
```

**Humidex (chaleur ressentie):**

```python
# Pour l'√©t√© (climatisation)
df['humidex'] = df['temperature_ext'] + 0.5555 * (6.11 * np.exp(5417.7530 * (1/273.16 - 1/(273.15 + df['temperature_ext']))) - 10)
```

### 5. Variables Temporelles Avanc√©es

**Indicateurs de p√©riodes sp√©cifiques:**

```python
# Heures de pointe matin/soir
df['est_pointe_matin'] = ((df['heure'] >= 7) & (df['heure'] <= 9)).astype(int)
df['est_pointe_soir'] = ((df['heure'] >= 17) & (df['heure'] <= 20)).astype(int)

# Saison
df['est_hiver'] = df['mois'].isin([12, 1, 2]).astype(int)
df['est_ete'] = df['mois'].isin([6, 7, 8]).astype(int)
```

**Distance au jour f√©ri√©:**

```python
# Comportement change quelques jours avant/apr√®s les f√™tes
# (N√©cessite une liste de dates de f√™tes)
```

## üíª Impl√©mentation Python Compl√®te

```python
import numpy as np
import pandas as pd

def creer_caracteristiques(df):
    """
    Cr√©e des caract√©ristiques suppl√©mentaires pour am√©liorer la pr√©diction.

    IMPORTANT pour l'entrevue:
    - Expliquer POURQUOI chaque feature est utile
    - Comprendre quand utiliser lags (attention data leakage!)
    - Savoir interpr√©ter les interactions

    Param√®tres:
        df : DataFrame avec colonnes de base (temp√©rature, heure, etc.)

    Retourne:
        df : DataFrame enrichi avec nouvelles features
    """
    df = df.copy()

    # ============================================
    # 1. RETARDS (LAGS) - Autocorr√©lation
    # ============================================

    # Lag 1: Consommation il y a 1 heure
    df['energie_lag1'] = df['energie_kwh'].shift(1)

    # Lag 24: M√™me heure hier (forte corr√©lation)
    df['energie_lag24'] = df['energie_kwh'].shift(24)

    # Lag 168: M√™me heure, m√™me jour la semaine derni√®re
    df['energie_lag168'] = df['energie_kwh'].shift(168)

    # Points pour l'entrevue:
    # - Pourquoi lag24? Consommation √† 8h aujourd'hui ‚âà 8h hier
    # - Pourquoi lag168? Lundi 8h ‚âà lundi pr√©c√©dent 8h
    # - Attention: Ces features cr√©ent des NaN au d√©but!


    # ============================================
    # 2. STATISTIQUES GLISSANTES (ROLLING)
    # ============================================

    # Moyenne mobile 6h: Tendance court terme
    df['energie_rolling_mean_6h'] = df['energie_kwh'].rolling(
        window=6,
        min_periods=1  # √âvite NaN si < 6 valeurs
    ).mean()

    # Moyenne mobile 24h: Tendance journali√®re
    df['energie_rolling_mean_24h'] = df['energie_kwh'].rolling(
        window=24,
        min_periods=1
    ).mean()

    # √âcart-type mobile 24h: Mesure de volatilit√©
    df['energie_rolling_std_24h'] = df['energie_kwh'].rolling(
        window=24,
        min_periods=1
    ).std().fillna(0)  # Remplacer NaN par 0

    # Max sur 12h: D√©tecte pointes r√©centes
    df['energie_rolling_max_12h'] = df['energie_kwh'].rolling(
        window=12,
        min_periods=1
    ).max()

    # Points pour l'entrevue:
    # - Moyenne lisse le bruit, capture tendance
    # - Std mesure variabilit√© (stable vs instable)
    # - Max d√©tecte si on sort d'une p√©riode de pointe


    # ============================================
    # 3. INTERACTIONS M√âT√âO √ó TEMPS
    # ============================================

    # Temp√©rature √ó heure_cos: Capture "froid la nuit"
    df['temp_heure_cos'] = df['temperature_ext'] * df['heure_cos']

    # Temp√©rature √ó heure_sin
    df['temp_heure_sin'] = df['temperature_ext'] * df['heure_sin']

    # Temp√©rature √ó weekend: Comportement diff√©rent
    df['temp_weekend'] = df['temperature_ext'] * df['est_weekend']

    # Temp√©rature √ó mois: Capture saisonnalit√©
    df['temp_mois_sin'] = df['temperature_ext'] * df['mois_sin']
    df['temp_mois_cos'] = df['temperature_ext'] * df['mois_cos']

    # Points pour l'entrevue:
    # - Pourquoi interaction? Effet de temp√©rature d√©pend de l'heure!
    # - Exemple: -10¬∞C √† 3h du matin ‚Üí tr√®s haute consommation (chauffage)
    #            -10¬∞C √† 14h ‚Üí moins √©lev√©e (soleil, activit√©)


    # ============================================
    # 4. TRANSFORMATIONS M√âT√âO
    # ============================================

    # Degr√©-jours de chauffage (seuil 18¬∞C)
    df['degres_jours_chauffage'] = np.maximum(18 - df['temperature_ext'], 0)

    # Degr√©-jours de climatisation (seuil 22¬∞C)
    df['degres_jours_clim'] = np.maximum(df['temperature_ext'] - 22, 0)

    # Temp√©rature au carr√© (non-lin√©arit√©)
    df['temp_squared'] = df['temperature_ext'] ** 2

    # Temp√©rature ressentie avec vent (wind chill simplifi√©)
    df['temp_ressentie'] = df['temperature_ext'] - 0.5 * df['vitesse_vent']

    # Humidit√© relative ajust√©e
    df['humidite_temp'] = df['humidite'] * np.abs(df['temperature_ext']) / 100

    # Points pour l'entrevue:
    # - Degr√©-jours: relation plus lin√©aire avec consommation
    # - Temp¬≤: capture acc√©l√©ration de consommation aux extr√™mes
    # - Ressentie: le vent augmente la sensation de froid


    # ============================================
    # 5. VARIABLES TEMPORELLES AVANC√âES
    # ============================================

    # Indicateur heures de pointe matin
    df['est_pointe_matin'] = ((df['heure'] >= 7) & (df['heure'] <= 9)).astype(int)

    # Indicateur heures de pointe soir
    df['est_pointe_soir'] = ((df['heure'] >= 17) & (df['heure'] <= 20)).astype(int)

    # Nuit (consommation basse)
    df['est_nuit'] = ((df['heure'] >= 0) & (df['heure'] <= 6)).astype(int)

    # Hiver (haute consommation)
    df['est_hiver'] = df['mois'].isin([12, 1, 2]).astype(int)

    # √ât√© (basse consommation, climatisation possible)
    df['est_ete'] = df['mois'].isin([6, 7, 8]).astype(int)

    # Points pour l'entrevue:
    # - Binning temporel: simplifie les patterns
    # - Capture des "r√©gimes" diff√©rents


    # ============================================
    # 6. STATISTIQUES M√âT√âO GLISSANTES
    # ============================================

    # Temp√©rature moyenne des 3 derni√®res heures
    df['temp_rolling_mean_3h'] = df['temperature_ext'].rolling(
        window=3,
        min_periods=1
    ).mean()

    # Changement de temp√©rature (gradient)
    df['temp_diff'] = df['temperature_ext'].diff().fillna(0)

    # Temp√©rature min/max sur 24h (amplitude thermique)
    df['temp_amplitude_24h'] = (
        df['temperature_ext'].rolling(window=24, min_periods=1).max() -
        df['temperature_ext'].rolling(window=24, min_periods=1).min()
    )

    # Points pour l'entrevue:
    # - Gradient temp√©rature: chute rapide ‚Üí plus de chauffage
    # - Amplitude: grande variation ‚Üí plus √©nergivore


    # ============================================
    # 7. NOMBRE DE CLIENTS (TR√àS IMPORTANT!)
    # ============================================

    # Si la colonne existe, cr√©er des interactions
    if 'clients_connectes' in df.columns:
        # Clients √ó temp√©rature
        df['clients_temp'] = df['clients_connectes'] * df['temperature_ext']

        # Consommation par client (normalis√©e)
        df['energie_per_client'] = df['energie_kwh'] / (df['clients_connectes'] + 1)

        # Clients √ó weekend
        df['clients_weekend'] = df['clients_connectes'] * df['est_weekend']

    # Points pour l'entrevue:
    # - clients_connectes est LA variable la plus pr√©dictive!
    # - Plus de clients ‚Üí plus de consommation (quasi lin√©aire)
    # - Interactions capturent comportements par client


    # ============================================
    # NETTOYAGE FINAL
    # ============================================

    # IMPORTANT: Les lags et rolling cr√©ent des NaN au d√©but
    # Options:
    # 1. Supprimer les lignes avec NaN: df.dropna()
    # 2. Remplir avec 0 ou moyenne: df.fillna(0) ou df.fillna(df.mean())
    # 3. Forward fill: df.fillna(method='ffill')

    # Pour ce projet, on va supprimer (plus s√ªr)
    # Note: On perd les premi√®res heures de train, mais c'est OK

    return df


# ============================================
# EXEMPLE D'UTILISATION
# ============================================

# Application aux donn√©es
train_enrichi = creer_caracteristiques(train)
test_enrichi = creer_caracteristiques(test)

# Supprimer les NaN (dus aux lags/rolling)
train_enrichi = train_enrichi.dropna()
test_enrichi = test_enrichi.dropna()

# V√©rifier les nouvelles colonnes
nouvelles_cols = [c for c in train_enrichi.columns if c not in train.columns]
print(f"Nombre de nouvelles features: {len(nouvelles_cols)}")
print(f"\nNouvelles features cr√©√©es:")
for col in nouvelles_cols:
    print(f"  - {col}")

# V√©rifier corr√©lations avec la cible
correlations = train_enrichi[nouvelles_cols + ['energie_kwh']].corr()['energie_kwh'].sort_values(ascending=False)
print(f"\nTop 10 features par corr√©lation avec energie_kwh:")
print(correlations.head(10))


# ============================================
# S√âLECTION DES FEATURES POUR LE MOD√àLE
# ============================================

# OPTION 1: Prendre toutes les features num√©riques
features_to_use = [col for col in train_enrichi.columns
                   if col not in ['energie_kwh', 'horodatage_local', 'evenement_pointe']]

# OPTION 2: S√©lection manuelle (recommand√© pour l'entrevue)
features_to_use = [
    # M√©t√©o de base
    'temperature_ext', 'humidite', 'vitesse_vent', 'irradiance_solaire',

    # Temps cyclique
    'heure_sin', 'heure_cos', 'mois_sin', 'mois_cos',
    'jour_semaine_sin', 'jour_semaine_cos',

    # Indicateurs binaires
    'est_weekend', 'est_ferie', 'est_pointe_matin', 'est_pointe_soir',

    # TR√àS IMPORTANT
    'clients_connectes',

    # Lags (attention Kaggle!)
    'energie_lag1', 'energie_lag24',

    # Rolling
    'energie_rolling_mean_6h', 'energie_rolling_mean_24h',

    # Interactions
    'temp_heure_cos', 'temp_weekend',

    # Transformations m√©t√©o
    'degres_jours_chauffage', 'temp_squared'
]

# Filtrer celles qui existent vraiment
features_disponibles = [f for f in features_to_use if f in train_enrichi.columns]

print(f"\nFeatures s√©lectionn√©es: {len(features_disponibles)}")

X_train = train_enrichi[features_disponibles].values
y_train = train_enrichi['energie_kwh'].values
X_test = test_enrichi[features_disponibles].values
y_test = test_enrichi['energie_kwh'].values

# Entra√Æner un mod√®le simple pour tester
from sklearn.linear_model import Ridge

model = Ridge(alpha=1.0)
model.fit(X_train, y_train)

from sklearn.metrics import r2_score
print(f"\nR¬≤ avec features enrichies: {r2_score(y_test, model.predict(X_test)):.4f}")
```

## ‚ö†Ô∏è Points Critiques pour l'Entrevue

### 1. Data Leakage avec les Lags

**Question type:** "Vous utilisez energie_lag1, mais comment faire sur Kaggle sans la vraie valeur?"

**R√©ponse:**
Deux approches:

**Option A: Ne pas utiliser de lags pour Kaggle**

```python
# Features pour train/test local (avec lags)
features_local = [..., 'energie_lag1', 'energie_lag24', ...]

# Features pour Kaggle (sans lags)
features_kaggle = [f for f in features_local if 'lag' not in f]
```

**Option B: Pr√©diction autor√©gressive**

```python
# Pr√©dire heure par heure en utilisant pr√©dictions pr√©c√©dentes
predictions = []
for t in range(len(test_kaggle)):
    # Utiliser la pr√©diction de t-1 comme lag pour pr√©dire t
    X[t, lag1_idx] = predictions[t-1] if t > 0 else last_train_value
    pred = model.predict(X[t])
    predictions.append(pred)
```

**Pour l'entrevue:** Reconna√Ætre le probl√®me montre que vous comprenez!

### 2. Pourquoi cr√©er des Interactions?

**Question type:** "Pourquoi multiplier temp√©rature et heure au lieu de les utiliser s√©par√©ment?"

**R√©ponse:**
Mod√®le lin√©aire **sans interaction:**
$$y = \beta_1 \cdot temp + \beta_2 \cdot heure$$
‚Üí L'effet de temp√©rature est **constant** quelle que soit l'heure

Mod√®le **avec interaction:**
$$y = \beta_1 \cdot temp + \beta_2 \cdot heure + \beta_3 \cdot (temp \times heure)$$
‚Üí L'effet de temp√©rature **d√©pend** de l'heure!

**Exemple concret:**

- Hiver, 3h du matin, -15¬∞C ‚Üí Chauffage maximal
- Hiver, 14h, -15¬∞C ‚Üí Chauffage moyen (soleil aide)
- L'interaction capture cette d√©pendance!

### 3. D√©calage Train/Test (Distribution Shift)

**Question type:** "Vos features aident-elles malgr√© le d√©calage hiver/√©t√©?"

**R√©ponse:**
Le probl√®me:

- Train = hiver (consommation √©lev√©e, chauffage)
- Test = √©t√© (consommation basse, climatisation)

**Features qui g√©n√©ralisent MAL:**

- Lags simples (valeurs absolues changent beaucoup)
- Moyennes mobiles (idem)

**Features qui g√©n√©ralisent BIEN:**

- `degres_jours_chauffage`: Relation physique stable
- Interactions m√©t√©o √ó temps: Patterns comportementaux persistants
- `clients_connectes`: Normalise la consommation
- Features cycliques (heure_sin/cos): Patterns horaires similaires

**Strat√©gie:** Privil√©gier features bas√©es sur **lois physiques** ou **comportements** plut√¥t que valeurs brutes

### 4. Importance Relative des Features

**Question type:** "Quelle feature a le plus am√©lior√© le mod√®le?"

**R√©ponse bas√©e sur l'exp√©rience typique:**

1. **`clients_connectes`** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (+30% R¬≤)
   - Plus de clients = plus de consommation (quasi lin√©aire)
2. **`degres_jours_chauffage`** ‚≠ê‚≠ê‚≠ê‚≠ê (+15% R¬≤)
   - Meilleure que temp√©rature brute
3. **Lags (lag24, lag168)** ‚≠ê‚≠ê‚≠ê (+10% R¬≤ local)
   - Forte autocorr√©lation, mais attention leakage Kaggle!
4. **Interactions m√©t√©o √ó temps** ‚≠ê‚≠ê (+5% R¬≤)
   - Capture effets conditionnels
5. **Rolling statistics** ‚≠ê (+2-3% R¬≤)
   - Lissent le bruit

**Pour l'entrevue:** Conna√Ætre l'ordre d'importance montre que vous avez test√©!

### 5. Combien de Features Cr√©er?

**Question type:** "Vous avez cr√©√© 30 features, est-ce trop?"

**R√©ponse nuanc√©e:**

**Avantages de beaucoup de features:**

- ‚úÖ Plus d'information pour le mod√®le
- ‚úÖ Ridge va automatiquement r√©duire les coefficients peu utiles

**Inconv√©nients:**

- ‚ùå Overfitting possible (m√™me avec Ridge)
- ‚ùå Temps de calcul
- ‚ùå Plus difficile √† interpr√©ter

**Bonne pratique:**

1. Cr√©er beaucoup de features (exploration)
2. Analyser les coefficients Ridge
3. Garder seulement les features importantes
4. **Pour l'entrevue:** Justifier chaque feature gard√©e!

**R√®gle empirique:** 10-20 features bien choisies > 100 features al√©atoires

## üéØ Checklist pour l'Entrevue Feature Engineering

- [ ] Expliquer 3 types de features cr√©√©es (lags, rolling, interactions)
- [ ] Justifier POURQUOI chaque feature est utile (pas juste "j'ai essay√©")
- [ ] Identifier le risque de data leakage avec les lags
- [ ] Expliquer pourquoi `degres_jours_chauffage` > `temperature_ext`
- [ ] Donner un exemple concret d'interaction (temp√©rature √ó heure)
- [ ] Expliquer comment g√©rer les NaN des lags/rolling
- [ ] Identifier quelle feature a le plus am√©lior√© R¬≤
- [ ] Expliquer strat√©gie pour g√©n√©raliser train (hiver) ‚Üí test (√©t√©)
- [ ] Dessiner graphique: corr√©lation features avec cible
- [ ] D√©fendre le nombre de features cr√©√©es

## üí° Conseils pour l'Entrevue

1. **Ne pas juste lister les features** ‚Üí Expliquer le RAISONNEMENT
   - ‚ùå "J'ai ajout√© energie_lag24"
   - ‚úÖ "J'ai ajout√© energie_lag24 car la consommation √† 8h aujourd'hui ressemble √† celle de 8h hier, ce qui capture les habitudes quotidiennes stables"

2. **Pr√©parer des graphiques** montrant l'impact
   - Courbe: R¬≤ avec 0, 5, 10, 20 features
   - Heatmap: Corr√©lations entre features

3. **Anticiper les questions sur le d√©calage distribution**
   - "Comment votre mod√®le peut-il pr√©dire l'√©t√© s'il n'a vu que l'hiver?"
   - R√©ponse: Features bas√©es sur comportements/physique, pas valeurs brutes

4. **Savoir quoi enlever si demand√©**
   - "Si vous deviez garder seulement 5 features, lesquelles?"
   - R√©ponse: clients_connectes, degres_jours_chauffage, heure_sin/cos, mois_sin/cos

---

## PARTIE 4: R√©gression Ridge - MA√éTRISER POUR L'ENTREVUE! ‚≠ê

### üéØ Le Probl√®me avec OLS

Apr√®s avoir cr√©√© plein de features (Partie 3), vous avez maintenant:

- **Beaucoup de variables** (20-30 features)
- **Certaines corr√©l√©es** entre elles (temp√©rature et degr√©-jours, lag1 et lag24)
- Risque d'**overfitting** (mod√®le trop complexe)

**Sympt√¥mes d'overfitting avec OLS:**

- R¬≤ train = 0.95, R¬≤ test = 0.60 (grand √©cart!)
- Coefficients tr√®s grands en valeur absolue (ex: Œ≤‚ÇÅ = +5000, Œ≤‚ÇÇ = -4998)
- Coefficients instables (changent beaucoup si on ajoute/retire 1 observation)

## üìê Th√©orie Math√©matique Compl√®te

### Rappel: OLS minimise uniquement l'erreur

$$\hat{\boldsymbol{\beta}}_{OLS} = \arg\min_{\boldsymbol{\beta}} \sum_{i=1}^{n} (y_i - \mathbf{x}_i^T\boldsymbol{\beta})^2$$

**Probl√®me:** Aucune contrainte sur la taille des coefficients!

### Ridge ajoute une P√©nalit√© L2

$$\hat{\boldsymbol{\beta}}_{Ridge} = \arg\min_{\boldsymbol{\beta}} \left[ \sum_{i=1}^{n} (y_i - \mathbf{x}_i^T\boldsymbol{\beta})^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right]$$

Ou en notation matricielle:

$$\hat{\boldsymbol{\beta}}_{Ridge} = \arg\min_{\boldsymbol{\beta}} \left[ \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda \|\boldsymbol{\beta}\|^2 \right]$$

**Composantes:**

- $\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2$ : **Erreur d'ajustement** (comme OLS)
- $\lambda \|\boldsymbol{\beta}\|^2$ : **P√©nalit√© de r√©gularisation** (nouveau!)
- $\lambda \geq 0$ : **Hyperparam√®tre** contr√¥lant l'√©quilibre

### Interpr√©tation du Param√®tre Œª

**Œª = 0:**

- Pas de p√©nalit√©
- Ridge = OLS exactement

**Œª tr√®s petit (ex: 0.01):**

- P√©nalit√© faible
- Ridge ‚âà OLS (peu de r√©gularisation)

**Œª mod√©r√© (ex: 1-100):**

- √âquilibre entre ajustement et simplicit√©
- **Zone optimale g√©n√©ralement**

**Œª tr√®s grand (ex: 10000):**

- P√©nalit√© dominante
- Tous les coefficients ‚Üí 0
- Mod√®le ‚Üí pr√©diction par la moyenne

### Solution Analytique

**D√©rivation (important pour l'entrevue!):**

Fonction objectif:
$$L(\boldsymbol{\beta}) = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} - \mathbf{X}\boldsymbol{\beta}) + \lambda \boldsymbol{\beta}^T\boldsymbol{\beta}$$

D√©velopper (comme pour OLS):
$$L(\boldsymbol{\beta}) = \mathbf{y}^T\mathbf{y} - 2\boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y} + \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} + \lambda \boldsymbol{\beta}^T\boldsymbol{\beta}$$

Gradient:
$$\nabla_{\boldsymbol{\beta}} L = -2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} + 2\lambda\boldsymbol{\beta}$$

√âgaler √† z√©ro:
$$-2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} + 2\lambda\boldsymbol{\beta} = 0$$

$$\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} + \lambda\boldsymbol{\beta} = \mathbf{X}^T\mathbf{y}$$

$$(\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I})\boldsymbol{\beta} = \mathbf{X}^T\mathbf{y}$$

**Solution Ridge:**

$$\boxed{\hat{\boldsymbol{\beta}}_{Ridge} = (\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}}$$

**Comparer avec OLS:**

- OLS: $(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$
- Ridge: $(\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}$
- **Diff√©rence:** $+ \lambda \mathbf{I}$ sur la diagonal!

### Avantage: Garantit l'Inversibilit√©

**Probl√®me avec OLS:**
Si $\mathbf{X}^T\mathbf{X}$ n'est pas inversible (multicolin√©arit√© parfaite):

- Pas de solution unique
- `np.linalg.solve` √©choue ou donne r√©sultat instable

**Solution Ridge:**
$\mathbf{X}^T\mathbf{X} + \lambda \mathbf{I}$ est **toujours inversible** si $\lambda > 0$!

- Ajouter $\lambda$ sur la diagonale "stabilise" la matrice
- Solution existe toujours et est unique

## üîó Interpr√©tation Bay√©sienne (Estimation MAP)

**Question cl√© d'entrevue:** "Quel est le lien entre Ridge et l'estimation MAP?"

### Rappel: Maximum A Posteriori (MAP)

En statistique bay√©sienne, on cherche:
$$\hat{\boldsymbol{\beta}}_{MAP} = \arg\max_{\boldsymbol{\beta}} P(\boldsymbol{\beta} | \mathbf{y}, \mathbf{X})$$

Par Bayes:
$$P(\boldsymbol{\beta} | \mathbf{y}, \mathbf{X}) = \frac{P(\mathbf{y} | \mathbf{X}, \boldsymbol{\beta}) \cdot P(\boldsymbol{\beta})}{P(\mathbf{y})}$$

Log-vraisemblance:
$$\log P(\boldsymbol{\beta} | \mathbf{y}, \mathbf{X}) = \log P(\mathbf{y} | \mathbf{X}, \boldsymbol{\beta}) + \log P(\boldsymbol{\beta}) + \text{cste}$$

### Hypoth√®ses Bay√©siennes

**Vraisemblance (likelihood):**
Erreurs gaussiennes: $y_i = \mathbf{x}_i^T\boldsymbol{\beta} + \epsilon_i$, o√π $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$

$$P(\mathbf{y} | \mathbf{X}, \boldsymbol{\beta}) \propto \exp\left(-\frac{1}{2\sigma^2}\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2\right)$$

**Prior (a priori):**
Distribution gaussienne centr√©e: $\boldsymbol{\beta} \sim \mathcal{N}(0, \tau^2\mathbf{I})$

$$P(\boldsymbol{\beta}) \propto \exp\left(-\frac{1}{2\tau^2}\|\boldsymbol{\beta}\|^2\right)$$

**Signification:** On croit a priori que les coefficients sont petits (proches de 0)

### D√©rivation MAP = Ridge

$$\log P(\boldsymbol{\beta} | \mathbf{y}, \mathbf{X}) \propto -\frac{1}{2\sigma^2}\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 - \frac{1}{2\tau^2}\|\boldsymbol{\beta}\|^2$$

Maximiser log-posterior = Minimiser son oppos√©:

$$\arg\min_{\boldsymbol{\beta}} \left[\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \frac{\sigma^2}{\tau^2}\|\boldsymbol{\beta}\|^2\right]$$

En posant $\lambda = \frac{\sigma^2}{\tau^2}$:

$$\boxed{\hat{\boldsymbol{\beta}}_{MAP} = \hat{\boldsymbol{\beta}}_{Ridge}}$$

**Interpr√©tation Ridge Bay√©sienne:**

- Ridge = MAP avec prior gaussien centr√©
- $\lambda$ grand ‚Üí prior fort (on croit fort que Œ≤ ‚âà 0)
- $\lambda$ petit ‚Üí prior faible (on fait plus confiance aux donn√©es)

**Pour l'entrevue:** Ridge n'est pas juste une "astuce", c'est une estimation bay√©sienne rigoureuse!

## üìä Effet sur les Coefficients

### R√©duction (Shrinkage) des Coefficients

**Propri√©t√© fondamentale:** Ridge **r√©duit** tous les coefficients vers 0, mais ne les met jamais exactement √† 0

**Visualisation:**

```
Coefficient OLS:  |--------‚óè-----------------|  Œ≤ = 100
Coefficient Ridge:|-----‚óè--------------------|  Œ≤ = 60
                  0                        150

Œª = 0    ‚Üí Œ≤ = 100 (OLS)
Œª = 1    ‚Üí Œ≤ = 60
Œª = 10   ‚Üí Œ≤ = 25
Œª = 100  ‚Üí Œ≤ = 5
Œª = ‚àû    ‚Üí Œ≤ = 0
```

### Comparaison OLS vs Ridge

| Feature         | Coefficient OLS | Coefficient Ridge (Œª=10) | R√©duction |
| --------------- | --------------- | ------------------------ | --------- |
| temperature_ext | -8.5            | -6.2                     | 27%       |
| energie_lag1    | 0.85            | 0.55                     | 35%       |
| energie_lag24   | 0.78            | 0.52                     | 33%       |
| temp_heure_cos  | 12.3            | 3.1                      | 75% ‚¨áÔ∏è    |
| vitesse_vent    | -0.3            | -0.2                     | 33%       |

**Observation:** Ridge r√©duit **surtout** les coefficients des features:

- Corr√©l√©es avec d'autres (lag1 et lag24)
- Moins importantes (vitesse_vent)
- Instables (interactions)

### Biais-Variance Tradeoff

**OLS (Œª = 0):**

- ‚úÖ Pas de biais (estimateur non biais√©)
- ‚ùå Haute variance (coefficients instables)
- R√©sultat: **Overfitting** possible

**Ridge (Œª > 0):**

- ‚ùå L√©g√®rement biais√© (coefficients r√©duits)
- ‚úÖ Basse variance (coefficients stables)
- R√©sultat: **Meilleure g√©n√©ralisation** sur test!

**Formule math√©matique:**
$$\text{Erreur totale} = \text{Biais}^2 + \text{Variance} + \text{Bruit irr√©ductible}$$

Ridge augmente l√©g√®rement le biais, mais **r√©duit beaucoup** la variance ‚Üí **erreur totale plus faible**!

## üîç Choix de Œª par Validation Crois√©e

### Probl√®me

Comment choisir Œª optimal? Tester manuellement?

**Mauvaise approche:**

```python
# ‚ùå NE PAS FAIRE
model = Ridge(alpha=1.0)  # Pourquoi 1.0? Au hasard?
```

**Bonne approche:** Validation crois√©e pour s√©lectionner automatiquement!

### Time Series Cross-Validation

**ATTENTION:** Pour s√©ries temporelles, **PAS** de validation crois√©e al√©atoire!

**Mauvais (K-Fold classique):**

```
Train: [‚ñà‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñà]
Test:  [  ‚ñà‚ñà‚ñà‚ñà  ‚ñà‚ñà‚ñà‚ñà  ]
```

‚Üí **Fuite d'information:** On utilise le futur pour pr√©dire le pass√©!

**Bon (TimeSeriesSplit):**

```
Fold 1: Train [‚ñà‚ñà‚ñà‚ñà]           Test [‚ñà]
Fold 2: Train [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]       Test [‚ñà]
Fold 3: Train [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]   Test [‚ñà]
Fold 4: Train [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] Test [‚ñà]
```

‚Üí **Respecte la chronologie:** Toujours pr√©dire le futur avec le pass√©

### Impl√©mentation avec RidgeCV

```python
from sklearn.linear_model import RidgeCV
from sklearn.model_selection import TimeSeriesSplit

# Valeurs de Œª √† tester (√©chelle logarithmique)
alphas = [0.01, 0.1, 1, 10, 100, 1000]

# Validation crois√©e temporelle
tscv = TimeSeriesSplit(n_splits=5)

# RidgeCV teste tous les alphas et s√©lectionne le meilleur
model_ridge = RidgeCV(alphas=alphas, cv=tscv)
model_ridge.fit(X_train, y_train)

# Meilleur Œª trouv√©
print(f"Œª optimal: {model_ridge.alpha_}")
```

**Points pour l'entrevue:**

1. Pourquoi √©chelle logarithmique? Œª varie sur plusieurs ordres de grandeur (0.01 ‚Üí 1000)
2. Pourquoi TimeSeriesSplit? Respect chronologie des donn√©es
3. Comment RidgeCV choisit? Minimise erreur de validation crois√©e

## üíª Impl√©mentation Python Compl√®te

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression, Ridge, RidgeCV
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt

# ============================================
# DONN√âES (apr√®s feature engineering)
# ============================================

# Supposons qu'on a 25 features apr√®s Partie 3
# X_train: (8000, 25)
# y_train: (8000,)
# X_test: (2000, 25)
# y_test: (2000,)

features_disponibles = [
    'temperature_ext', 'humidite', 'vitesse_vent',
    'heure_sin', 'heure_cos', 'mois_sin', 'mois_cos',
    'est_weekend', 'clients_connectes',
    'energie_lag1', 'energie_lag24',
    'energie_rolling_mean_6h', 'degres_jours_chauffage',
    'temp_heure_cos', 'temp_weekend',
    # ... autres features
]

X_train = train_eng[features_disponibles].values
y_train = train_eng['energie_kwh'].values
X_test = test_eng[features_disponibles].values
y_test = test_eng['energie_kwh'].values

# ============================================
# BASELINE: OLS
# ============================================

print("=" * 60)
print("BASELINE: OLS (Ordinary Least Squares)")
print("=" * 60)

model_ols = LinearRegression()
model_ols.fit(X_train, y_train)

y_pred_ols_train = model_ols.predict(X_train)
y_pred_ols_test = model_ols.predict(X_test)

r2_ols_train = r2_score(y_train, y_pred_ols_train)
r2_ols_test = r2_score(y_test, y_pred_ols_test)
rmse_ols_test = np.sqrt(mean_squared_error(y_test, y_pred_ols_test))

print(f"R¬≤ train: {r2_ols_train:.4f}")
print(f"R¬≤ test:  {r2_ols_test:.4f}")
print(f"RMSE test: {rmse_ols_test:.2f} kWh")
print(f"√âcart train-test: {abs(r2_ols_train - r2_ols_test):.4f}")

# Diagnostique overfitting
if r2_ols_train - r2_ols_test > 0.1:
    print("‚ö†Ô∏è  OVERFITTING d√©tect√©! (√©cart train-test > 0.1)")


# ============================================
# RIDGE AVEC Œª FIXE
# ============================================

print("\n" + "=" * 60)
print("RIDGE avec Œª = 1.0")
print("=" * 60)

model_ridge_fixed = Ridge(alpha=1.0)
model_ridge_fixed.fit(X_train, y_train)

y_pred_ridge_train = model_ridge_fixed.predict(X_train)
y_pred_ridge_test = model_ridge_fixed.predict(X_test)

r2_ridge_train = r2_score(y_train, y_pred_ridge_train)
r2_ridge_test = r2_score(y_test, y_pred_ridge_test)
rmse_ridge_test = np.sqrt(mean_squared_error(y_test, y_pred_ridge_test))

print(f"R¬≤ train: {r2_ridge_train:.4f}")
print(f"R¬≤ test:  {r2_ridge_test:.4f}")
print(f"RMSE test: {rmse_ridge_test:.2f} kWh")
print(f"√âcart train-test: {abs(r2_ridge_train - r2_ridge_test):.4f}")


# ============================================
# RIDGE avec VALIDATION CROIS√âE (OPTIMAL)
# ============================================

print("\n" + "=" * 60)
print("RIDGE avec RidgeCV (s√©lection automatique de Œª)")
print("=" * 60)

# Valeurs de Œª √† tester (√©chelle log)
alphas = [0.01, 0.1, 1, 10, 100, 1000]

# Time Series Cross-Validation (CRUCIAL pour s√©ries temporelles!)
tscv = TimeSeriesSplit(n_splits=5)

# RidgeCV teste tous les alphas
model_ridge_cv = RidgeCV(alphas=alphas, cv=tscv)
model_ridge_cv.fit(X_train, y_train)

print(f"Œª optimal trouv√©: {model_ridge_cv.alpha_}")

y_pred_ridgecv_train = model_ridge_cv.predict(X_train)
y_pred_ridgecv_test = model_ridge_cv.predict(X_test)

r2_ridgecv_train = r2_score(y_train, y_pred_ridgecv_train)
r2_ridgecv_test = r2_score(y_test, y_pred_ridgecv_test)
rmse_ridgecv_test = np.sqrt(mean_squared_error(y_test, y_pred_ridgecv_test))

print(f"R¬≤ train: {r2_ridgecv_train:.4f}")
print(f"R¬≤ test:  {r2_ridgecv_test:.4f}")
print(f"RMSE test: {rmse_ridgecv_test:.2f} kWh")
print(f"√âcart train-test: {abs(r2_ridgecv_train - r2_ridgecv_test):.4f}")


# ============================================
# R√âCAPITULATIF COMPARATIF
# ============================================

print("\n" + "=" * 60)
print("R√âCAPITULATIF")
print("=" * 60)

results = pd.DataFrame({
    'Mod√®le': ['OLS', 'Ridge (Œª=1)', f'Ridge (Œª={model_ridge_cv.alpha_})'],
    'R¬≤ train': [r2_ols_train, r2_ridge_train, r2_ridgecv_train],
    'R¬≤ test': [r2_ols_test, r2_ridge_test, r2_ridgecv_test],
    'RMSE test': [rmse_ols_test, rmse_ridge_test, rmse_ridgecv_test],
    '√âcart': [abs(r2_ols_train - r2_ols_test),
              abs(r2_ridge_train - r2_ridge_test),
              abs(r2_ridgecv_train - r2_ridgecv_test)]
})

print(results.to_string(index=False))

# Meilleur mod√®le
best_idx = results['R¬≤ test'].idxmax()
print(f"\nüèÜ Meilleur mod√®le: {results.loc[best_idx, 'Mod√®le']}")


# ============================================
# ANALYSE DES COEFFICIENTS
# ============================================

print("\n" + "=" * 60)
print("COMPARAISON DES COEFFICIENTS OLS vs RIDGE")
print("=" * 60)

# Comparer coefficients
coef_comparison = pd.DataFrame({
    'Feature': features_disponibles,
    'OLS': model_ols.coef_,
    'Ridge': model_ridge_cv.coef_
})

# Calculer r√©duction (shrinkage)
coef_comparison['R√©duction (%)'] = 100 * (
    1 - np.abs(coef_comparison['Ridge']) / (np.abs(coef_comparison['OLS']) + 1e-8)
)

# Trier par r√©duction
coef_comparison = coef_comparison.sort_values('R√©duction (%)', ascending=False)

print(coef_comparison.to_string(index=False))

print("\nüìä Observations:")
print(f"  - R√©duction moyenne: {coef_comparison['R√©duction (%)'].mean():.1f}%")
print(f"  - R√©duction max: {coef_comparison['R√©duction (%)'].max():.1f}%")
print(f"  - Feature la plus r√©duite: {coef_comparison.iloc[0]['Feature']}")


# ============================================
# VISUALISATION: CHEMIN DE R√âGULARISATION
# ============================================

print("\n" + "=" * 60)
print("VISUALISATION: Effet de Œª sur les coefficients")
print("=" * 60)

# Tester plusieurs Œª
lambdas_test = np.logspace(-2, 4, 50)  # 0.01 √† 10000
coefficients_path = []

for lam in lambdas_test:
    model_temp = Ridge(alpha=lam)
    model_temp.fit(X_train, y_train)
    coefficients_path.append(model_temp.coef_)

coefficients_path = np.array(coefficients_path)

# Tracer
plt.figure(figsize=(12, 6))
for i, feature in enumerate(features_disponibles[:10]):  # 10 premi√®res features
    plt.plot(lambdas_test, coefficients_path[:, i], label=feature, linewidth=2)

plt.xscale('log')
plt.xlabel('Œª (√©chelle log)', fontsize=12)
plt.ylabel('Coefficient', fontsize=12)
plt.title('Chemin de R√©gularisation Ridge', fontsize=14, fontweight='bold')
plt.axvline(model_ridge_cv.alpha_, color='red', linestyle='--',
            linewidth=2, label=f'Œª optimal = {model_ridge_cv.alpha_}')
plt.grid(True, alpha=0.3)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
plt.tight_layout()
plt.show()

# Points pour l'entrevue:
# - Tous les coefficients ‚Üí 0 quand Œª ‚Üí ‚àû
# - Mais jamais exactement 0 (diff√©rence avec Lasso)
# - Coefficients instables (corr√©l√©s) r√©duits plus vite


# ============================================
# COURBE DE VALIDATION
# ============================================

print("\n" + "=" * 60)
print("COURBE DE VALIDATION: R¬≤ vs Œª")
print("=" * 60)

r2_train_list = []
r2_test_list = []

for lam in lambdas_test:
    model_temp = Ridge(alpha=lam)
    model_temp.fit(X_train, y_train)

    r2_train_list.append(r2_score(y_train, model_temp.predict(X_train)))
    r2_test_list.append(r2_score(y_test, model_temp.predict(X_test)))

plt.figure(figsize=(10, 6))
plt.plot(lambdas_test, r2_train_list, label='R¬≤ train', linewidth=2, color='blue')
plt.plot(lambdas_test, r2_test_list, label='R¬≤ test', linewidth=2, color='orange')
plt.axvline(model_ridge_cv.alpha_, color='red', linestyle='--',
            linewidth=2, label=f'Œª optimal = {model_ridge_cv.alpha_}')

plt.xscale('log')
plt.xlabel('Œª (√©chelle log)', fontsize=12)
plt.ylabel('R¬≤', fontsize=12)
plt.title('Courbe de Validation Ridge', fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.legend(fontsize=11)
plt.tight_layout()
plt.show()

# Interpr√©tation pour l'entrevue:
# - Œª petit ‚Üí R¬≤ train √©lev√©, R¬≤ test bas (overfitting)
# - Œª optimal ‚Üí Meilleur compromis
# - Œª grand ‚Üí R¬≤ train et test bas (underfitting)
```

## ‚ö†Ô∏è Points Critiques pour l'Entrevue

### 1. Quelle est la diff√©rence entre Ridge et OLS?

**R√©ponse structur√©e:**

**Math√©matiquement:**

- OLS: $\min \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2$
- Ridge: $\min \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda\|\boldsymbol{\beta}\|^2$

**Conceptuellement:**

- OLS: Minimise uniquement l'erreur
- Ridge: Minimise erreur + complexit√© (taille des coefficients)

**En pratique:**

- OLS: Peut overfitter avec beaucoup de features
- Ridge: R√©duit overfitting en "shrinking" coefficients

### 2. Pourquoi Ridge plut√¥t que Lasso?

**Question pi√®ge!** Conna√Ætre les diff√©rences:

| Aspect                 | Ridge (L2)                  | Lasso (L1)                  |
| ---------------------- | --------------------------- | --------------------------- |
| **P√©nalit√©**           | $\lambda \sum \beta_j^2$    | $\lambda \sum \|\beta_j\|$  |
| **Coefficients**       | R√©duits vers 0              | Certains **exactement** 0   |
| **S√©lection**          | Non                         | Oui (s√©lection automatique) |
| **Features corr√©l√©es** | Garde toutes, partage poids | Choisit arbitrairement 1    |
| **Solution**           | Analytique                  | It√©rative (pas de formule)  |

**Pour ce projet:**

- Ridge pr√©f√©rable car on veut **garder toutes les features** (m√™me corr√©l√©es)
- Lasso √©liminerait arbitrairement lag1 ou lag24

### 3. Expliquez Ridge comme estimation MAP

**R√©ponse compl√®te:**

Ridge = Maximum A Posteriori avec prior gaussien:

**Prior:** $\boldsymbol{\beta} \sim \mathcal{N}(0, \tau^2\mathbf{I})$
‚Üí "Je crois a priori que les coefficients sont petits"

**Posterior:** $P(\boldsymbol{\beta}|\mathbf{y}) \propto P(\mathbf{y}|\boldsymbol{\beta}) \cdot P(\boldsymbol{\beta})$

**MAP:** $\arg\max P(\boldsymbol{\beta}|\mathbf{y}) = \arg\min [-\log P(\mathbf{y}|\boldsymbol{\beta}) - \log P(\boldsymbol{\beta})]$

Ce qui donne exactement la formule Ridge avec $\lambda = \frac{\sigma^2}{\tau^2}$

**Interpr√©tation Œª:**

- Œª grand ‚Üí Prior fort (on croit vraiment que Œ≤ ‚âà 0)
- Œª petit ‚Üí Prior faible (on fait confiance aux donn√©es)

### 4. Pourquoi utiliser TimeSeriesSplit?

**Question type:** "Pourquoi pas KFold classique pour choisir Œª?"

**R√©ponse:**

**Probl√®me avec KFold al√©atoire:**

```
Train: [Jan Feb ‚ñà Apr ‚ñà Jun Jul ‚ñà]
Test:  [‚ñà ‚ñà Mar ‚ñà May ‚ñà ‚ñà Aug]
```

‚Üí On utilise le futur (Juin) pour pr√©dire le pass√© (Mars)!
‚Üí **Fuite d'information temporelle** ‚Üí Œª sous-optimal

**TimeSeriesSplit respecte chronologie:**

```
Fold 1: Train [Jan Feb Mar] Test [Apr]
Fold 2: Train [Jan Feb Mar Apr May] Test [Jun]
Fold 3: Train [Jan Feb Mar Apr May Jun Jul] Test [Aug]
```

‚Üí Toujours: Entra√Ænement sur pass√©, test sur futur ‚úÖ

### 5. Comment interpr√©ter la r√©duction des coefficients?

**Question type:** "Le coefficient de temp_heure_cos a √©t√© r√©duit de 75%, qu'est-ce que √ßa signifie?"

**R√©ponse:**

**Coefficient r√©duit beaucoup (>50%) ‚Üí** Feature probablement:

1. Corr√©l√©e avec d'autres (redondance)
2. Peu importante pour la pr√©diction
3. Instable (varie beaucoup selon √©chantillon)

**Coefficient r√©duit peu (<20%) ‚Üí** Feature probablement:

1. Importante et unique
2. Stable
3. Peu corr√©l√©e avec d'autres

**Exemple concret:**

```
energie_lag1:     OLS = 0.85, Ridge = 0.55 (35% r√©duction)
energie_lag24:    OLS = 0.78, Ridge = 0.52 (33% r√©duction)
```

‚Üí lag1 et lag24 sont tr√®s corr√©l√©s ‚Üí Ridge les r√©duit tous deux pour √©viter redondance

```
clients_connectes: OLS = 12.3, Ridge = 11.8 (4% r√©duction)
```

‚Üí Variable tr√®s importante et peu corr√©l√©e ‚Üí Ridge la garde presque intacte

## üéØ Checklist pour l'Entrevue Ridge

- [ ] D√©river la solution Ridge: $(\mathbf{X}^T\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}$
- [ ] Expliquer le r√¥le de Œª (avec 3 cas: 0, mod√©r√©, ‚àû)
- [ ] Montrer lien Ridge = MAP avec prior gaussien
- [ ] Diff√©rencier Ridge vs Lasso (tableau comparatif)
- [ ] Justifier TimeSeriesSplit pour s√©ries temporelles
- [ ] Interpr√©ter courbe validation (R¬≤ vs Œª)
- [ ] Expliquer biais-variance tradeoff
- [ ] Analyser quelles features sont le plus r√©duites et pourquoi
- [ ] Tracer chemin de r√©gularisation (coefficients vs Œª)
- [ ] Expliquer pourquoi Ridge garantit l'inversibilit√©

---

## PARTIE 5: Mod√®le √† 2 √âtages (Classification ‚Üí R√©gression) ‚≠ê

### üéØ L'Id√©e Centrale

**Observation:** Pas toutes les heures sont √©quivalentes!

- **Heures de pointe:** Consommation tr√®s √©lev√©e, patterns diff√©rents
- **Heures normales:** Consommation moyenne, plus pr√©visible

**Strat√©gie:**

1. **√âtage 1 (Classification):** Pr√©dire si l'heure sera en "pointe" (0/1)
2. **√âtage 2 (R√©gression):** Utiliser $P(\text{pointe})$ comme **feature suppl√©mentaire**

## üìê Pourquoi Utiliser des Probabilit√©s?

### Option 1: Indicateur Binaire (0/1)

```python
# Pr√©dire classe binaire
classe_pred = clf.predict(X)  # [0, 0, 1, 0, 1, ...]
```

**Probl√®me:**

- Perte d'information!
- $P = 0.51$ ‚Üí classe 1
- $P = 0.99$ ‚Üí classe 1
- **Mais la confiance est tr√®s diff√©rente!**

### Option 2: Probabilit√© Continue ([0, 1])

```python
# Pr√©dire probabilit√©
proba_pred = clf.predict_proba(X)[:, 1]  # [0.05, 0.23, 0.87, 0.12, 0.98, ...]
```

**Avantages:**

- ‚úÖ Information nuanc√©e (certitude vs incertitude)
- ‚úÖ Variable continue ‚Üí Ridge peut l'utiliser lin√©airement
- ‚úÖ $P(\text{pointe})$ √©lev√©e ‚Üí mod√®le sait que consommation sera haute

**Exemple concret:**

```
Heure 1: P(pointe) = 0.05 ‚Üí Probablement normal ‚Üí Pr√©dire ~80 kWh
Heure 2: P(pointe) = 0.50 ‚Üí Incertain ‚Üí Pr√©dire ~120 kWh
Heure 3: P(pointe) = 0.95 ‚Üí Presque s√ªr pointe ‚Üí Pr√©dire ~180 kWh
```

$P(\text{pointe})$ devient une **feature informative** pour la r√©gression!

## üîÑ Architecture du Mod√®le √† 2 √âtages

```
                    DONN√âES
                       |
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        |                             |
    √âTAGE 1                       √âTAGE 2
Classification                   R√©gression
        |                             |
Features pour clf:             Features pour reg:
- temp√©rature                  - temp√©rature
- heure_sin/cos                - heure_sin/cos
- weekend                      - weekend
- clients_connectes            - clients_connectes
- (PAS de lags!)               - lags
        |                      - rolling stats
        ‚Üì                      - P(pointe) ‚Üê NOUVEAU!
        |                             |
Logistic Regression                  ‚Üì
        |                             |
        ‚Üì                             |
  P(pointe) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Ridge Regression
                                      |
                                      ‚Üì
                                energie_kwh
```

## üíª Impl√©mentation Python Compl√®te

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression, RidgeCV
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, classification_report, r2_score
from sklearn.preprocessing import StandardScaler

# ============================================
# √âTAPE 1: PR√âPARER FEATURES POUR CLASSIFICATION
# ============================================

print("=" * 70)
print("√âTAGE 1: CLASSIFICATION DES √âV√âNEMENTS DE POINTE")
print("=" * 70)

# Features pour classifier pointe/normal
# IMPORTANT: Ne PAS utiliser lags d'√©nergie (ce serait de la triche!)
# On veut pr√©dire la pointe AVANT de conna√Ætre la consommation

features_classification = [
    # M√©t√©o
    'temperature_ext', 'humidite', 'vitesse_vent', 'irradiance_solaire',

    # Temps (cyclique)
    'heure_sin', 'heure_cos', 'mois_sin', 'mois_cos',
    'jour_semaine_sin', 'jour_semaine_cos',

    # Indicateurs
    'est_weekend', 'est_ferie',

    # TR√àS IMPORTANT
    'clients_connectes',

    # Transformations m√©t√©o
    'degres_jours_chauffage',

    # Interactions
    'temp_heure_cos', 'temp_weekend'
]

# V√©rifier disponibilit√©
features_clf_dispo = [f for f in features_classification if f in train_eng.columns]

X_train_clf = train_eng[features_clf_dispo].values
y_train_clf = train_eng['evenement_pointe'].values
X_test_clf = test_eng[features_clf_dispo].values
y_test_clf = test_eng['evenement_pointe'].values

print(f"Features pour classification: {len(features_clf_dispo)}")
print(f"Distribution train: {y_train_clf.mean():.1%} pointes")
print(f"Distribution test: {y_test_clf.mean():.1%} pointes")


# ============================================
# √âTAPE 2: ENTRA√éNER CLASSIFIEUR LOGISTIQUE
# ============================================

# OPTIONNEL mais recommand√©: Normaliser les features
scaler_clf = StandardScaler()
X_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)
X_test_clf_scaled = scaler_clf.transform(X_test_clf)

# Entra√Ænement
# Note: On peut utiliser sklearn (Partie 2 permettait from scratch)
clf_pointe = LogisticRegression(max_iter=1000, random_state=42)
clf_pointe.fit(X_train_clf_scaled, y_train_clf)

# √âvaluation du classifieur
y_pred_clf_train = clf_pointe.predict(X_train_clf_scaled)
y_pred_clf_test = clf_pointe.predict(X_test_clf_scaled)

acc_train = accuracy_score(y_train_clf, y_pred_clf_train)
acc_test = accuracy_score(y_test_clf, y_pred_clf_test)

print(f"\nPerformance classification:")
print(f"  Accuracy train: {acc_train:.4f}")
print(f"  Accuracy test:  {acc_test:.4f}")

print(f"\nRapport de classification (test):")
print(classification_report(y_test_clf, y_pred_clf_test,
                          target_names=['Normal', 'Pointe']))


# ============================================
# √âTAPE 3: EXTRAIRE PROBABILIT√âS P(pointe)
# ============================================

print("\n" + "=" * 70)
print("EXTRACTION DES PROBABILIT√âS")
print("=" * 70)

# Probabilit√©s de la classe 1 (pointe)
# predict_proba retourne [P(classe 0), P(classe 1)]
# On veut P(classe 1) ‚Üí colonne 1
train_eng['P_pointe'] = clf_pointe.predict_proba(X_train_clf_scaled)[:, 1]
test_eng['P_pointe'] = clf_pointe.predict_proba(X_test_clf_scaled)[:, 1]

print(f"Statistiques P(pointe):")
print(f"\nTrain:")
print(f"  Moyenne: {train_eng['P_pointe'].mean():.3f}")
print(f"  Std:     {train_eng['P_pointe'].std():.3f}")
print(f"  Min:     {train_eng['P_pointe'].min():.3f}")
print(f"  Max:     {train_eng['P_pointe'].max():.3f}")

print(f"\nTest:")
print(f"  Moyenne: {test_eng['P_pointe'].mean():.3f}")
print(f"  Std:     {test_eng['P_pointe'].std():.3f}")
print(f"  Min:     {test_eng['P_pointe'].min():.3f}")
print(f"  Max:     {test_eng['P_pointe'].max():.3f}")

# Visualiser distribution
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Histogramme par classe
axes[0].hist(train_eng[train_eng['evenement_pointe']==0]['P_pointe'],
            bins=50, alpha=0.6, label='Normal', edgecolor='black')
axes[0].hist(train_eng[train_eng['evenement_pointe']==1]['P_pointe'],
            bins=50, alpha=0.6, label='Pointe', edgecolor='black')
axes[0].set_xlabel('P(pointe)', fontsize=11)
axes[0].set_ylabel('Fr√©quence', fontsize=11)
axes[0].set_title('Distribution de P(pointe) par Classe', fontsize=13, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# Boxplot par classe
data_boxplot = [
    train_eng[train_eng['evenement_pointe']==0]['P_pointe'],
    train_eng[train_eng['evenement_pointe']==1]['P_pointe']
]
axes[1].boxplot(data_boxplot, labels=['Normal', 'Pointe'])
axes[1].set_ylabel('P(pointe)', fontsize=11)
axes[1].set_title('P(pointe) par Classe R√©elle', fontsize=13, fontweight='bold')
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# Points pour l'entrevue:
# - Bonne s√©paration? Normal devrait avoir P faible, Pointe P √©lev√©
# - Si chevauchement important ‚Üí classifieur peu performant


# ============================================
# √âTAPE 4: R√âGRESSION AVEC P(pointe)
# ============================================

print("\n" + "=" * 70)
print("√âTAGE 2: R√âGRESSION AVEC P(pointe)")
print("=" * 70)

# Features pour r√©gression = features classiques + P(pointe)
features_regression = features_clf_dispo + [
    # Ajouter features sp√©cifiques r√©gression (lags OK ici)
    'energie_lag1', 'energie_lag24',
    'energie_rolling_mean_6h', 'energie_rolling_mean_24h',
    'temp_squared'
]

# Filtrer celles qui existent
features_reg_dispo = [f for f in features_regression if f in train_eng.columns]

# AJOUTER P(pointe) !
features_final = features_reg_dispo + ['P_pointe']

X_train_final = train_eng[features_final].values
y_train_final = train_eng['energie_kwh'].values
X_test_final = test_eng[features_final].values
y_test_final = test_eng['energie_kwh'].values

print(f"Features totales pour r√©gression: {len(features_final)}")
print(f"  - Features de base: {len(features_reg_dispo)}")
print(f"  - P(pointe): 1")


# ============================================
# COMPARAISON: AVEC vs SANS P(pointe)
# ============================================

# Mod√®le SANS P(pointe)
X_train_sans = train_eng[features_reg_dispo].values
X_test_sans = test_eng[features_reg_dispo].values

model_sans_p = RidgeCV(alphas=[0.1, 1, 10, 100], cv=TimeSeriesSplit(n_splits=5))
model_sans_p.fit(X_train_sans, y_train_final)
y_pred_sans = model_sans_p.predict(X_test_sans)

r2_sans = r2_score(y_test_final, y_pred_sans)
rmse_sans = np.sqrt(mean_squared_error(y_test_final, y_pred_sans))

print(f"\nRidge SANS P(pointe):")
print(f"  Œª optimal: {model_sans_p.alpha_}")
print(f"  R¬≤ test:   {r2_sans:.4f}")
print(f"  RMSE test: {rmse_sans:.2f} kWh")


# Mod√®le AVEC P(pointe)
model_avec_p = RidgeCV(alphas=[0.1, 1, 10, 100], cv=TimeSeriesSplit(n_splits=5))
model_avec_p.fit(X_train_final, y_train_final)
y_pred_avec = model_avec_p.predict(X_test_final)

r2_avec = r2_score(y_test_final, y_pred_avec)
rmse_avec = np.sqrt(mean_squared_error(y_test_final, y_pred_avec))

print(f"\nRidge AVEC P(pointe):")
print(f"  Œª optimal: {model_avec_p.alpha_}")
print(f"  R¬≤ test:   {r2_avec:.4f}")
print(f"  RMSE test: {rmse_avec:.2f} kWh")


# AM√âLIORATION
amelioration_r2 = r2_avec - r2_sans
amelioration_pct = 100 * amelioration_r2 / (1 - r2_sans)  # % de r√©duction de l'erreur restante

print(f"\nüìà IMPACT DE P(pointe):")
print(f"  Am√©lioration R¬≤: +{amelioration_r2:.4f}")
print(f"  R√©duction erreur: {amelioration_pct:.1f}%")
print(f"  RMSE r√©duit de: {rmse_sans - rmse_avec:.2f} kWh")


# ============================================
# ANALYSE DU COEFFICIENT DE P(pointe)
# ============================================

# Coefficient de P(pointe dans le mod√®le Ridge
idx_p_pointe = features_final.index('P_pointe')
coef_p_pointe = model_avec_p.coef_[idx_p_pointe]

print(f"\nüîç ANALYSE DU COEFFICIENT P(pointe):")
print(f"  Coefficient: {coef_p_pointe:.2f}")

# Interpr√©tation
if coef_p_pointe > 0:
    print(f"  Interpr√©tation: Augmenter P(pointe) de 0.1 (10 points de %) "
          f"augmente consommation pr√©dite de {coef_p_pointe * 0.1:.1f} kWh")
else:
    print(f"  ‚ö†Ô∏è  Coefficient n√©gatif inattendu!")

# Top features par importance (valeur absolue coefficient)
coef_abs = pd.DataFrame({
    'Feature': features_final,
    'Coefficient': model_avec_p.coef_,
    '|Coefficient|': np.abs(model_avec_p.coef_)
}).sort_values('|Coefficient|', ascending=False)

print(f"\nTop 10 features par importance:")
print(coef_abs.head(10).to_string(index=False))


# ============================================
# VISUALISATION: PR√âDICTIONS
# ============================================

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Scatter: R√©el vs Pr√©dit (SANS P)
axes[0].scatter(y_test_final, y_pred_sans, alpha=0.4, s=10, label='Pr√©dictions')
axes[0].plot([y_test_final.min(), y_test_final.max()],
            [y_test_final.min(), y_test_final.max()],
            'r--', linewidth=2, label='Parfait')
axes[0].set_xlabel('√ânergie r√©elle (kWh)', fontsize=11)
axes[0].set_ylabel('√ânergie pr√©dite (kWh)', fontsize=11)
axes[0].set_title(f'SANS P(pointe) - R¬≤ = {r2_sans:.4f}',
                 fontsize=12, fontweight='bold')
axes[0].legend(fontsize=10)
axes[0].grid(True, alpha=0.3)

# Scatter: R√©el vs Pr√©dit (AVEC P)
axes[1].scatter(y_test_final, y_pred_avec, alpha=0.4, s=10, label='Pr√©dictions')
axes[1].plot([y_test_final.min(), y_test_final.max()],
            [y_test_final.min(), y_test_final.max()],
            'r--', linewidth=2, label='Parfait')
axes[1].set_xlabel('√ânergie r√©elle (kWh)', fontsize=11)
axes[1].set_ylabel('√ânergie pr√©dite (kWh)', fontsize=11)
axes[1].set_title(f'AVEC P(pointe) - R¬≤ = {r2_avec:.4f}',
                 fontsize=12, fontweight='bold')
axes[1].legend(fontsize=10)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Points pour l'entrevue:
# - Points devraient √™tre plus proches de la ligne rouge avec P(pointe)
# - Moins de dispersion = meilleures pr√©dictions
```

## ‚ö†Ô∏è Points Critiques pour l'Entrevue

### 1. Pourquoi utiliser P(pointe) et non la classe 0/1?

**R√©ponse:**

**Classe binaire (0/1):** Perte d'information

- P=0.49 ‚Üí 0, P=0.51 ‚Üí 1 : tr√®s similaires mais classes diff√©rentes!
- P=0.51 ‚Üí 1, P=0.99 ‚Üí 1 : tr√®s diff√©rentes mais m√™me classe!

**Probabilit√© continue:** Capture la nuance

- P=0.05 ‚Üí Tr√®s s√ªr normal ‚Üí Pr√©dire consommation basse
- P=0.50 ‚Üí Incertain ‚Üí Pr√©dire consommation interm√©diaire
- P=0.95 ‚Üí Tr√®s s√ªr pointe ‚Üí Pr√©dire consommation √©lev√©e

**Avantage pour Ridge:**

- Ridge est un mod√®le **lin√©aire**
- Peut utiliser P(pointe) comme variable continue
- Apprend automatiquement: $\beta_{P\text{pointe}} \times P(\text{pointe})$

### 2. Pourquoi ne pas utiliser lags pour classifier la pointe?

**Question pi√®ge:** "Vous utilisez energie_lag1 pour la r√©gression mais pas pour la classification, pourquoi?"

**R√©ponse:**

**Pour classification (√âtage 1):**

- But: Pr√©dire **√©v√©nement de pointe** (0/1)
- On veut pr√©dire AVANT de conna√Ætre la consommation!
- Utiliser lags d'√©nergie = **tricher** (information du futur)
- Features: seulement m√©t√©o + temps + clients

**Pour r√©gression (√âtage 2):**

- But: Pr√©dire **valeur de consommation** (kWh)
- Les lags aident car consommation actuelle ~ consommation pass√©e
- Ici c'est OK tant qu'on respecte chronologie (pas de fuite Kaggle)

**Exemple concret:**

```
‚ùå MAL (fuite):
   "Si consommation hier = 180 kWh ‚Üí probablement pointe aujourd'hui"
   ‚Üí On utilise une proxy de la cible pour pr√©dire la cible!

‚úÖ BON:
   "Si temp√©rature = ‚àí15¬∞C ET heure = 18h ‚Üí probablement pointe"
   ‚Üí On utilise seulement info disponible avant la pointe
```

### 3. Quelle am√©lioration attendez-vous de P(pointe)?

**Question type:** "Vous ajoutez P(pointe), combien de points de R¬≤ esp√©rez-vous gagner?"

**R√©ponse r√©aliste bas√©e sur exp√©rience:**

- **Baseline (sans P):** R¬≤ ‚âà 0.75-0.85
- **Avec P(pointe):** R¬≤ ‚âà 0.80-0.90
- **Am√©lioration:** +0.03 √† +0.08 points de R¬≤

**Facteurs influen√ßant l'am√©lioration:**

1. **Performance du classifieur**
   - Accuracy > 0.90 ‚Üí grosse am√©lioration
   - Accuracy < 0.70 ‚Üí petite am√©lioration (bruit)

2. **Diff√©rence consommation pointe/normal**
   - Si pointe = 3√ó normal ‚Üí P(pointe) tr√®s utile!
   - Si pointe ‚âà normal ‚Üí P(pointe) peu utile

3. **Variables d√©j√† pr√©sentes**
   - Si `clients_connectes` d√©j√† l√† ‚Üí moins d'am√©lioration
   - Si seulement m√©t√©o de base ‚Üí plus d'am√©lioration

**Pour l'entrevue:** Dire "J'attends +5% de R¬≤ car..." montre que vous avez r√©fl√©chi!

### 4. Mod√®le √† 2 √©tages vs Mod√®le unique?

**Question:** "Pourquoi ne pas juste ajouter `evenement_pointe` comme feature?"

**Comparaison:**

**Option 1: Ajouter `evenement_pointe` binaire**

```python
features = [..., 'evenement_pointe']  # 0 ou 1
```

‚Üí ‚ùå Sur test/Kaggle, on ne CONNA√éT PAS la vraie classe!

**Option 2: Pr√©dire puis utiliser P(pointe)** (notre approche)

```python
# √âtage 1
P_pointe = clf.predict_proba(X)[:, 1]
# √âtage 2
features = [..., P_pointe]
```

‚Üí ‚úÖ On PR√âDIT P(pointe), puis on l'utilise comme feature

**Cl√©:** On ne triche pas! On pr√©dit une proxy, pas la vraie valeur.

### 5. Comment g√©rer l'erreur propag√©e?

**Question avanc√©e:** "L'erreur du classifieur ne se propage-t-elle pas √† la r√©gression?"

**R√©ponse:**

**Oui, il y a propagation d'erreur!**

Si classifieur pr√©dit:

- P(pointe) = 0.80 alors que vraie classe = 0 (faux positif)
  ‚Üí R√©gression va sur-pr√©dire la consommation

**Mais:**

1. **Ridge est robuste** aux features bruit√©es (r√©gularisation)
2. **P(pointe) est probabiliste** (pas binaire) ‚Üí moins sensible
3. **Am√©lioration nette** m√™me avec erreur de classification

**Analogie:**

- M√©t√©o imparfaite est mieux que pas de m√©t√©o!
- P(pointe) imparfaite est mieux que rien!

**Trade-off:**

- +Information utile (P distingue patterns pointe/normal)
- -Bruit ajout√© (erreurs classification)
- **Net: positif** dans la plupart des cas

## üéØ Checklist pour l'Entrevue Mod√®le √† 2 √âtages

- [ ] Dessiner architecture 2 √©tages au tableau
- [ ] Expliquer pourquoi probabilit√© > classe binaire
- [ ] Justifier features diff√©rentes pour √âtage 1 vs 2
- [ ] Expliquer pourquoi PAS de lags pour classification
- [ ] Calculer am√©lioration R¬≤ due √† P(pointe)
- [ ] Interpr√©ter coefficient de P(pointe): "Œ≤ = 50 signifie..."
- [ ] Comparer histogrammes P(pointe) pour classes 0 et 1
- [ ] Expliquer propagation d'erreur mais b√©n√©fice net
- [ ] D√©fendre: Pourquoi 2 √©tages vs mod√®le unique?
- [ ] Montrer graphiques: Scatter avec/sans P(pointe)

---

## üí° Astuces pour l'Entrevue

1. **Pr√©parez un brouillon OLS**: Entra√Ænez-vous √† d√©river $\hat{\beta} = (X^TX)^{-1}X^Ty$ au tableau
2. **Connaissez vos choix**: Pourquoi ces features? Pourquoi cette validation?
3. **Visualisez**: Dessinez les concepts (cercle pour cyclique, graphe loss pour GD)
4. **Soyez honn√™te**: Si vous ne savez pas, expliquez votre raisonnement
5. **Pr√©parez des exemples**: "Par exemple, pour la temp√©rature..."

---

## üìà Suivi de Progression

- [ ] Partie 0: Configuration ‚úÖ
- [ ] Partie 1: OLS from scratch
- [ ] Partie 2: R√©gression logistique & gradient descent
- [ ] Partie 3: Ridge regression
- [ ] Partie 4: Mod√®le √† 2 √©tages
- [ ] Partie 5: Validation temporelle
- [ ] Partie 6: Mod√®le final
- [ ] Partie 7: Extension
- [ ] Soumission Kaggle
