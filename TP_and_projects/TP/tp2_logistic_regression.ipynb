{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2: Régression logistique à la main\n",
    "\n",
    "**IFT6390 - Fondements de l'apprentissage machine**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pierrelux/mlbook/blob/main/exercises/tp2_logistic_regression.ipynb)\n",
    "\n",
    "Ce notebook accompagne le [Chapitre 3: Classification linéaire](https://pierrelux.github.io/mlbook/ch3_classification).\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "À la fin de ce TP, vous serez en mesure de:\n",
    "- Implémenter la fonction sigmoïde et comprendre son rôle\n",
    "- Calculer l'entropie croisée binaire\n",
    "- Dériver et implémenter le gradient de la perte logistique\n",
    "- Entraîner un classifieur par descente de gradient\n",
    "- Interpréter les coefficients appris\n",
    "\n",
    "Ce TP implémente **tout à la main**, sans utiliser scikit-learn pour l'entraînement. Nous utilisons le jeu de données du béton, déjà rencontré au chapitre 2, mais cette fois pour un problème de **classification**: le béton est-il conforme (résistance suffisante) ou non?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 0: Configuration\n",
    "\n",
    "Exécutez cette cellule pour importer les bibliothèques nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pour de jolis graphiques\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Installation de ucimlrepo si nécessaire\n",
    "try:\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "except ImportError:\n",
    "    !pip install ucimlrepo\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "print(\"Configuration terminée!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 1: Les données du béton\n",
    "\n",
    "Au chapitre 2, nous avons prédit la **résistance du béton** (en MPa) à partir de sa formulation. Ici, nous posons une question binaire: la résistance dépasse-t-elle un seuil donné?\n",
    "\n",
    "- **Classe 1 (conforme)**: résistance $\\geq$ seuil\n",
    "- **Classe 0 (non conforme)**: résistance $<$ seuil\n",
    "\n",
    "Les mêmes caractéristiques (ciment, eau, âge, etc.) servent à prédire cette étiquette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le jeu de données\n",
    "beton = fetch_ucirepo(id=165)\n",
    "X_df = beton.data.features\n",
    "y_resistance = beton.data.targets.values.ravel()\n",
    "\n",
    "# Noms des caractéristiques\n",
    "noms_caracteristiques = ['Ciment', 'Laitier', 'Cendres', 'Eau', 'Plastifiant',\n",
    "                         'Granulat gros', 'Granulat fin', 'Âge']\n",
    "\n",
    "print(f\"Nombre d'échantillons: {len(y_resistance)}\")\n",
    "print(f\"Nombre de caractéristiques: {X_df.shape[1]}\")\n",
    "print(f\"\\nCaractéristiques: {noms_caracteristiques}\")\n",
    "print(f\"\\nRésistance: min={y_resistance.min():.1f} MPa, max={y_resistance.max():.1f} MPa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer la cible binaire: conforme si résistance >= médiane\n",
    "seuil = np.median(y_resistance)\n",
    "y = (y_resistance >= seuil).astype(int)\n",
    "\n",
    "print(f\"Seuil de conformité: {seuil:.1f} MPa (médiane)\")\n",
    "print(f\"\\nClasse 0 (non conforme): {np.sum(y == 0)} échantillons ({np.mean(y == 0):.1%})\")\n",
    "print(f\"Classe 1 (conforme): {np.sum(y == 1)} échantillons ({np.mean(y == 1):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Histogramme de la résistance\n",
    "ax = axes[0]\n",
    "ax.hist(y_resistance, bins=30, edgecolor='white', alpha=0.7)\n",
    "ax.axvline(seuil, color='red', linestyle='--', linewidth=2, label=f'Seuil = {seuil:.1f} MPa')\n",
    "ax.set_xlabel('Résistance (MPa)')\n",
    "ax.set_ylabel('Fréquence')\n",
    "ax.set_title('Distribution de la résistance du béton')\n",
    "ax.legend()\n",
    "\n",
    "# Répartition des classes\n",
    "ax = axes[1]\n",
    "classes, counts = np.unique(y, return_counts=True)\n",
    "bars = ax.bar(['Non conforme\\n(classe 0)', 'Conforme\\n(classe 1)'], counts, \n",
    "              color=['C1', 'C0'], alpha=0.7)\n",
    "ax.set_ylabel('Nombre d\\'échantillons')\n",
    "ax.set_title('Répartition des classes')\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "            str(count), ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation des données\n",
    "\n",
    "Les caractéristiques ont des échelles très différentes (l'âge en jours vs le ciment en kg/m³). Pour que la descente de gradient converge bien, nous **normalisons** les données: chaque caractéristique est centrée (moyenne 0) et réduite (écart-type 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les données brutes\n",
    "X_raw = X_df.values\n",
    "\n",
    "# Normalisation: centrer et réduire\n",
    "X_mean = X_raw.mean(axis=0)\n",
    "X_std = X_raw.std(axis=0)\n",
    "X_normalized = (X_raw - X_mean) / X_std\n",
    "\n",
    "# Ajouter la colonne de biais (colonne de 1)\n",
    "X = np.column_stack([np.ones(len(X_normalized)), X_normalized])\n",
    "\n",
    "print(f\"Forme de X (avec biais): {X.shape}\")\n",
    "print(f\"\\nMoyennes avant normalisation: {X_raw.mean(axis=0).round(1)}\")\n",
    "print(f\"Moyennes après normalisation: {X_normalized.mean(axis=0).round(6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Séparation entraînement / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation train/test (80% / 20%)\n",
    "np.random.seed(42)\n",
    "n = len(y)\n",
    "indices = np.random.permutation(n)\n",
    "n_train = int(0.8 * n)\n",
    "\n",
    "train_idx = indices[:n_train]\n",
    "test_idx = indices[n_train:]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "print(f\"Entraînement: {len(X_train)} échantillons\")\n",
    "print(f\"Test: {len(X_test)} échantillons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 2: La fonction sigmoïde\n",
    "\n",
    "La **fonction sigmoïde** transforme n'importe quel nombre réel en une valeur entre 0 et 1:\n",
    "\n",
    "$$\\sigma(a) = \\frac{1}{1 + e^{-a}}$$\n",
    "\n",
    "C'est la clé de la régression logistique: elle permet d'interpréter la sortie comme une probabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1: Implémenter la sigmoïde ★\n",
    "\n",
    "Complétez la fonction ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    \"\"\"\n",
    "    Calcule la fonction sigmoïde.\n",
    "    \n",
    "    Args:\n",
    "        a: scalaire ou array numpy\n",
    "    \n",
    "    Returns:\n",
    "        sigma(a) = 1 / (1 + exp(-a))\n",
    "    \"\"\"\n",
    "    # ============================================\n",
    "    # TODO: Implémentez la sigmoïde\n",
    "    # Indice: utilisez np.exp()\n",
    "    # ============================================\n",
    "    \n",
    "    result = None  # <- Remplacez par votre code\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre fonction\n",
    "if sigmoid(0) is not None:\n",
    "    print(f\"sigma(0) = {sigmoid(0):.4f}  (attendu: 0.5)\")\n",
    "    print(f\"sigma(2) = {sigmoid(2):.4f}  (attendu: 0.8808)\")\n",
    "    print(f\"sigma(-2) = {sigmoid(-2):.4f}  (attendu: 0.1192)\")\n",
    "    \n",
    "    # Vérification\n",
    "    if np.isclose(sigmoid(0), 0.5) and np.isclose(sigmoid(2), 0.8808, atol=1e-3):\n",
    "        print(\"\\nCorrect!\")\n",
    "    else:\n",
    "        print(\"\\nVérifiez votre implémentation.\")\n",
    "else:\n",
    "    print(\"Complétez la fonction sigmoid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b> (cliquez pour afficher)</summary>\n",
    "\n",
    "```python\n",
    "def sigmoid(a):\n",
    "    return 1 / (1 + np.exp(-a))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualiser la sigmoïde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sigmoid(0) is not None:\n",
    "    a_values = np.linspace(-6, 6, 200)\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(a_values, sigmoid(a_values), 'C0-', linewidth=2)\n",
    "    plt.axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    plt.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.xlabel('$a = \\\\boldsymbol{\\\\theta}^\\\\top \\\\mathbf{x}$')\n",
    "    plt.ylabel('$\\\\sigma(a) = P(\\\\text{conforme}|\\\\mathbf{x})$')\n",
    "    plt.title('La fonction sigmoïde')\n",
    "    \n",
    "    plt.annotate('$a < 0$: non conforme plus probable', xy=(-4.5, 0.15), fontsize=10, color='C1')\n",
    "    plt.annotate('$a > 0$: conforme plus probable', xy=(1.5, 0.85), fontsize=10, color='C0')\n",
    "    \n",
    "    plt.xlim(-6, 6)\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Complétez d'abord la fonction sigmoid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 3: L'entropie croisée\n",
    "\n",
    "La **perte** mesure à quel point nos prédictions sont mauvaises. Pour la régression logistique, nous utilisons l'**entropie croisée binaire**:\n",
    "\n",
    "$$\\mathcal{L}(\\boldsymbol{\\theta}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\mu_i) + (1-y_i) \\log(1-\\mu_i) \\right]$$\n",
    "\n",
    "où $\\mu_i = \\sigma(\\boldsymbol{\\theta}^\\top \\mathbf{x}_i)$ est la probabilité prédite pour la classe 1 (conforme)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2: Implémenter l'entropie croisée ★\n",
    "\n",
    "Complétez la fonction ci-dessous.\n",
    "\n",
    "**Conseil**: Utilisez `np.clip()` pour éviter les problèmes numériques avec $\\log(0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcule l'entropie croisée binaire moyenne.\n",
    "    \n",
    "    Args:\n",
    "        y_true: étiquettes vraies (0 ou 1), array de taille (N,)\n",
    "        y_pred: probabilités prédites (entre 0 et 1), array de taille (N,)\n",
    "    \n",
    "    Returns:\n",
    "        Entropie croisée moyenne (scalaire)\n",
    "    \"\"\"\n",
    "    # Éviter log(0) en \"clipant\" les probabilités\n",
    "    eps = 1e-10\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    \n",
    "    # ============================================\n",
    "    # TODO: Calculez l'entropie croisée\n",
    "    # Formule: -mean(y * log(pred) + (1-y) * log(1-pred))\n",
    "    # ============================================\n",
    "    \n",
    "    loss = None  # <- Remplacez par votre code\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre fonction\n",
    "y_test_ex = np.array([1, 0, 1, 0])\n",
    "pred_parfait = np.array([0.99, 0.01, 0.99, 0.01])\n",
    "pred_mauvais = np.array([0.01, 0.99, 0.01, 0.99])\n",
    "pred_neutre = np.array([0.5, 0.5, 0.5, 0.5])\n",
    "\n",
    "if cross_entropy_loss(y_test_ex, pred_parfait) is not None:\n",
    "    print(f\"Perte (prédictions parfaites): {cross_entropy_loss(y_test_ex, pred_parfait):.4f}  (proche de 0)\")\n",
    "    print(f\"Perte (prédictions neutres): {cross_entropy_loss(y_test_ex, pred_neutre):.4f}  (environ 0.69)\")\n",
    "    print(f\"Perte (prédictions inversées): {cross_entropy_loss(y_test_ex, pred_mauvais):.4f}  (très élevée)\")\n",
    "    \n",
    "    if cross_entropy_loss(y_test_ex, pred_parfait) < 0.1:\n",
    "        print(\"\\nCorrect!\")\n",
    "else:\n",
    "    print(\"Complétez la fonction cross_entropy_loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b> (cliquez pour afficher)</summary>\n",
    "\n",
    "```python\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    eps = 1e-10\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    \n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 4: Le gradient\n",
    "\n",
    "Pour minimiser la perte par descente de gradient, nous avons besoin du **gradient**. La dérivation (voir le chapitre 3) donne une formule simple:\n",
    "\n",
    "$$\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} (\\mu_i - y_i) \\mathbf{x}_i = \\frac{1}{N} \\mathbf{X}^\\top (\\boldsymbol{\\mu} - \\mathbf{y})$$\n",
    "\n",
    "Le gradient est une moyenne pondérée des entrées, où le poids est **l'erreur de prédiction** $\\mu_i - y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3: Implémenter le gradient ★★\n",
    "\n",
    "Complétez la fonction ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    Calcule le gradient de l'entropie croisée.\n",
    "    \n",
    "    Args:\n",
    "        X: matrice de données avec biais (N, d)\n",
    "        y: étiquettes (N,)\n",
    "        theta: paramètres actuels (d,)\n",
    "    \n",
    "    Returns:\n",
    "        Gradient de la perte (d,)\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    \n",
    "    # ============================================\n",
    "    # TODO: \n",
    "    # 1. Calculer les probabilités mu = sigmoid(X @ theta)\n",
    "    # 2. Calculer le gradient = X.T @ (mu - y) / N\n",
    "    # ============================================\n",
    "    \n",
    "    mu = None  # <- Prédictions\n",
    "    gradient = None  # <- Gradient\n",
    "    \n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre fonction\n",
    "theta_test = np.zeros(X_train.shape[1])  # 9 paramètres (biais + 8 caractéristiques)\n",
    "\n",
    "if sigmoid(0) is not None:\n",
    "    grad = compute_gradient(X_train, y_train, theta_test)\n",
    "    \n",
    "    if grad is not None:\n",
    "        print(f\"Gradient avec theta = 0:\")\n",
    "        print(f\"  Biais: {grad[0]:.4f}\")\n",
    "        for i, nom in enumerate(noms_caracteristiques):\n",
    "            print(f\"  {nom}: {grad[i+1]:.4f}\")\n",
    "        print(f\"\\nNorme du gradient: {np.linalg.norm(grad):.4f}\")\n",
    "    else:\n",
    "        print(\"Complétez la fonction compute_gradient!\")\n",
    "else:\n",
    "    print(\"Complétez d'abord la fonction sigmoid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b> (cliquez pour afficher)</summary>\n",
    "\n",
    "```python\n",
    "def compute_gradient(X, y, theta):\n",
    "    N = len(y)\n",
    "    mu = sigmoid(X @ theta)\n",
    "    gradient = X.T @ (mu - y) / N\n",
    "    return gradient\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 5: Descente de gradient\n",
    "\n",
    "Nous avons tous les ingrédients! La **descente de gradient** met à jour les paramètres de façon itérative:\n",
    "\n",
    "$$\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t - \\eta \\cdot \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}_t)$$\n",
    "\n",
    "où $\\eta > 0$ est le **taux d'apprentissage**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4: Implémenter la descente de gradient ★★\n",
    "\n",
    "Complétez la boucle d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X, y, lr=0.1, n_iterations=100):\n",
    "    \"\"\"\n",
    "    Entraîne un classifieur logistique par descente de gradient.\n",
    "    \n",
    "    Args:\n",
    "        X: matrice de données avec biais (N, d)\n",
    "        y: étiquettes (N,)\n",
    "        lr: taux d'apprentissage\n",
    "        n_iterations: nombre d'itérations\n",
    "    \n",
    "    Returns:\n",
    "        theta: paramètres appris (d,)\n",
    "        losses: historique de la perte\n",
    "    \"\"\"\n",
    "    d = X.shape[1]\n",
    "    theta = np.zeros(d)  # Initialisation\n",
    "    losses = []\n",
    "    \n",
    "    for t in range(n_iterations):\n",
    "        # ============================================\n",
    "        # TODO:\n",
    "        # 1. Calculer les probabilités prédites\n",
    "        # 2. Calculer la perte (pour l'historique)\n",
    "        # 3. Calculer le gradient\n",
    "        # 4. Mettre à jour theta\n",
    "        # ============================================\n",
    "        \n",
    "        # Probabilités prédites\n",
    "        mu = None  # <- Complétez\n",
    "        \n",
    "        # Perte actuelle\n",
    "        loss = None  # <- Complétez\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Gradient\n",
    "        grad = None  # <- Complétez\n",
    "        \n",
    "        # Mise à jour: theta = theta - lr * grad\n",
    "        # <- Complétez\n",
    "        pass\n",
    "    \n",
    "    return theta, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement!\n",
    "if sigmoid(0) is not None and cross_entropy_loss(y_test_ex, pred_parfait) is not None:\n",
    "    theta_learned, loss_history = train_logistic_regression(\n",
    "        X_train, y_train, lr=0.5, n_iterations=200\n",
    "    )\n",
    "    \n",
    "    if loss_history[0] is not None:\n",
    "        print(\"Paramètres appris:\")\n",
    "        print(f\"  Biais: {theta_learned[0]:.4f}\")\n",
    "        for i, nom in enumerate(noms_caracteristiques):\n",
    "            print(f\"  {nom}: {theta_learned[i+1]:.4f}\")\n",
    "        print(f\"\\nPerte initiale: {loss_history[0]:.4f}\")\n",
    "        print(f\"Perte finale: {loss_history[-1]:.4f}\")\n",
    "    else:\n",
    "        print(\"Complétez la fonction train_logistic_regression!\")\n",
    "else:\n",
    "    print(\"Complétez d'abord les fonctions précédentes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b> (cliquez pour afficher)</summary>\n",
    "\n",
    "```python\n",
    "def train_logistic_regression(X, y, lr=0.1, n_iterations=100):\n",
    "    d = X.shape[1]\n",
    "    theta = np.zeros(d)\n",
    "    losses = []\n",
    "    \n",
    "    for t in range(n_iterations):\n",
    "        # Probabilités prédites\n",
    "        mu = sigmoid(X @ theta)\n",
    "        \n",
    "        # Perte actuelle\n",
    "        loss = cross_entropy_loss(y, mu)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Gradient\n",
    "        grad = compute_gradient(X, y, theta)\n",
    "        \n",
    "        # Mise à jour\n",
    "        theta = theta - lr * grad\n",
    "    \n",
    "    return theta, losses\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualiser la convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'loss_history' in dir() and loss_history[0] is not None:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(loss_history, 'C0-', linewidth=2)\n",
    "    plt.xlabel('Itération')\n",
    "    plt.ylabel('Entropie croisée')\n",
    "    plt.title('Convergence de la descente de gradient')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Complétez d'abord la fonction d'entraînement!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 6: Prédiction et évaluation\n",
    "\n",
    "Pour prédire, nous classifions comme \"conforme\" si $p(y=1|\\mathbf{x}) > 0.5$, c'est-à-dire si $\\boldsymbol{\\theta}^\\top \\mathbf{x} > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5: Implémenter la prédiction ★"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(X, theta):\n",
    "    \"\"\"Retourne les probabilités de la classe 1 (conforme).\"\"\"\n",
    "    # ============================================\n",
    "    # TODO: Calculez sigmoid(X @ theta)\n",
    "    # ============================================\n",
    "    return None  # <- Remplacez\n",
    "\n",
    "\n",
    "def predict(X, theta, threshold=0.5):\n",
    "    \"\"\"Retourne les prédictions binaires (0 ou 1).\"\"\"\n",
    "    # ============================================\n",
    "    # TODO: Retournez 1 si proba >= threshold, 0 sinon\n",
    "    # Indice: (predict_proba(X, theta) >= threshold).astype(int)\n",
    "    # ============================================\n",
    "    return None  # <- Remplacez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation\n",
    "if 'theta_learned' in dir() and predict(X_train, theta_learned) is not None:\n",
    "    # Précision sur l'ensemble d'entraînement\n",
    "    y_pred_train = predict(X_train, theta_learned)\n",
    "    accuracy_train = np.mean(y_pred_train == y_train)\n",
    "    \n",
    "    # Précision sur l'ensemble de test\n",
    "    y_pred_test = predict(X_test, theta_learned)\n",
    "    accuracy_test = np.mean(y_pred_test == y_test)\n",
    "    \n",
    "    print(f\"Précision sur l'entraînement: {accuracy_train:.1%}\")\n",
    "    print(f\"Précision sur le test: {accuracy_test:.1%}\")\n",
    "else:\n",
    "    print(\"Complétez les fonctions de prédiction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b> (cliquez pour afficher)</summary>\n",
    "\n",
    "```python\n",
    "def predict_proba(X, theta):\n",
    "    return sigmoid(X @ theta)\n",
    "\n",
    "def predict(X, theta, threshold=0.5):\n",
    "    return (predict_proba(X, theta) >= threshold).astype(int)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 7: Interprétation des coefficients\n",
    "\n",
    "Un avantage de la régression logistique: les coefficients sont **interprétables**. Chaque $\\theta_j$ représente le changement dans le log-odds (logarithme du rapport de cotes) pour une augmentation d'un écart-type de la caractéristique $j$.\n",
    "\n",
    "- $\\theta_j > 0$: la caractéristique augmente la probabilité de conformité\n",
    "- $\\theta_j < 0$: la caractéristique diminue la probabilité de conformité\n",
    "- $|\\theta_j|$ grand: effet fort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'theta_learned' in dir() and loss_history[0] is not None:\n",
    "    # Coefficients (sans le biais)\n",
    "    coefficients = theta_learned[1:]\n",
    "    \n",
    "    # Graphique\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    couleurs = ['C0' if c > 0 else 'C1' for c in coefficients]\n",
    "    bars = ax.barh(noms_caracteristiques, coefficients, color=couleurs, alpha=0.8)\n",
    "    ax.axvline(0, color='gray', linewidth=1)\n",
    "    \n",
    "    ax.set_xlabel('Coefficient (effet sur le log-odds de conformité)')\n",
    "    ax.set_title('Influence des caractéristiques sur la probabilité de conformité')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Interprétation:\")\n",
    "    print(\"- Coefficients positifs (bleu): augmentent la probabilité de conformité\")\n",
    "    print(\"- Coefficients négatifs (orange): diminuent la probabilité de conformité\")\n",
    "else:\n",
    "    print(\"Complétez d'abord les fonctions précédentes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions de réflexion:**\n",
    "1. Quelles caractéristiques augmentent le plus la probabilité de conformité?\n",
    "2. Est-ce cohérent avec la physique du béton? (Plus de ciment = plus résistant? Plus d'eau = moins résistant?)\n",
    "3. Pourquoi l'âge a-t-il un effet positif?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 8: Expérimentations ★★\n",
    "\n",
    "Explorons l'effet du taux d'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sigmoid(0) is not None and cross_entropy_loss(y_test_ex, pred_parfait) is not None:\n",
    "    learning_rates = [0.01, 0.1, 0.5, 1.0, 2.0]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        _, losses = train_logistic_regression(X_train, y_train, lr=lr, n_iterations=200)\n",
    "        if losses[0] is not None:\n",
    "            plt.plot(losses, label=f'η = {lr}', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Itération')\n",
    "    plt.ylabel('Entropie croisée')\n",
    "    plt.title('Effet du taux d\\'apprentissage')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Observations:\")\n",
    "    print(\"- η trop petit (0.01): convergence très lente\")\n",
    "    print(\"- η trop grand (2.0): peut osciller ou diverger\")\n",
    "    print(\"- η = 0.5 ou 1.0: bon compromis pour ce jeu de données\")\n",
    "else:\n",
    "    print(\"Complétez d'abord les fonctions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 9: Comparaison avec scikit-learn ★\n",
    "\n",
    "Vérifions que notre implémentation donne des résultats similaires à scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as SklearnLogReg\n",
    "\n",
    "if 'theta_learned' in dir() and loss_history[0] is not None:\n",
    "    # Entraîner avec scikit-learn (sur données normalisées, sans la colonne de biais)\n",
    "    X_train_sklearn = X_train[:, 1:]  # Enlever la colonne de 1\n",
    "    X_test_sklearn = X_test[:, 1:]\n",
    "    \n",
    "    model_sklearn = SklearnLogReg(penalty=None, max_iter=1000)\n",
    "    model_sklearn.fit(X_train_sklearn, y_train)\n",
    "    \n",
    "    # Comparer les précisions\n",
    "    accuracy_sklearn_train = model_sklearn.score(X_train_sklearn, y_train)\n",
    "    accuracy_sklearn_test = model_sklearn.score(X_test_sklearn, y_test)\n",
    "    \n",
    "    print(\"Comparaison des précisions:\")\n",
    "    print(f\"\\nNotre implémentation:\")\n",
    "    print(f\"  Train: {accuracy_train:.1%}\")\n",
    "    print(f\"  Test: {accuracy_test:.1%}\")\n",
    "    \n",
    "    print(f\"\\nScikit-learn:\")\n",
    "    print(f\"  Train: {accuracy_sklearn_train:.1%}\")\n",
    "    print(f\"  Test: {accuracy_sklearn_test:.1%}\")\n",
    "    \n",
    "    # Accord des prédictions\n",
    "    y_pred_sklearn = model_sklearn.predict(X_test_sklearn)\n",
    "    accord = np.mean(y_pred_test == y_pred_sklearn)\n",
    "    print(f\"\\nAccord des prédictions sur le test: {accord:.1%}\")\n",
    "else:\n",
    "    print(\"Complétez d'abord les fonctions précédentes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Récapitulatif\n",
    "\n",
    "Dans ce TP, vous avez implémenté la régression logistique de A à Z sur le jeu de données du béton:\n",
    "\n",
    "1. **Sigmoïde**: $\\sigma(a) = \\frac{1}{1 + e^{-a}}$ transforme un score en probabilité\n",
    "\n",
    "2. **Entropie croisée**: La perte qui pénalise les mauvaises prédictions\n",
    "   $$\\mathcal{L} = -\\frac{1}{N} \\sum_i \\left[ y_i \\log(\\mu_i) + (1-y_i) \\log(1-\\mu_i) \\right]$$\n",
    "\n",
    "3. **Gradient**: Direction pour réduire la perte\n",
    "   $$\\nabla_{\\boldsymbol{\\theta}} \\mathcal{L} = \\frac{1}{N} \\mathbf{X}^\\top (\\boldsymbol{\\mu} - \\mathbf{y})$$\n",
    "\n",
    "4. **Descente de gradient**: Mise à jour itérative\n",
    "   $$\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\eta \\cdot \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}$$\n",
    "\n",
    "5. **Interprétation**: Les coefficients indiquent l'effet de chaque caractéristique sur la probabilité de conformité\n",
    "\n",
    "Ces mêmes idées se généralisent aux réseaux de neurones, où la régression logistique devient la couche de sortie pour la classification.\n",
    "\n",
    "---\n",
    "\n",
    "**Pour aller plus loin**: [Chapitre 3: Classification linéaire](https://pierrelux.github.io/mlbook/ch3_classification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
