# üìö NOTES POUR L'ENTREVUE ORALE - PROJET √âNERGIE
## IFT6390 - Fondements de l'Apprentissage Machine

---

## ‚ö†Ô∏è RAPPEL: L'entrevue orale = 60% de la note!

**Ce qu'on attend de vous:**
- D√©river OLS au tableau ‚úçÔ∏è
- Expliquer CHAQUE ligne de code que vous avez √©crite
- Justifier VOS CHOIX (pourquoi ces features? pourquoi ce Œª?)
- Modifier le code en direct et pr√©dire l'effet
- R√©pondre aux questions th√©oriques

---

## üìñ PARTIE 1: OLS (Ordinary Least Squares)

### Formule √† conna√Ætre PAR C≈íUR:
$$\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}$$

### D√©rivation compl√®te (ESSENTIEL pour l'entrevue!):

**Objectif:** Minimiser l'erreur quadratique moyenne (MSE)

1. **Fonction de co√ªt:**
   $$J(\boldsymbol{\beta}) = \frac{1}{n} \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2$$

2. **D√©veloppement:**
   $$J(\boldsymbol{\beta}) = \frac{1}{n}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^\top(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})$$
   
   $$= \frac{1}{n}(\mathbf{y}^\top\mathbf{y} - \mathbf{y}^\top\mathbf{X}\boldsymbol{\beta} - \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{y} + \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta})$$

3. **Gradient (d√©riv√©e par rapport √† Œ≤):**
   $$\nabla J(\boldsymbol{\beta}) = \frac{1}{n}(-2\mathbf{X}^\top\mathbf{y} + 2\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta})$$

4. **Mettre le gradient √† z√©ro:**
   $$-2\mathbf{X}^\top\mathbf{y} + 2\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta} = 0$$
   
   $$\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta} = \mathbf{X}^\top\mathbf{y}$$

5. **Solution finale:**
   $$\hat{\boldsymbol{\beta}} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}$$

### Code Python (votre impl√©mentation):
```python
def ols_fit(X, y):
    """
    Impl√©mentation OLS from scratch
    X: matrice de features (n, p)
    y: vecteur cible (n,)
    """
    # Ajouter colonne de 1 pour le biais (intercept)
    X_bias = np.column_stack([np.ones(len(X)), X])
    
    # Calcul de Œ≤ = (X^T X)^(-1) X^T y
    XtX = X_bias.T @ X_bias
    Xty = X_bias.T @ y
    beta = np.linalg.solve(XtX, Xty)  # Plus stable que inv()
    
    return beta

def ols_predict(X, beta):
    """Pr√©dictions avec le mod√®le OLS"""
    X_bias = np.column_stack([np.ones(len(X)), X])
    return X_bias @ beta
```

### Questions possibles:
‚ùì **Pourquoi utiliser `np.linalg.solve()` plut√¥t que `np.linalg.inv()`?**
- Plus stable num√©riquement
- √âvite d'inverser explicitement la matrice (co√ªteux et sensible au bruit)

‚ùì **Quand OLS √©choue-t-il?**
- Si X^T X n'est pas inversible (colonnes colin√©aires)
- Si p > n (plus de features que d'exemples)

‚ùì **Pourquoi ajouter une colonne de 1?**
- Pour le terme de biais (intercept) Œ≤‚ÇÄ
- Sans √ßa, la droite passerait par l'origine (0,0)

---

## üìñ PARTIE 2: R√©gression Logistique

### Fonction sigmo√Øde:
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**Propri√©t√© importante:** $\sigma'(z) = \sigma(z)(1 - \sigma(z))$

### Entropie crois√©e (Cross-Entropy Loss):
$$L(\boldsymbol{\beta}) = -\frac{1}{n}\sum_{i=1}^n [y_i \log(\hat{p}_i) + (1-y_i)\log(1-\hat{p}_i)]$$

o√π $\hat{p}_i = \sigma(\mathbf{x}_i^\top\boldsymbol{\beta})$

### Gradient de l'entropie crois√©e:
$$\nabla L(\boldsymbol{\beta}) = \frac{1}{n}\mathbf{X}^\top(\hat{\mathbf{p}} - \mathbf{y})$$

o√π $\hat{\mathbf{p}}$ est le vecteur des probabilit√©s pr√©dites.

### Descente de gradient:
```python
def logistic_regression_gd(X, y, learning_rate=0.01, n_iterations=1000):
    """
    R√©gression logistique avec descente de gradient
    """
    n, p = X.shape
    X_bias = np.column_stack([np.ones(n), X])
    beta = np.zeros(p + 1)
    
    for iteration in range(n_iterations):
        # Pr√©dictions (probabilit√©s)
        z = X_bias @ beta
        p_hat = 1 / (1 + np.exp(-z))  # sigmo√Øde
        
        # Gradient
        gradient = (1/n) * X_bias.T @ (p_hat - y)
        
        # Mise √† jour
        beta = beta - learning_rate * gradient
    
    return beta
```

### Questions possibles:
‚ùì **Pourquoi entropie crois√©e et pas MSE pour classification?**
- MSE n'est pas convexe pour la classification (risque de minima locaux)
- Entropie crois√©e est bien adapt√©e aux probabilit√©s (0-1)

‚ùì **Comment choisir le learning rate?**
- Trop grand ‚Üí divergence
- Trop petit ‚Üí convergence lente
- Regarder la courbe de loss

‚ùì **Que repr√©sente P(pointe)=0.7?**
- 70% de probabilit√© que ce soit un √©v√©nement de pointe
- C'est une PROBABILIT√â, pas une pr√©diction binaire

---

## üìñ PARTIE 3: R√©gularisation Ridge

### Formule Ridge:
$$\hat{\boldsymbol{\beta}}_{\text{Ridge}} = (\mathbf{X}^\top\mathbf{X} + \lambda\mathbf{I})^{-1}\mathbf{X}^\top\mathbf{y}$$

### Fonction de co√ªt Ridge:
$$J(\boldsymbol{\beta}) = \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda\|\boldsymbol{\beta}\|^2$$

### Effet de Œª (lambda):
- **Œª = 0** ‚Üí OLS classique (pas de r√©gularisation)
- **Œª petit** ‚Üí Peu de r√©gularisation
- **Œª grand** ‚Üí Forte r√©gularisation (coefficients ‚Üí 0)

### Code:
```python
def ridge_fit(X, y, lambda_reg=1.0):
    """Ridge from scratch"""
    X_bias = np.column_stack([np.ones(len(X)), X])
    n, p = X_bias.shape
    
    # Matrice identit√© (ne pas r√©gulariser le biais!)
    I = np.eye(p)
    I[0, 0] = 0  # Pas de r√©gularisation sur Œ≤‚ÇÄ
    
    # Solution Ridge
    XtX = X_bias.T @ X_bias
    beta = np.linalg.solve(XtX + lambda_reg * I, X_bias.T @ y)
    
    return beta
```

### Questions possibles:
‚ùì **Pourquoi Ridge aide avec des features corr√©l√©es?**
- Stabilise l'inversion de X^T X
- Distribue le poids entre features corr√©l√©es
- R√©duit la variance du mod√®le

‚ùì **Ridge = MAP! Expliquez.**
- Ridge = Maximum A Posteriori avec prior gaussien sur Œ≤
- Prior: $p(\boldsymbol{\beta}) \sim \mathcal{N}(0, \sigma^2\mathbf{I})$
- √âquivaut √† ajouter une p√©nalit√© L2

‚ùì **Comment choisir Œª?**
- **Validation crois√©e** (TimeSeriesSplit pour s√©ries temporelles!)
- RidgeCV en sklearn
- Chercher Œª qui minimise l'erreur de validation

---

## üìñ PARTIE 4: Division Temporelle (CRUCIAL!)

### ‚ö†Ô∏è INTERDICTION de la validation crois√©e al√©atoire!

**Pourquoi?**
- Les donn√©es sont **s√©ries temporelles**
- KFold al√©atoire ‚Üí **fuite d'information** (data leakage)
- On utiliserait le futur pour pr√©dire le pass√©!

### TimeSeriesSplit:
```
Train: [-------------------]
                Test: [----]

Train: [------------------------]
                        Test: [----]

Train: [-----------------------------]
                                Test: [----]
```

**Principe:** On entra√Æne toujours sur le pass√©, on teste sur le futur.

### Dans ce projet:
- **Train:** Hiver 2023-2024
- **Test:** Printemps/√ât√© 2024

**D√©calage de distribution:**
- Hiver ‚Üí consommation plus √©lev√©e
- √ât√© ‚Üí consommation plus faible
- **C'est r√©aliste!** Le mod√®le doit g√©n√©raliser entre saisons.

---

## üìñ PARTIE 5: Mod√®le √† 2 √©tages

### Architecture:
```
1. Classifieur logistique ‚Üí P(pointe)
           ‚Üì
2. R√©gression Ridge avec P(pointe) comme feature
```

### Pourquoi P(pointe) et pas 0/1?

**Mauvaise approche:**
```python
pred_binaire = (p_pointe > 0.5).astype(int)  # Seulement 0 ou 1
```

**Bonne approche:**
```python
# Utiliser la probabilit√© continue
X_with_proba = np.column_stack([X, p_pointe])  # p_pointe ‚àà [0, 1]
```

**Pourquoi?**
- P=0.6 contient PLUS d'info que juste "1"
- P=0.51 vs P=0.99 sont tous deux "pointe" mais tr√®s diff√©rents!
- Le mod√®le de r√©gression peut **pond√©rer** cette information

### Questions possibles:
‚ùì **Pourquoi 2 √©tages?**
- √âv√©nement de pointe = info cruciale pour consommation
- Mais c'est une variable qu'on ne conna√Æt pas √† l'avance
- On doit d'abord la pr√©dire, puis l'utiliser

‚ùì **Risque de ce mod√®le?**
- **Propagation d'erreur:** Si le classifieur se trompe, √ßa affecte la r√©gression
- Solution: am√©liorer le classifieur en premier!

---

## üìñ CONCEPTS TH√âORIQUES AVANC√âS

### Ridge = MAP (Maximum A Posteriori)

**Interpr√©tation probabiliste:**
- **Likelihood:** $p(\mathbf{y}|\mathbf{X}, \boldsymbol{\beta}) \sim \mathcal{N}(\mathbf{X}\boldsymbol{\beta}, \sigma^2\mathbf{I})$
- **Prior:** $p(\boldsymbol{\beta}) \sim \mathcal{N}(0, \tau^2\mathbf{I})$
- **Posterior:** $p(\boldsymbol{\beta}|\mathbf{X}, \mathbf{y}) \propto p(\mathbf{y}|\mathbf{X}, \boldsymbol{\beta}) \cdot p(\boldsymbol{\beta})$

**MAP = maximiser le posterior:**
$$\max_{\boldsymbol{\beta}} \log p(\boldsymbol{\beta}|\mathbf{X}, \mathbf{y})$$

√âquivaut √† minimiser:
$$\|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2 + \lambda\|\boldsymbol{\beta}\|^2$$

o√π $\lambda = \sigma^2/\tau^2$

### Minimiser entropie crois√©e

**Pour classification binaire:**
- On veut que $\hat{p}_i$ soit proche de $y_i$ (0 ou 1)
- Entropie crois√©e p√©nalise les mauvaises probabilit√©s
- Si $y_i=1$ et $\hat{p}_i=0.9$ ‚Üí petite perte
- Si $y_i=1$ et $\hat{p}_i=0.1$ ‚Üí GROSSE perte

**Gradient:** Simple et √©l√©gant!
$$\nabla L = \frac{1}{n}\mathbf{X}^\top(\hat{\mathbf{p}} - \mathbf{y})$$

---

## üìñ M√âTRIQUES D'√âVALUATION

### Pour R√©gression:

**R¬≤ (Coefficient de d√©termination):**
$$R^2 = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$$

- **R¬≤ = 1** ‚Üí Pr√©diction parfaite
- **R¬≤ = 0** ‚Üí Mod√®le = moyenne
- **R¬≤ < 0** ‚Üí Pire que la moyenne!

**RMSE (Root Mean Squared Error):**
$$\text{RMSE} = \sqrt{\frac{1}{n}\sum(y_i - \hat{y}_i)^2}$$

- En kWh dans ce projet
- Plus petit = mieux
- Interpr√©table (m√™me unit√© que y)

### Pour Classification:

**Accuracy:**
$$\text{Accuracy} = \frac{\text{Nombre correct}}{n}$$

**Precision/Recall:**
- **Precision:** Des points pr√©dits "pointe", combien le sont vraiment?
- **Recall:** Des vrais "pointe", combien on en d√©tecte?

---

## üìù CHECKLIST AVANT L'ENTREVUE

### Je DOIS savoir:
- [ ] D√©river OLS au tableau (de m√©moire)
- [ ] Expliquer gradient descent √©tape par √©tape
- [ ] Justifier pourquoi TimeSeriesSplit (pas KFold)
- [ ] Expliquer Ridge = MAP avec prior gaussien
- [ ] Diff√©rence entre P(pointe) continue vs indicateur 0/1
- [ ] Pourquoi entropie crois√©e pour classification
- [ ] Comment j'ai choisi mes features
- [ ] Comment j'ai choisi Œª (validation crois√©e)
- [ ] Interpr√©ter les coefficients de mon mod√®le
- [ ] Expliquer mes r√©sidus (graphique)

### Je DOIS pouvoir:
- [ ] Modifier le code en direct
- [ ] Ajouter/enlever une feature et pr√©dire l'effet
- [ ] Changer Œª et expliquer ce qui arrive
- [ ] Changer learning rate et voir l'impact
- [ ] Expliquer chaque ligne de mon code

### Questions pi√®ges possibles:
‚ùì "Pourquoi pas MSE pour la logistique?"
‚ùì "Si Œª ‚Üí ‚àû, que deviennent les coefficients?"
‚ùì "Quelle feature a le coefficient le plus r√©duit par Ridge? Pourquoi?"
‚ùì "P(pointe)=0.7 pour une observation. Que signifie ce chiffre?"
‚ùì "Votre R¬≤ train est 0.95 et test 0.60. Probl√®me?"
‚ùì "Changez ce seuil de 0.5 √† 0.3 - qu'arrive-t-il?"

---

## üéØ STRAT√âGIE POUR L'ENTREVUE

### 1. Soyez confiant et clair
- Parlez lentement
- Utilisez des exemples concrets
- Admettez si vous ne savez pas (mieux que bluffer)

### 2. Justifiez TOUT
- "Pourquoi cette feature?" ‚Üí "Parce que la temp√©rature affecte directement la consommation de chauffage..."
- "Pourquoi ce Œª?" ‚Üí "J'ai fait une validation crois√©e temporelle et Œª=10 minimise l'erreur..."

### 3. Montrez votre compr√©hension
- Connectez la th√©orie au code
- Expliquez les choix d'impl√©mentation
- Anticipez les questions

### 4. Soyez pr√™t √† modifier le code
- "Que se passe-t-il si on enl√®ve la temp√©rature?"
- "Changez le learning rate √† 0.001"
- Pr√©disez AVANT d'ex√©cuter!

---

## üìö R√âVISION PAR CHAPITRE

√Ä remplir au fur et √† mesure:

### Chapitre 1: Learning Problem
- Concepts cl√©s: _____________
- Liens avec le projet: _____________

### Chapitre 2: Linear Regression
- Concepts cl√©s: _____________
- Liens avec le projet: _____________

### Chapitre 3: Classification
- Concepts cl√©s: _____________
- Liens avec le projet: _____________

### Chapitre 4: Generalization
- Concepts cl√©s: _____________
- Liens avec le projet: _____________

### Chapitre 5: Probabilistic
- Concepts cl√©s: _____________
- Liens avec le projet: _____________

---

## ‚úçÔ∏è ESPACE POUR VOS NOTES

### Mes choix de features et pourquoi:


### Mes r√©sultats et interpr√©tation:


### Questions que je ne comprends pas encore:


### Points √† clarifier avant l'entrevue:


---

**Bonne chance! Vous allez r√©ussir! üöÄ**
