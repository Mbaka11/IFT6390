# üìö Notes de Pr√©paration - Entrevue Orale (60%)

## ‚ö†Ô∏è RAPPEL: L'entrevue orale vaut 60% de la note totale!

---

## üîç Questions sur les Donn√©es et Features Engineering

### Q1: Pourquoi utilise-t-on un encodage cyclique pour les heures et les mois?

**Contexte:**
Dans le dataset, on a:

- Variables simples: `heure` (0-23), `mois` (1-12)
- Variables cycliques: `heure_sin`, `heure_cos`, `mois_sin`, `mois_cos`

**Le probl√®me avec l'encodage num√©rique simple:**

Si on utilise juste `heure = 0, 1, 2, ..., 23`:

```
Distance entre 0h et 1h:   |1 - 0| = 1
Distance entre 23h et 0h:  |0 - 23| = 23  ‚ùå FAUX!
```

Le mod√®le penserait que 23h et 0h sont **tr√®s √©loign√©es** alors qu'elles sont s√©par√©es d'**une seule heure**!

**La solution: Encodage cyclique avec sin/cos**

On transforme l'heure en coordonn√©es sur un cercle:

```python
heure_sin = sin(2œÄ √ó heure/24)
heure_cos = cos(2œÄ √ó heure/24)
```

**Visualisation mentale:**

```
        12h (midi)
         |
    9h --|-- 15h
         |
    6h --|-- 18h
         |
        0h (minuit)
         ‚Üì
       23h ‚Üê Proche de 0h! ‚úÖ
```

**Pourquoi sin ET cos (pas juste sin)?**

- Avec **seulement sin**: sin(0¬∞) = sin(360¬∞) = 0 ‚Üí 0h et 12h auraient la m√™me valeur!
- Avec **sin + cos**: Chaque heure a une combinaison **unique** (x, y) sur le cercle

**Exemple num√©rique:**

```
0h:  sin=0.00,  cos=1.00
6h:  sin=1.00,  cos=0.00
12h: sin=0.00,  cos=-1.00
18h: sin=-1.00, cos=0.00
23h: sin=0.26,  cos=0.97  ‚Üê Proche de 0h ‚úÖ
```

**Avantages principaux:**

1. ‚úÖ **Pr√©serve la proximit√©**: 23h et 0h sont proches dans l'espace des features
2. ‚úÖ **Continuit√©**: Pas de "saut" artificiel entre 23h et 0h
3. ‚úÖ **G√©n√©ralisation**: Le mod√®le apprend que les comportements √† 23h peuvent ressembler √† ceux de 0h
4. ‚úÖ **Applicabilit√©**: Fonctionne pour toute variable cyclique (jour de la semaine, mois, saison, angle, etc.)

**Pour l'entrevue, soyez pr√™t √†:**

- ‚úèÔ∏è Dessiner un cercle et placer quelques heures dessus
- üßÆ Expliquer pourquoi on a besoin de DEUX dimensions (sin + cos)
- üí° Donner un exemple concret: "La consommation √† 23h est proche de celle √† 0h (nuit)"

**Formule math√©matique √† conna√Ætre:**
$$\text{feature\_sin} = \sin\left(\frac{2\pi \times \text{valeur}}{\text{p√©riode}}\right)$$
$$\text{feature\_cos} = \cos\left(\frac{2\pi \times \text{valeur}}{\text{p√©riode}}\right)$$

O√π p√©riode = 24 pour les heures, 12 pour les mois.

---

## üìù Questions √† Pr√©parer pour l'Entrevue

### Fondamentaux

- [ ] D√©rivez la solution OLS sur le tableau
- [ ] Pourquoi division temporelle et non al√©atoire?
- [ ] Que voyez-vous dans vos r√©sidus?

### R√©gularisation

- [ ] Pourquoi Ridge aide avec des features corr√©l√©es?
- [ ] Comment choisir Œª?
- [ ] Quel coefficient a √©t√© le plus r√©duit? Pourquoi?

### Classification

- [ ] Quelle cible binaire avez-vous choisie? Justifiez.
- [ ] Le classifieur donne P=0.7. Signification?
- [ ] Pourquoi utiliser P(pointe) plut√¥t qu'un indicateur 0/1?

### Th√©orie probabiliste

- [ ] Expliquez Ridge comme estimation MAP
- [ ] Pourquoi la r√©gression logistique minimise l'entropie crois√©e?

### Synth√®se

- [ ] Parcourez votre mod√®le complet √©tape par √©tape
- [ ] Quelle am√©lioration de R¬≤ √©tait la plus importante?
- [ ] Modifiez ce seuil en direct - pr√©disez les effets

---

## üìä Concepts Cl√©s √† Ma√Ætriser

_(Cette section sera compl√©t√©e au fur et √† mesure)_

### 1. OLS (Ordinary Least Squares) - MA√éTRISER POUR L'ENTREVUE! ‚≠ê

## üìê Th√©orie Math√©matique Compl√®te

### Le Probl√®me

On cherche √† pr√©dire une variable cible $y$ (consommation √©nerg√©tique) √† partir de caract√©ristiques $\mathbf{X}$ (temp√©rature, humidit√©, etc.).

**Mod√®le lin√©aire:**
$$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip} + \epsilon_i$$

Ou en notation matricielle:
$$\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}$$

O√π:

- $\mathbf{y}$ : vecteur de taille $(n, 1)$ - les valeurs cibles
- $\mathbf{X}$ : matrice de taille $(n, p+1)$ - les caract√©ristiques (avec une colonne de 1 pour l'intercept)
- $\boldsymbol{\beta}$ : vecteur de taille $(p+1, 1)$ - les coefficients √† trouver
- $\boldsymbol{\epsilon}$ : vecteur d'erreurs (bruit)

### Objectif: Minimiser l'Erreur Quadratique

On veut trouver $\hat{\boldsymbol{\beta}}$ qui minimise:
$$L(\boldsymbol{\beta}) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \|\mathbf{y} - \mathbf{X}\boldsymbol{\beta}\|^2$$

## üìù D√©rivation √âtape par √âtape (IMPORTANT pour l'entrevue!)

**√âtape 1: √âcrire la fonction de perte**
$$L(\boldsymbol{\beta}) = (\mathbf{y} - \mathbf{X}\boldsymbol{\beta})^T(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})$$

**√âtape 2: D√©velopper**
$$L(\boldsymbol{\beta}) = \mathbf{y}^T\mathbf{y} - \mathbf{y}^T\mathbf{X}\boldsymbol{\beta} - \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y} + \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}$$

Comme $\mathbf{y}^T\mathbf{X}\boldsymbol{\beta}$ est un scalaire, $\mathbf{y}^T\mathbf{X}\boldsymbol{\beta} = \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y}$

$$L(\boldsymbol{\beta}) = \mathbf{y}^T\mathbf{y} - 2\boldsymbol{\beta}^T\mathbf{X}^T\mathbf{y} + \boldsymbol{\beta}^T\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}$$

**√âtape 3: Calculer le gradient**

Rappels d'alg√®bre lin√©aire:

- $\frac{\partial}{\partial \boldsymbol{\beta}}(\mathbf{A}\boldsymbol{\beta}) = \mathbf{A}^T$
- $\frac{\partial}{\partial \boldsymbol{\beta}}(\boldsymbol{\beta}^T\mathbf{A}\boldsymbol{\beta}) = 2\mathbf{A}\boldsymbol{\beta}$ (si $\mathbf{A}$ sym√©trique)

$$\nabla_{\boldsymbol{\beta}} L = -2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\boldsymbol{\beta}$$

**√âtape 4: √âgaler √† z√©ro (condition n√©cessaire pour un minimum)**
$$-2\mathbf{X}^T\mathbf{y} + 2\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} = 0$$

$$\mathbf{X}^T\mathbf{X}\boldsymbol{\beta} = \mathbf{X}^T\mathbf{y}$$

C'est l'**√©quation normale** !

**√âtape 5: R√©soudre pour Œ≤**

Si $\mathbf{X}^T\mathbf{X}$ est inversible:
$$\boxed{\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}}$$

C'est la **solution analytique OLS** ! üéØ

## üé® Interpr√©tation G√©om√©trique

### Vision 1: Projection orthogonale

- $\mathbf{y}$ est un vecteur dans $\mathbb{R}^n$
- $\mathbf{X}\boldsymbol{\beta}$ cr√©e un sous-espace de dimension $p$
- OLS trouve la **projection orthogonale** de $\mathbf{y}$ sur ce sous-espace
- Le r√©sidu $\mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}}$ est perpendiculaire au sous-espace

```
        y (vrai)
        |
        |     r√©sidu (erreur)
        |    /
        |   /
        |  /
        | /
        |/
        ≈∑ = XŒ≤ (pr√©diction)
    ------------------- (sous-espace g√©n√©r√© par X)
```

### Vision 2: Minimisation de distance

OLS trouve le point dans le sous-espace g√©n√©r√© par $\mathbf{X}$ le plus proche de $\mathbf{y}$ (au sens de la distance euclidienne).

## üíª Impl√©mentation Python avec Commentaires D√©taill√©s

```python
import numpy as np

def ols_fit(X, y):
    """
    Calcule les coefficients OLS via la solution analytique.

    Param√®tres:
        X : ndarray de forme (n, p) - matrice de caract√©ristiques SANS colonne de 1
        y : ndarray de forme (n,) - vecteur cible

    Retourne:
        beta : ndarray de forme (p+1,) - coefficients [intercept, coef1, coef2, ...]

    Points cl√©s pour l'entrevue:
    - Pourquoi ajouter une colonne de 1? Pour mod√©liser l'intercept Œ≤‚ÇÄ
    - Pourquoi np.linalg.solve et non l'inverse? Stabilit√© num√©rique + efficacit√©
    - Que faire si X^TX n'est pas inversible? Ridge / r√©gularisation
    """

    # √âTAPE 1: Ajouter colonne de 1 pour l'intercept
    # X devient (n, p+1) avec X[:, 0] = 1
    n = X.shape[0]
    X_with_intercept = np.column_stack([np.ones(n), X])
    # √âquivalent: X_with_intercept = np.c_[np.ones(n), X]

    # √âTAPE 2: Calculer X^T X (matrice de Gram)
    # Forme: (p+1, p+1)
    XTX = X_with_intercept.T @ X_with_intercept

    # √âTAPE 3: Calculer X^T y
    # Forme: (p+1,)
    XTy = X_with_intercept.T @ y

    # √âTAPE 4: R√©soudre le syst√®me X^T X Œ≤ = X^T y
    # IMPORTANT: On utilise solve() plut√¥t que inv() pour:
    #   - Stabilit√© num√©rique (√©vite erreurs d'arrondi)
    #   - Efficacit√© (O(n¬≥) vs O(n¬≥) mais avec meilleure constante)
    #   - Robustesse (g√®re mieux les matrices mal conditionn√©es)
    beta = np.linalg.solve(XTX, XTy)

    # Alternative (NON recommand√©e):
    # beta = np.linalg.inv(XTX) @ XTy  # ‚ùå Moins stable!

    return beta
    # beta[0] = intercept (Œ≤‚ÇÄ)
    # beta[1:] = coefficients des features (Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤‚Çö)


def ols_predict(X, beta):
    """
    Pr√©dit les valeurs avec les coefficients OLS.

    Param√®tres:
        X : ndarray de forme (n, p) - caract√©ristiques SANS colonne de 1
        beta : ndarray de forme (p+1,) - coefficients [intercept, coef1, ...]

    Retourne:
        y_pred : ndarray de forme (n,) - pr√©dictions

    Points pour l'entrevue:
    - Comment s√©parer intercept et coefficients? beta[0] vs beta[1:]
    - Forme matricielle: y = X @ w + b, o√π w=beta[1:] et b=beta[0]
    """

    # M√âTHODE 1: Ajouter colonne de 1 et multiplier
    n = X.shape[0]
    X_with_intercept = np.column_stack([np.ones(n), X])
    y_pred = X_with_intercept @ beta

    # M√âTHODE 2 (√©quivalente): S√©parer intercept et coefficients
    # y_pred = beta[0] + X @ beta[1:]

    return y_pred


# ============================================
# EXEMPLE D'UTILISATION AVEC EXPLICATIONS
# ============================================

# Supposons qu'on a:
# - n = 8760 observations (1 an de donn√©es horaires)
# - p = 3 features: temp√©rature, humidit√©, vitesse_vent

# CHARGEMENT DES DONN√âES
# X_train shape: (8760, 3)
# y_train shape: (8760,)

# 1. ENTRA√éNEMENT
beta_ols = ols_fit(X_train, y_train)
# beta_ols shape: (4,)
# beta_ols[0] = intercept = consommation de base
# beta_ols[1] = coefficient temp√©rature
# beta_ols[2] = coefficient humidit√©
# beta_ols[3] = coefficient vitesse_vent

print(f"Intercept (Œ≤‚ÇÄ): {beta_ols[0]:.2f} kWh")
print(f"Coefficient temp√©rature (Œ≤‚ÇÅ): {beta_ols[1]:.2f} kWh/¬∞C")
print(f"Coefficient humidit√© (Œ≤‚ÇÇ): {beta_ols[2]:.2f} kWh/%")
print(f"Coefficient vent (Œ≤‚ÇÉ): {beta_ols[3]:.2f} kWh/(km/h)")

# INTERPR√âTATION (pour l'entrevue):
# Si Œ≤‚ÇÅ = -5.2, cela signifie:
# "Pour chaque degr√© de temp√©rature en plus, la consommation
#  diminue de 5.2 kWh (moins de chauffage)"

# 2. PR√âDICTION
y_pred_train = ols_predict(X_train, beta_ols)
y_pred_test = ols_predict(X_test, beta_ols)

# 3. √âVALUATION
from sklearn.metrics import r2_score, mean_squared_error

r2_train = r2_score(y_train, y_pred_train)
r2_test = r2_score(y_test, y_pred_test)
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))

print(f"\nPerformance:")
print(f"  R¬≤ train: {r2_train:.4f}")
print(f"  R¬≤ test: {r2_test:.4f}")
print(f"  RMSE test: {rmse_test:.2f} kWh")

# Pour l'entrevue: Soyez pr√™t √† expliquer R¬≤!
# R¬≤ = 0.75 signifie: "Le mod√®le explique 75% de la variance de y"
# R¬≤ = 1.0 ‚Üí Pr√©diction parfaite
# R¬≤ < 0 ‚Üí Mod√®le pire qu'une simple moyenne


# ============================================
# COMPARAISON AVEC SKLEARN (Validation)
# ============================================

from sklearn.linear_model import LinearRegression

model_sklearn = LinearRegression()
model_sklearn.fit(X_train, y_train)

print("\n=== Validation avec sklearn ===")
print(f"Intercept - Vous: {beta_ols[0]:.6f}")
print(f"Intercept - sklearn: {model_sklearn.intercept_:.6f}")
print(f"Coefficients identiques: {np.allclose(beta_ols[1:], model_sklearn.coef_)}")
# Devrait afficher True si impl√©mentation correcte!
```

## ‚ö†Ô∏è Points Critiques pour l'Entrevue

### 1. Pourquoi ajouter une colonne de 1?

**Question type:** "Pourquoi ajoutez-vous np.ones dans votre code?"

**R√©ponse:**

- Sans intercept: $y = \beta_1 x_1 + \beta_2 x_2$ ‚Üí la droite passe par l'origine (0, 0)
- Avec intercept: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2$ ‚Üí plus flexible
- La colonne de 1 permet de traiter $\beta_0$ comme les autres coefficients dans le calcul matriciel

### 2. Pourquoi np.linalg.solve plut√¥t que l'inverse?

**Question type:** "Vous n'utilisez pas l'inverse explicitement, pourquoi?"

**R√©ponse:**

- `solve(A, b)` r√©sout $Ax = b$ directement via d√©composition LU/Cholesky
- `inv(A) @ b` calcule d'abord $A^{-1}$ puis multiplie ‚Üí 2 op√©rations co√ªteuses
- `solve()` est **num√©riquement plus stable** (moins d'erreurs d'arrondi)
- Exemple: Si $X^TX$ est mal conditionn√©e, inverse peut √©chouer

### 3. Quand OLS √©choue-t-il?

**Question type:** "Dans quelles situations OLS pose-t-il probl√®me?"

**R√©ponse:**

1. **$X^TX$ non inversible** (collin√©arit√© parfaite)
   - Exemple: temp√©rature_celsius et temp√©rature_fahrenheit
   - Solution: Ridge r√©gularisation
2. **Mal conditionn√©e** (features tr√®s corr√©l√©es)
   - Coefficients instables (variance √©lev√©e)
   - Solution: Ridge ou PCA
3. **p > n** (plus de features que d'observations)
   - Syst√®me sous-d√©termin√©
   - Solution: Ridge ou Lasso

4. **Outliers** (valeurs extr√™mes)
   - OLS sensible aux outliers (erreur quadratique)
   - Solution: R√©gression robuste (Huber loss)

### 4. Complexit√© computationnelle

**Question type:** "Quelle est la complexit√© de OLS?"

**R√©ponse:**

- Calcul de $X^TX$: $O(np^2)$ o√π n=#observations, p=#features
- R√©solution du syst√®me: $O(p^3)$
- **Total: $O(np^2 + p^3)$**
- Dominant: $O(np^2)$ si $n >> p$ (cas typique)

### 5. Hypoth√®ses de OLS

**Pour l'entrevue, conna√Ætre les hypoth√®ses (mais pas besoin de les v√©rifier pour ce projet):**

1. **Lin√©arit√©**: La relation est lin√©aire
2. **Ind√©pendance**: Les observations sont ind√©pendantes
3. **Homosc√©dasticit√©**: Variance constante des erreurs
4. **Normalit√©**: Les erreurs suivent une loi normale (pour l'inf√©rence)
5. **Pas de multicolin√©arit√©**: Les features ne sont pas trop corr√©l√©es

## üéØ Checklist pour l'Entrevue OLS

Pratiquez ces exercices:

- [ ] D√©river $\hat{\beta} = (X^TX)^{-1}X^Ty$ au tableau en 5 minutes
- [ ] Expliquer pourquoi on minimise l'erreur quadratique (et non absolue)
- [ ] Dessiner l'interpr√©tation g√©om√©trique (projection)
- [ ] Coder ols_fit() de m√©moire en 3 minutes
- [ ] Expliquer np.linalg.solve vs inv
- [ ] Interpr√©ter un coefficient: "Œ≤‚ÇÅ = -5.2 signifie..."
- [ ] Expliquer R¬≤ √† votre grand-m√®re
- [ ] Donner 3 situations o√π OLS √©choue

### 2. Gradient Descent

- TODO: Ajouter algorithme
- TODO: Ajouter choix du learning rate

### 3. Ridge Regression

- TODO: Ajouter lien avec MAP
- TODO: Ajouter effet sur les coefficients

### 4. R√©gression Logistique

- TODO: Ajouter fonction sigmo√Øde
- TODO: Ajouter entropie crois√©e

---

## üí° Astuces pour l'Entrevue

1. **Pr√©parez un brouillon OLS**: Entra√Ænez-vous √† d√©river $\hat{\beta} = (X^TX)^{-1}X^Ty$ au tableau
2. **Connaissez vos choix**: Pourquoi ces features? Pourquoi cette validation?
3. **Visualisez**: Dessinez les concepts (cercle pour cyclique, graphe loss pour GD)
4. **Soyez honn√™te**: Si vous ne savez pas, expliquez votre raisonnement
5. **Pr√©parez des exemples**: "Par exemple, pour la temp√©rature..."

---

## üìà Suivi de Progression

- [ ] Partie 0: Configuration ‚úÖ
- [ ] Partie 1: OLS from scratch
- [ ] Partie 2: R√©gression logistique & gradient descent
- [ ] Partie 3: Ridge regression
- [ ] Partie 4: Mod√®le √† 2 √©tages
- [ ] Partie 5: Validation temporelle
- [ ] Partie 6: Mod√®le final
- [ ] Partie 7: Extension
- [ ] Soumission Kaggle
